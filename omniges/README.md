# Omniges: Multimodal Text-Audio-Gesture Generation

**Complete OmniFlow-based Text-Audio-Gesture Generation Framework**

OmnigesëŠ” OmniFlow íŒŒì´í”„ë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ ìŠ¤íŠ¸ë¦¼ì„ ì œìŠ¤ì²˜ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ì¹˜í™˜í•œ ë©€í‹°ëª¨ë‹¬ ìƒì„± í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 6ê°œ ëª¨ë“  íƒœìŠ¤í¬ ì¡°í•©ì„ ì§€ì›í•©ë‹ˆë‹¤: **T2G, G2T, A2G, G2A, T2A, A2T**.

## ğŸ¯ Architecture Overview

### Core Components

1. **OmnigesFlowTransformerModel**: OmniFlow ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ Transformer
   - Text, Audio, Gesture 3ê°œ ëª¨ë‹¬ë¦¬í‹° joint attention
   - 24 layers, 24 heads, 1536 embedding dimension
   - ëª¨ë“  6ê°œ íƒœìŠ¤í¬ ì¡°í•© ì§€ì›: T2G, G2T, A2G, G2A, T2A, A2T

2. **GestureProcessor**: 4x Pre-trained RVQVAE
   - Upper body: 78D â†’ 128D latent
   - Hands: 180D â†’ 128D latent  
   - Lower+Trans: 57D â†’ 128D latent
   - Face: 100D â†’ 128D latent
   - 4ê°œ ë¶€ìœ„ concatenation: 512D total latent

3. **OmnigesPipeline**: ì™„ì „í•œ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
   - OmniFlow ê¸°ë°˜ ëª¨ë“  êµ¬ì„±ìš”ì†Œ í†µí•©
   - load_pretrained() ìë™ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
   - ì‚¬ìš©ì ì¹œí™”ì  API: prompt â†’ gesture/text/audio

4. **Multi-Task Training**: í†µí•© í•™ìŠµ í”„ë ˆì„ì›Œí¬
   - BEAT2 ë°ì´í„°ì…‹ ê¸°ë°˜
   - Round-robin ë˜ëŠ” í™•ë¥ ì  íƒœìŠ¤í¬ ìƒ˜í”Œë§
   - OmniFlow í˜¸í™˜ loss weighting

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Install dependencies
pip install torch torchvision torchaudio transformers accelerate wandb
pip install librosa textgrid loguru tqdm pyyaml
pip install smplx trimesh pyrender  # For rendering

# Install additional packages
pip install -e .
```

### 2. Data Preparation

Ensure BEAT2 dataset is available at:
```
./datasets/BEAT_SMPL/beat_v2.0.0/beat_english_v2.0.0/
â”œâ”€â”€ beat_smplx_141/          # SMPLX pose files (.npz)
â”œâ”€â”€ wave16k/                 # Audio files (.wav)  
â”œâ”€â”€ textgrid/                # Word alignment (.TextGrid)
â”œâ”€â”€ expression/              # Facial expressions
â””â”€â”€ train_test_split.csv     # Dataset splits
```

### 3. Multi-Task Training

#### Full Training (Multi-GPU)
```bash
# Complete 6-task training (T2G, G2T, A2G, G2A, T2A, A2T)
accelerate launch \
    --config_file configs/accelerate_config.yaml \
    omniges/scripts/train_omniges.py \
    --pretrained_model_name_or_path ./checkpoint/OmniFlow-v0.5 \
    --beat_config_path configs/shortcut_rvqvae_128.yaml \
    --rvqvae_checkpoints ./ckpt/ \
    --text_vae ./checkpoint/OmniFlow-v0.5/text_vae \
    --tokenizer ./checkpoint/OmniFlow-v0.5/vae_tokenizer \
    --output_dir ./results/omniges_multitask \
    --train_batch_size 4 \
    --num_train_epochs 20 \
    --learning_rate 1e-4 \
    --val_every 100 \
    --report_to wandb
```

#### Single GPU Training
```bash
# For testing or smaller hardware
python omniges/scripts/train_omniges.py \
    --pretrained_model_name_or_path ./checkpoint/OmniFlow-v0.5 \
    --beat_config_path configs/shortcut_rvqvae_128.yaml \
    --rvqvae_checkpoints ./ckpt/ \
    --text_vae ./checkpoint/OmniFlow-v0.5/text_vae \
    --tokenizer ./checkpoint/OmniFlow-v0.5/vae_tokenizer \
    --output_dir ./results/omniges_multitask \
    --train_batch_size 1 \
    --num_train_epochs 1 \
    --val_every 50
```

### 4. Inference & Evaluation

#### Using Pipeline (Recommended)
```python
from omniges import OmnigesPipeline

# Load pipeline
pipeline = OmnigesPipeline.load_pretrained(
    omniflow_path="./checkpoint/OmniFlow-v0.5",
    rvqvae_checkpoints={
        'upper': './ckpt/net_300000_upper.pth',
        'hands': './ckpt/net_300000_hands.pth',
        'lower_trans': './ckpt/net_300000_lower.pth',
        'face': './ckpt/net_300000_face.pth'
    }
)

# Text to Gesture
gesture = pipeline(prompt="A person waving hello", task='t2g')

# Audio to Gesture  
gesture = pipeline(input_aud="./audio.wav", task='a2g')

# Gesture to Text
text = pipeline(input_gesture=gesture_tensor, task='g2t')

# All 6 tasks supported!
```

## ğŸ“ File Structure

```
omniges/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ omniges_flow.py         # í•µì‹¬: OmniFlow ê¸°ë°˜ ë©€í‹°ëª¨ë‹¬ transformer
â”‚   â”œâ”€â”€ gesture_processor.py   # í•µì‹¬: 4x RVQVAE ì œìŠ¤ì²˜ ì²˜ë¦¬ê¸°
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ omniges_pipeline.py     # í•µì‹¬: ì™„ì „í•œ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ (ëª¨ë“  6ê°œ íƒœìŠ¤í¬)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_omniges.py        # í•µì‹¬: ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ __init__.py
â””â”€â”€ README.md

configs/
â””â”€â”€ shortcut_rvqvae_128.yaml    # BEAT ë°ì´í„°ì…‹ ì„¤ì •

checkpoint/
â””â”€â”€ OmniFlow-v0.5/              # OmniFlow ì‚¬ì „í›ˆë ¨ ëª¨ë¸

ckpt/                           # Pre-trained RVQVAE models
â”œâ”€â”€ net_300000_upper.pth
â”œâ”€â”€ net_300000_hands.pth
â”œâ”€â”€ net_300000_lower.pth
â””â”€â”€ net_300000_face.pth
```

## âš™ï¸ Key Features

### Supported Tasks
- **T2G**: Text â†’ Gesture (í…ìŠ¤íŠ¸ë¡œ ì œìŠ¤ì²˜ ìƒì„±)
- **G2T**: Gesture â†’ Text (ì œìŠ¤ì²˜ì—ì„œ í…ìŠ¤íŠ¸ ìƒì„±)  
- **A2G**: Audio â†’ Gesture (ì˜¤ë””ì˜¤ë¡œ ì œìŠ¤ì²˜ ìƒì„±)
- **G2A**: Gesture â†’ Audio (ì œìŠ¤ì²˜ì—ì„œ ì˜¤ë””ì˜¤ ìƒì„±)
- **T2A**: Text â†’ Audio (í…ìŠ¤íŠ¸ë¡œ ì˜¤ë””ì˜¤ ìƒì„±, OmniFlow ê¸°ëŠ¥)
- **A2T**: Audio â†’ Text (ì˜¤ë””ì˜¤ì—ì„œ í…ìŠ¤íŠ¸ ìƒì„±, OmniFlow ê¸°ëŠ¥)

### Architecture Details
- **Backbone**: OmniFlow MMDiT (24 layers, 24 heads, 1536D)
- **Text Processing**: CLIPÃ—2 + T5 + LLaMA VAE (OmniFlow ë°©ì‹)
- **Audio Processing**: WavLM + MFCC fusion (OmniFlow ë°©ì‹)  
- **Gesture Processing**: 4x RVQVAE (upper/hands/lower_trans/face)
- **Multi-Task**: Round-robin ë˜ëŠ” í™•ë¥ ì  íƒœìŠ¤í¬ ìƒ˜í”Œë§

## ğŸ“Š Expected Performance

### Training Metrics (All Tasks)
- **Gesture Loss**: Flow matching loss for gesture generation (T2G, A2G)
- **Text Loss**: Flow matching + decode loss for text generation (G2T, A2T)  
- **Audio Loss**: Flow matching loss for audio generation (G2A, T2A)
- **Multi-Task Weighting**: Task-specific loss factors
  
### Evaluation Metrics
- **Gesture Quality**: FGD, L1 Diversity, Motion smoothness
- **Text Quality**: BLEU, ROUGE, Semantic similarity
- **Audio Quality**: Spectrogram alignment, Audio-gesture sync
- **Cross-Modal**: Alignment scores between modalities

### Hardware Requirements
- **Memory**: ~12-16GB per GPU for batch_size=4 (3 modalities)
- **Training Time**: ~3-5 days for 20 epochs on multi-GPU
- **Storage**: ~80GB for BEAT cache + checkpoints + OmniFlow model

## ğŸ”§ Troubleshooting

### Common Issues

1. **CUDA OOM**: Multi-modality í•™ìŠµì€ ë©”ëª¨ë¦¬ê°€ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤
   ```bash
   # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   --train_batch_size 1
   # ë˜ëŠ” gradient accumulation ì‚¬ìš©
   --gradient_accumulation_steps 4
   ```

2. **Checkpoint Loading**: OmniFlow ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸
   ```bash
   # ì²´í¬í¬ì¸íŠ¸ êµ¬ì¡° í™•ì¸
   ls -la ./checkpoint/OmniFlow-v0.5/
   # transformer/, text_vae/, vae_tokenizer/ ì¡´ì¬í•´ì•¼ í•¨
   ```

3. **RVQVAE Models**: 4ê°œ ë¶€ìœ„ ì²´í¬í¬ì¸íŠ¸ ëª¨ë‘ í•„ìš”
   ```bash
   # RVQVAE íŒŒì¼ í™•ì¸
   ls -la ./ckpt/net_300000_*.pth
   ```

4. **Task-specific Issues**: íŠ¹ì • íƒœìŠ¤í¬ì—ì„œ validation ì‹¤íŒ¨ ì‹œ í•´ë‹¹ íƒœìŠ¤í¬ ë¹„í™œì„±í™”

### Monitoring
- **W&B Dashboard**: ëª¨ë“  6ê°œ íƒœìŠ¤í¬ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- **TensorBoard**: Local metric visualization  
- **Validation**: ë§¤ 100 ìŠ¤í…ë§ˆë‹¤ ëª¨ë“  íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸

## ğŸ›£ï¸ Future Extensions

1. **LibriTTS Integration**: ë” í° ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ í™•ì¥
2. **Real-time Inference**: ì‹¤ì‹œê°„ ì œìŠ¤ì²˜ ìƒì„± ìµœì í™”
3. **Fine-tuning**: íŠ¹ì • ë„ë©”ì¸/ìŠ¤íƒ€ì¼ fine-tuning
4. **3D Rendering**: SMPL-X ê¸°ë°˜ ì‹¤ì‹œê°„ 3D ë Œë”ë§
5. **Mobile Deployment**: ê²½ëŸ‰í™” ëª¨ë¸ for mobile/edge devices

## ğŸ“š References

- **OmniFlow**: Base multimodal pipeline
- **MMDiT**: Multimodal Diffusion Transformer
- **BEAT2**: Large-scale gesture dataset
- **RVQVAE**: Residual Vector Quantized VAE for motion
