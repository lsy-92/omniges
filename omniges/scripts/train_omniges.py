#!/usr/bin/env python
# coding=utf-8
# Copyright 2024 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and

"""
Omniges Training Script - BEAT2 Dataset Integration
Complete multi-task training for Text-Audio-Gesture generation
Based on OmniFlow + RVQVAE gesture processing + BEAT2 Dataset

ğŸ¯ SUPPORTED TASKS:
- t2g: Text to Gesture (í…ìŠ¤ì²˜ â†’ ì œìŠ¤ì²˜)  
- g2t: Gesture to Text (ì œìŠ¤ì²˜ â†’ í…ìŠ¤íŠ¸)
- a2g: Audio to Gesture (ì˜¤ë””ì˜¤ â†’ ì œìŠ¤ì²˜)
- g2a: Gesture to Audio (ì œìŠ¤ì²˜ â†’ ì˜¤ë””ì˜¤)
- t2a: Text to Audio (í…ìŠ¤íŠ¸ â†’ ì˜¤ë””ì˜¤)
- a2t: Audio to Text (ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸)

ğŸ“Š BEAT2 DATASET INTEGRATION:
âœ… Removed all dummy data (torch.randn, fake gestures)
âœ… Uses real BEAT2 speech WAV files from wave16k directory
âœ… Uses real BEAT2 gesture NPZ files (SMPL-X format) 
âœ… Uses real BEAT2 text from TextGrid files
âœ… Real gesture validation during training
âœ… BEAT2 metadata tracking for debugging

ğŸš€ USAGE:
python train_omniges.py \
    --pretrained_model_name_or_path /path/to/omniflow \
    --beat2_data_root ./datasets/BEAT_SMPL/ \
    --beat2_wav_dir wave16k \
    --beat2_gesture_dir speakers_1234_smplx_neutral_npz \
    --beat2_text_dir word \
    --use_beat2_cache
"""

# ============================================================================
# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ============================================================================
import argparse  # ëª…ë ¹í–‰ ì¸ìˆ˜ íŒŒì‹±
import copy  # ê°ì²´ ë³µì‚¬
import gc  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
import time  # ì‹œê°„ ì¸¡ì •
from safetensors import safe_open  # ì•ˆì „í•œ í…ì„œ íŒŒì¼ ë¡œë”©
import sys  # ì‹œìŠ¤í…œ ê´€ë ¨ ê¸°ëŠ¥
import os  # ìš´ì˜ì²´ì œ ê´€ë ¨ ê¸°ëŠ¥
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬

# ============================================================================
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
# ============================================================================
sys.path.append(str(Path(__file__).parent.parent.parent))  # í˜„ì¬ íŒŒì¼ì˜ ìƒìœ„ 3ë‹¨ê³„ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€

# ============================================================================
# Omniges ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸
# ============================================================================
from omniges.models import OmnigesFlowTransformerModel, GestureProcessor  # Omniges ëª¨ë¸ ë° ì œìŠ¤ì²˜ í”„ë¡œì„¸ì„œ
from omniges.pipelines import OmnigesPipeline, OmnigesGestureVAE  # Omniges íŒŒì´í”„ë¼ì¸ ë° ì œìŠ¤ì²˜ VAE

# ============================================================================
# OmniFlow ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸
# ============================================================================
from omniflow.utils.ema import EMAModel  # ì§€ìˆ˜ ì´ë™ í‰ê·  ëª¨ë¸
import torch.utils.data  # PyTorch ë°ì´í„° ìœ í‹¸ë¦¬í‹°
from transformers.trainer_pt_utils import LabelSmoother  # ë¼ë²¨ ìŠ¤ë¬´ë”©
import itertools  # ë°˜ë³µì ìœ í‹¸ë¦¬í‹°
import logging  # ë¡œê¹…
import math  # ìˆ˜í•™ í•¨ìˆ˜
import random  # ëœë¤ ìƒì„±
import shutil  # íŒŒì¼/ë””ë ‰í† ë¦¬ ë³µì‚¬
import warnings  # ê²½ê³  ì²˜ë¦¬
from contextlib import nullcontext  # ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €
import pandas as pd  # ë°ì´í„° ë¶„ì„
import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚°
import torch  # PyTorch ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
import torch.utils.checkpoint  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
import transformers  # Hugging Face íŠ¸ëœìŠ¤í¬ë¨¸
from accelerate import Accelerator  # ë¶„ì‚° í›ˆë ¨ ê°€ì†ê¸°
from accelerate.logging import get_logger  # ë¡œê±° ìƒì„±
from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed  # ë¶„ì‚° í›ˆë ¨ ìœ í‹¸ë¦¬í‹°
from huggingface_hub import create_repo, upload_folder  # Hugging Face Hub ì—°ë™
from huggingface_hub.utils import insecure_hashlib  # í•´ì‹œ ìœ í‹¸ë¦¬í‹°
from PIL import Image  # ì´ë¯¸ì§€ ì²˜ë¦¬
from PIL.ImageOps import exif_transpose  # EXIF ì •ë³´ ì²˜ë¦¬
from torch.utils.data import Dataset  # ë°ì´í„°ì…‹ í´ë˜ìŠ¤
from torchvision import transforms  # ì´ë¯¸ì§€ ë³€í™˜
from torchvision.transforms.functional import crop  # ì´ë¯¸ì§€ í¬ë¡­
from tqdm.auto import tqdm  # ì§„í–‰ë¥  í‘œì‹œ
from transformers import CLIPTextModelWithProjection, CLIPVisionModelWithProjection, CLIPTokenizer, PretrainedConfig, T5EncoderModel, T5TokenizerFast, CLIPImageProcessor  # íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ë“¤
import torch.nn.functional as F  # PyTorch í•¨ìˆ˜í˜• API
import diffusers  # ë””í“¨ì „ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from diffusers import AutoencoderKL  # ìë™ ì¸ì½”ë”
from omniflow.utils.scheduler import OmniFlowMatchEulerDiscreteScheduler as FlowMatchEulerDiscreteScheduler  # OmniFlow ìŠ¤ì¼€ì¤„ëŸ¬ (Omnigesì—ì„œ ì¬ì‚¬ìš©)
from transformers.pytorch_utils import ALL_LAYERNORM_LAYERS  # ë ˆì´ì–´ ì •ê·œí™”
from transformers.trainer_pt_utils import get_parameter_names  # íŒŒë¼ë¯¸í„° ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
from diffusers.image_processor import VaeImageProcessor  # VAE ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ
from diffusers.optimization import get_scheduler  # ì˜µí‹°ë§ˆì´ì € ìŠ¤ì¼€ì¤„ëŸ¬
from diffusers.training_utils import compute_density_for_timestep_sampling, compute_loss_weighting_for_sd3  # í›ˆë ¨ ìœ í‹¸ë¦¬í‹°
from diffusers.utils import (  # diffusers ìœ í‹¸ë¦¬í‹°
    check_min_version,  # ë²„ì „ ì²´í¬
    is_wandb_available,  # wandb ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
)
from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card  # ëª¨ë¸ ì¹´ë“œ ìœ í‹¸ë¦¬í‹°
from diffusers.utils.torch_utils import is_compiled_module  # ì»´íŒŒì¼ëœ ëª¨ë“ˆ ì²´í¬
import torch.distributed as dist  # ë¶„ì‚° í›ˆë ¨
import glob  # íŒŒì¼ íŒ¨í„´ ë§¤ì¹­
from omniflow.models.audio_vae import load_audio_vae  # ì˜¤ë””ì˜¤ VAE ë¡œë”©
from omniflow.utils.text_encode import encode_prompt_train, cat_and_pad, encode_prompt_for_decoder  # í…ìŠ¤íŠ¸ ì¸ì½”ë”© ìœ í‹¸ë¦¬í‹°

# TextGrid íŒŒì¼ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import textgrid as tg
except ImportError:
    print("Warning: textgrid library not found. Installing...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "textgrid"])
    import textgrid as tg

# ============================================================================
# BEAT2 TextGrid íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜
# ============================================================================
def extract_text_from_textgrid(textgrid_path):
    """
    BEAT2 TextGrid íŒŒì¼ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        textgrid_path (str): TextGrid íŒŒì¼ ê²½ë¡œ
        
    Returns:
        str: ì¶”ì¶œëœ í…ìŠ¤íŠ¸
    """
    try:
        if not os.path.exists(textgrid_path):
            return "Gesture movement"  # ê¸°ë³¸ í…ìŠ¤íŠ¸
            
        tgrid = tg.TextGrid.fromFile(textgrid_path)
        words = []
        
        # ì²« ë²ˆì§¸ tierì—ì„œ ë‹¨ì–´ë“¤ ì¶”ì¶œ
        for interval in tgrid[0]:
            if interval.mark and interval.mark.strip():
                words.append(interval.mark)
                
        if words:
            return ' '.join(words)
        else:
            return "Gesture movement"
            
    except Exception as e:
        print(f"Warning: Failed to extract text from {textgrid_path}: {e}")
        return "Gesture movement"

# ============================================================================
# wandb ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ì„í¬íŠ¸
# ============================================================================
if is_wandb_available():  # wandbê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
    import wandb  # ì‹¤í—˜ ì¶”ì  ë¼ì´ë¸ŒëŸ¬ë¦¬
from torch import nn  # ì‹ ê²½ë§ ëª¨ë“ˆ
check_min_version("0.30.0.dev0")  # ìµœì†Œ ë²„ì „ ì²´í¬

logger = get_logger(__name__)  # ë¡œê±° ìƒì„±

# ============================================================================
# ê²€ì¦ìš© íŒŒì¼ ê²½ë¡œ (í…ŒìŠ¤íŠ¸ìš©)
# ============================================================================
VAL_FILES = ['./assets/girl.png']  # ê²€ì¦ìš© ì´ë¯¸ì§€ íŒŒì¼
VAL_FILES_AUDIO = ['./assets/car engine.mp3']  # ê²€ì¦ìš© ì˜¤ë””ì˜¤ íŒŒì¼

# ============================================================================
# ì¶”ê°€ ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
# ============================================================================
from omniflow.models.text_vae import LLamaForLatentConnector  # í…ìŠ¤íŠ¸ VAE (LLaMA ê¸°ë°˜)
from omniflow.models.encoders import LanguageBindAudioProcessor, LanguageBindAudio  # ì˜¤ë””ì˜¤ ì¸ì½”ë”
import yaml  # YAML ì„¤ì • íŒŒì¼ ì²˜ë¦¬
from transformers import AutoTokenizer, AutoConfig  # ìë™ í† í¬ë‚˜ì´ì € ë° ì„¤ì •

# ============================================================================
# BEAT ë°ì´í„° ì²˜ë¦¬ ì„í¬íŠ¸
# ============================================================================
from dataloaders.beat_sep_lower import CustomDataset  # BEAT ë°ì´í„°ì…‹
from dataloaders.data_tools import joints_list  # ê´€ì ˆ ë¦¬ìŠ¤íŠ¸
from utils import rotation_conversions as rc  # íšŒì „ ë³€í™˜ ìœ í‹¸ë¦¬í‹°


# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================

def load_yaml(fp: str):
    """
    YAML ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    with open(fp, 'r') as file:  # íŒŒì¼ì„ ì½ê¸° ëª¨ë“œë¡œ ì—´ê¸°
        data = yaml.safe_load(file)  # YAML íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±
    return data  # íŒŒì‹±ëœ ë°ì´í„° ë°˜í™˜


def n_get_sigmas(noise_scheduler_copy, device, timesteps, n_dim=4, dtype=torch.float32):
    """
    ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ íŠ¹ì • íƒ€ì„ìŠ¤í…ì— í•´ë‹¹í•˜ëŠ” ì‹œê·¸ë§ˆ ê°’ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        noise_scheduler_copy: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸
        device: ê³„ì‚° ë””ë°”ì´ìŠ¤
        timesteps: íƒ€ì„ìŠ¤í… í…ì„œ
        n_dim: ì›í•˜ëŠ” ì°¨ì› ìˆ˜ (ê¸°ë³¸ê°’: 4)
        dtype: ë°ì´í„° íƒ€ì… (ê¸°ë³¸ê°’: torch.float32)
    
    Returns:
        sigma: í•´ë‹¹ íƒ€ì„ìŠ¤í…ì˜ ì‹œê·¸ë§ˆ ê°’ë“¤
    """
    sigmas = noise_scheduler_copy.sigmas.to(device=device, dtype=dtype)  # ì‹œê·¸ë§ˆ ê°’ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    schedule_timesteps = noise_scheduler_copy.timesteps.to(device)  # ìŠ¤ì¼€ì¤„ íƒ€ì„ìŠ¤í…ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    timesteps = timesteps.to(device)  # ì…ë ¥ íƒ€ì„ìŠ¤í…ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]  # ê° íƒ€ì„ìŠ¤í…ì— í•´ë‹¹í•˜ëŠ” ì¸ë±ìŠ¤ ì°¾ê¸°

    sigma = sigmas[step_indices].flatten()  # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì‹œê·¸ë§ˆ ê°’ë“¤ì„ ì¶”ì¶œí•˜ê³  í‰íƒ„í™”
    while len(sigma.shape) < n_dim:  # ì›í•˜ëŠ” ì°¨ì› ìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ì°¨ì› ì¶”ê°€
        sigma = sigma.unsqueeze(-1)  # ë§ˆì§€ë§‰ ì°¨ì›ì— 1ì°¨ì› ì¶”ê°€
    return sigma  # ìµœì¢… ì‹œê·¸ë§ˆ í…ì„œ ë°˜í™˜


def n_compute_text_embeddings(device, prompt, text_encoders, tokenizers, add_token_embed=True, train=False):
    """
    í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        device: ê³„ì‚° ë””ë°”ì´ìŠ¤
        prompt: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
        text_encoders: í…ìŠ¤íŠ¸ ì¸ì½”ë” ëª¨ë¸ë“¤
        tokenizers: í† í¬ë‚˜ì´ì €ë“¤
        add_token_embed: í† í° ì„ë² ë”© ì¶”ê°€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        train: í›ˆë ¨ ëª¨ë“œ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
    
    Returns:
        prompt_embeds: í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
        pooled_prompt_embeds: í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
    """
    print(f"DEBUG: [n_compute_text_embeddings] Input prompts count: {len(prompt)}")  # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ê°œìˆ˜ ì¶œë ¥
    print(f"DEBUG: [n_compute_text_embeddings] Sample prompt: {prompt[0] if prompt else 'No prompts'}")  # ìƒ˜í”Œ í”„ë¡¬í”„íŠ¸ ì¶œë ¥
    print(f"DEBUG: [n_compute_text_embeddings] add_token_embed: {add_token_embed}, train: {train}")  # ì„¤ì •ê°’ ì¶œë ¥
    
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡  ëª¨ë“œ)
        prompt_embeds, pooled_prompt_embeds = encode_prompt_train(  # í›ˆë ¨ìš© í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”© í•¨ìˆ˜ í˜¸ì¶œ
            text_encoders,  # í…ìŠ¤íŠ¸ ì¸ì½”ë”ë“¤
            tokenizers,  # í† í¬ë‚˜ì´ì €ë“¤
            prompt,  # í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            256,  # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
            add_token_embed=add_token_embed,  # í† í° ì„ë² ë”© ì¶”ê°€ ì—¬ë¶€
            normalize=True,  # ì •ê·œí™” í™œì„±í™”
            drops=list(  # ë“œë¡­ì•„ì›ƒ ì„¤ì •
                np.random.rand() > 0.5 for _ in range(4)  # í›ˆë ¨ ì‹œ ëœë¤ ë“œë¡­ì•„ì›ƒ
            ) if train else [False, False, False, False]  # ì¶”ë¡  ì‹œ ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™”
        )
        print(f"DEBUG: [n_compute_text_embeddings] Raw prompt_embeds shape: {prompt_embeds.shape}")  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: [n_compute_text_embeddings] Raw pooled_prompt_embeds shape: {pooled_prompt_embeds.shape}")  # ì›ë³¸ í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        
        prompt_embeds = prompt_embeds.to(device)  # í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        pooled_prompt_embeds = pooled_prompt_embeds.to(device)  # í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        
        print(f"DEBUG: [n_compute_text_embeddings] Final prompt_embeds shape: {prompt_embeds.shape}")  # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: [n_compute_text_embeddings] Final pooled_prompt_embeds shape: {pooled_prompt_embeds.shape}")  # ìµœì¢… í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
    return prompt_embeds, pooled_prompt_embeds  # í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ê³¼ í’€ë§ëœ ì„ë² ë”© ë°˜í™˜


class OmnigesDataset(Dataset):
    """
    Omniges ë°ì´í„°ì…‹ - BEAT ì œìŠ¤ì²˜ ë°ì´í„°ì™€ í…ìŠ¤íŠ¸/ì˜¤ë””ì˜¤ë¥¼ ê²°í•©
    ëª¨ë“  íƒœìŠ¤í¬ ì¡°í•© ì§€ì›: t2g, g2t, a2g, g2a, t2a, a2t
    """

    def __init__(
        self,
        beat_config_path="configs/shortcut_rvqvae_128.yaml",  # BEAT2 ì„¤ì • íŒŒì¼ ê²½ë¡œ
        task_weights=[1/6] * 6,  # ëª¨ë“  6ê°œ íƒœìŠ¤í¬ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜
        size=512,  # ì´ë¯¸ì§€ í¬ê¸° (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
        is_train=True,  # í›ˆë ¨ ëª¨ë“œ ì—¬ë¶€
        image_processor=None,  # ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
        audio_processor=None,  # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ (BEAT2 WAV íŒŒì¼ìš©)
        audio_processor_clip=None,  # CLIPìš© ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ (BEAT2 WAV íŒŒì¼ìš©)
        # BEAT2 ë°ì´í„°ì…‹ íŠ¹ì • ë§¤ê°œë³€ìˆ˜ë“¤
        beat2_data_root="./datasets/BEAT_SMPL/",  # BEAT2 ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        beat2_wav_dir="wave16k",  # BEAT2 WAV íŒŒì¼ ë””ë ‰í† ë¦¬
        beat2_gesture_dir="speakers_1234_smplx_neutral_npz",  # BEAT2 ì œìŠ¤ì²˜ NPZ íŒŒì¼ ë””ë ‰í† ë¦¬
        beat2_text_dir="word",  # BEAT2 TextGrid íŒŒì¼ ë””ë ‰í† ë¦¬
        use_beat2_cache=False,  # ìºì‹œëœ BEAT2 ë°ì´í„° ì‚¬ìš© ì—¬ë¶€
        beat2_cache_dir="./datasets/beat_cache/"  # BEAT2 ìºì‹œ ë””ë ‰í† ë¦¬
    ):
        # ============================================================================
        # ê¸°ë³¸ ì†ì„± ì´ˆê¸°í™”
        # ============================================================================
        self.size = size  # ì´ë¯¸ì§€ í¬ê¸° ì €ì¥
        self.image_processor = image_processor  # ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ì €ì¥
        self.audio_processor = audio_processor  # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì €ì¥ (BEAT2 WAV íŒŒì¼ ì²˜ë¦¬ìš©)
        self.audio_processor_clip = audio_processor_clip  # CLIPìš© ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ ì €ì¥ (BEAT2 WAV íŒŒì¼ìš©)
        self.task_weights = task_weights  # íƒœìŠ¤í¬ ê°€ì¤‘ì¹˜ ì €ì¥
        self.is_train = is_train  # í›ˆë ¨ ëª¨ë“œ ì €ì¥
        
        # ============================================================================
        # BEAT2 ë°ì´í„°ì…‹ íŠ¹ì • ì†ì„± ì €ì¥
        # ============================================================================
        self.beat2_data_root = beat2_data_root  # BEAT2 ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        self.beat2_wav_dir = beat2_wav_dir      # BEAT2 WAV íŒŒì¼ ë””ë ‰í† ë¦¬ ì´ë¦„
        self.beat2_gesture_dir = beat2_gesture_dir  # BEAT2 ì œìŠ¤ì²˜ NPZ íŒŒì¼ ë””ë ‰í† ë¦¬ ì´ë¦„
        self.beat2_text_dir = beat2_text_dir    # BEAT2 TextGrid íŒŒì¼ ë””ë ‰í† ë¦¬ ì´ë¦„
        self.use_beat2_cache = use_beat2_cache  # ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        self.beat2_cache_dir = beat2_cache_dir  # ìºì‹œ ë””ë ‰í† ë¦¬
        
        logger.info(f"ğŸ­ Initializing Omniges Dataset with BEAT2 data:")
        logger.info(f"  â€¢ Data root: {self.beat2_data_root}")
        logger.info(f"  â€¢ Audio dir: {self.beat2_wav_dir}")
        logger.info(f"  â€¢ Gesture dir: {self.beat2_gesture_dir}")
        logger.info(f"  â€¢ Text dir: {self.beat2_text_dir}")
        logger.info(f"  â€¢ Cache enabled: {self.use_beat2_cache}")
        
        # ============================================================================
        # BEAT2 ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ë°ì´í„°ì…‹ ìƒì„±
        # ============================================================================
        with open(beat_config_path, 'r') as f:  # BEAT2 ì„¤ì • íŒŒì¼ì„ ì½ê¸° ëª¨ë“œë¡œ ì—´ê¸°
            beat_config = yaml.safe_load(f)  # YAML íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±
            
        # ============================================================================
        # BEAT ì„¤ì •ì„ ìœ„í•œ ì¸ì ê°ì²´ ìƒì„±
        # ============================================================================
        class BeatArgs:
            def __init__(self, config):
                for key, value in config.items():  # ì„¤ì • ë”•ì…”ë„ˆë¦¬ì˜ ëª¨ë“  í‚¤-ê°’ ìŒì„ ë°˜ë³µ
                    setattr(self, key, value)  # ê°ì²´ì— ì†ì„±ìœ¼ë¡œ ì„¤ì •
                # BEAT2 ë°ì´í„°ì…‹ì„ ìœ„í•œ ëª¨ë“  ëˆ„ë½ëœ ì†ì„± ì¶”ê°€
                self.multi_length_training = [1.0]  # ë‹¤ì¤‘ ê¸¸ì´ í›ˆë ¨ ì„¤ì •
                self.beat_align = False  # BEAT ì •ë ¬ ë¹„í™œì„±í™”
                # ìºì‹œ ì„¤ì • - BEAT2 ì¸ìì— ë”°ë¼ ì¡°ì •
                self.word_cache = use_beat2_cache          # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ ë‹¨ì–´ ìºì‹œ
                self.facial_cache = use_beat2_cache        # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ ì–¼êµ´ ìºì‹œ
                self.audio_cache = use_beat2_cache         # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ ì˜¤ë””ì˜¤ ìºì‹œ
                self.pose_cache = use_beat2_cache          # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ í¬ì¦ˆ ìºì‹œ
                self.trans_cache = use_beat2_cache         # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ ë³€í™˜ ìºì‹œ
                self.speaker_cache = use_beat2_cache       # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ í™”ì ìºì‹œ
                self.emotion_cache = use_beat2_cache       # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ ê°ì • ìºì‹œ
                self.semantic_cache = use_beat2_cache      # BEAT2 ìºì‹œ ì„¤ì •ì— ë”°ë¥¸ ì˜ë¯¸ ìºì‹œ
                # BEAT2 ë°ì´í„° ê²½ë¡œ ì„¤ì •
                if hasattr(self, 'data_root'):
                    self.data_root = beat2_data_root  # BEAT2 ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
                
        self.beat_args = BeatArgs(beat_config)  # BEAT2 ì¸ì ê°ì²´ ìƒì„±
        
        # ============================================================================
        # BEAT2 ë°ì´í„°ì…‹ ìƒì„± (ì‹¤ì œ WAV, NPZ, TextGrid íŒŒì¼ ì‚¬ìš©)
        # ============================================================================
        logger.info(f"ğŸ“Š Creating CustomDataset for BEAT2 data...")
        logger.info(f"  â€¢ Config cache settings: word={self.beat_args.word_cache}, audio={self.beat_args.audio_cache}")
        
        self.beat_dataset = CustomDataset(  # ì»¤ìŠ¤í…€ BEAT2 ë°ì´í„°ì…‹ ìƒì„±
            self.beat_args,  # BEAT2 ì¸ì ì „ë‹¬
            loader_type="train" if is_train else "test",  # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
            build_cache=use_beat2_cache  # BEAT2 ìºì‹œ ì„¤ì • ì‚¬ìš©
        )
        
        logger.info(f"âœ… BEAT2 dataset loaded: {len(self.beat_dataset)} samples")
        
        # ============================================================================
        # íƒœìŠ¤í¬ ì¡°í•© ë° í”„ë¡¬í”„íŠ¸ ì„¤ì •
        # ============================================================================
        # ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ ì¡°í•©ë“¤
        self.tasks = ['t2g', 'g2t', 'a2g', 'g2a', 't2a', 'a2t']  # 6ê°€ì§€ ë©€í‹°ëª¨ë‹¬ íƒœìŠ¤í¬
        
        # ì œìŠ¤ì²˜ íƒœìŠ¤í¬ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
        self.gesture_prompts = [  # ì œìŠ¤ì²˜ ê´€ë ¨ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            "A person waving hello",  # ì¸ì‚¬í•˜ëŠ” ì‚¬ëŒ
            "Someone clapping their hands",  # ë°•ìˆ˜ì¹˜ëŠ” ì‚¬ëŒ
            "A person pointing forward",  # ì•ì„ ê°€ë¦¬í‚¤ëŠ” ì‚¬ëŒ
            "Dancing with arm movements",  # íŒ” ì›€ì§ì„ìœ¼ë¡œ ì¶¤ì¶”ëŠ” ì‚¬ëŒ
            "Gesturing while speaking",  # ë§í•˜ë©´ì„œ ì œìŠ¤ì²˜í•˜ëŠ” ì‚¬ëŒ
            "Hand gestures during conversation",  # ëŒ€í™” ì¤‘ ì† ì œìŠ¤ì²˜
            "Expressive body language",  # í‘œí˜„ì ì¸ ë°”ë””ë­ê·€ì§€
            "Animated talking with hands",  # ì†ìœ¼ë¡œ ì• ë‹ˆë©”ì´ì…˜í•˜ë©° ë§í•˜ëŠ” ì‚¬ëŒ
            "Conducting orchestra movements",  # ì˜¤ì¼€ìŠ¤íŠ¸ë¼ ì§€íœ˜ ë™ì‘
            "Sign language communication"  # ìˆ˜í™” ì˜ì‚¬ì†Œí†µ
        ]

    def __len__(self):
        """
        ë°ì´í„°ì…‹ì˜ ê¸¸ì´ë¥¼ ë°˜í™˜í•˜ëŠ” ë©”ì„œë“œ
        """
        return len(self.beat_dataset)  # BEAT ë°ì´í„°ì…‹ì˜ ê¸¸ì´ ë°˜í™˜

    def __getitem__(self, index):
        """
        ë°ì´í„°ì…‹ì—ì„œ íŠ¹ì • ì¸ë±ìŠ¤ì˜ ì•„ì´í…œì„ ê°€ì ¸ì˜¤ëŠ” ë©”ì„œë“œ
        
        Args:
            index: ê°€ì ¸ì˜¬ ì•„ì´í…œì˜ ì¸ë±ìŠ¤
        
        Returns:
            dict: ì²˜ë¦¬ëœ ë°ì´í„° ì•„ì´í…œ
        """
        # ============================================================================
        # BEAT ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        # ============================================================================
        beat_item = self.beat_dataset[index]  # BEAT ë°ì´í„°ì…‹ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        
        # ============================================================================
        # ëœë¤ íƒœìŠ¤í¬ ì„ íƒ
        # ============================================================================
        task = np.random.choice(self.tasks, p=self.task_weights)  # ê°€ì¤‘ì¹˜ì— ë”°ë¼ ëœë¤í•˜ê²Œ íƒœìŠ¤í¬ ì„ íƒ
        
        # ============================================================================
        # íƒœìŠ¤í¬ë³„ ì²˜ë¦¬ ë¡œì§
        # ============================================================================
        if task in ['t2g', 'g2t']:  # í…ìŠ¤íŠ¸-ì œìŠ¤ì²˜ íƒœìŠ¤í¬ë“¤
            # í…ìŠ¤íŠ¸-ì œìŠ¤ì²˜ íƒœìŠ¤í¬
            prompt = np.random.choice(self.gesture_prompts)  # ì œìŠ¤ì²˜ í”„ë¡¬í”„íŠ¸ ì¤‘ ëœë¤ ì„ íƒ
            prompt2 = prompt  # ë‘ ì¸ì½”ë” ëª¨ë‘ì— ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            has_text = True  # í…ìŠ¤íŠ¸ ì‚¬ìš©
            has_gesture = True  # ì œìŠ¤ì²˜ ì‚¬ìš©
            has_audio = False  # ì˜¤ë””ì˜¤ ë¯¸ì‚¬ìš©
            
        elif task in ['a2g', 'g2a']:  # ì˜¤ë””ì˜¤-ì œìŠ¤ì²˜ íƒœìŠ¤í¬ë“¤
            # ì˜¤ë””ì˜¤-ì œìŠ¤ì²˜ íƒœìŠ¤í¬
            prompt = ""  # ìˆœìˆ˜ ì˜¤ë””ì˜¤-ì œìŠ¤ì²˜ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì—†ìŒ
            prompt2 = ""
            has_text = False  # í…ìŠ¤íŠ¸ ë¯¸ì‚¬ìš©
            has_gesture = True  # ì œìŠ¤ì²˜ ì‚¬ìš©
            has_audio = True  # ì˜¤ë””ì˜¤ ì‚¬ìš©
            
        elif task == 't2a':  # í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ íƒœìŠ¤í¬ (OmniFlowì—ì„œ ê°€ì ¸ì˜´)
            # í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ íƒœìŠ¤í¬
            prompt = "Music playing"  # ì˜¤ë””ì˜¤ ì„¤ëª…
            prompt2 = prompt
            has_text = True  # í…ìŠ¤íŠ¸ ì‚¬ìš©
            has_gesture = False  # ì œìŠ¤ì²˜ ë¯¸ì‚¬ìš©
            has_audio = True  # ì˜¤ë””ì˜¤ ì‚¬ìš©
            
        elif task == 'a2t':  # ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ íƒœìŠ¤í¬ (OmniFlowì—ì„œ ê°€ì ¸ì˜´)
            # ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ íƒœìŠ¤í¬
            prompt = ""  # ì˜¤ë””ì˜¤ì—ì„œ ìƒì„±ë  í…ìŠ¤íŠ¸
            prompt2 = ""
            has_text = True  # í…ìŠ¤íŠ¸ ì‚¬ìš©
            has_gesture = False  # ì œìŠ¤ì²˜ ë¯¸ì‚¬ìš©
            has_audio = True  # ì˜¤ë””ì˜¤ ì‚¬ìš©
            
        # ============================================================================
        # BEAT2 Datasetì—ì„œ ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬
        # ============================================================================
        beat_item = self.beat_dataset[index]  # BEAT ë°ì´í„°ì…‹ì—ì„œ í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì•„ì´í…œ ê°€ì ¸ì˜¤ê¸°
        
        pose = beat_item['pose']       # í¬ì¦ˆ ë°ì´í„° (T, pose_dim) - BEAT2 NPZ íŒŒì¼ì—ì„œ
        facial = beat_item['facial']   # ì–¼êµ´ ë°ì´í„° (T, 100) - BEAT2 NPZ íŒŒì¼ì—ì„œ
        trans = beat_item['trans']     # ë³€í™˜ ë°ì´í„° (T, 3) - BEAT2 NPZ íŒŒì¼ì—ì„œ
        trans_v = beat_item['trans_v'] # ë³€í™˜ ì†ë„ ë°ì´í„° (T, 3) - BEAT2 NPZ íŒŒì¼ì—ì„œ
        audio_features = beat_item['audio']     # ì˜¤ë””ì˜¤ íŠ¹ì§• (T_audio, audio_dim) - BEAT2 WAV íŒŒì¼ì—ì„œ
        word_features = beat_item['word']       # í…ìŠ¤íŠ¸ íŠ¹ì§• (T_text, word_dim) - BEAT2 TextGrid íŒŒì¼ì—ì„œ
        audio_name = beat_item['audio_name']    # ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        
        # BEAT2ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (TextGrid íŒŒì¼ì—ì„œ)
        actual_text = ""
        try:
            # BEAT2 ë°ì´í„°ì—ì„œ audio_nameì„ ê¸°ë°˜ìœ¼ë¡œ TextGrid íŒŒì¼ ê²½ë¡œ ìƒì„±
            if 'audio_name' in beat_item and beat_item['audio_name']:
                audio_name = beat_item['audio_name']
                # audio_nameì—ì„œ íŒŒì¼ëª…ë§Œ ì¶”ì¶œ (ì˜ˆ: "1_wayne_0_103_110.wav" -> "1_wayne_0_103_110") 
                base_name = os.path.splitext(os.path.basename(audio_name))[0]
                
                # TextGrid íŒŒì¼ ê²½ë¡œ ìƒì„± 
                textgrid_path = os.path.join(self.beat2_data_root, "textgrid", f"{base_name}.TextGrid")
                
                # TextGrid íŒŒì¼ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                actual_text = extract_text_from_textgrid(textgrid_path)
                
            else:
                actual_text = "Gesture movement"  # ê¸°ë³¸ê°’
                
        except Exception as e:
            print(f"Warning: Failed to extract text for index {index}: {e}")
            actual_text = "Gesture movement"
        
        # ============================================================================
        # íƒœìŠ¤í¬ë³„ ì²˜ë¦¬ ë¡œì§ (ì‹¤ì œ BEAT2 í…ìŠ¤íŠ¸ ì‚¬ìš©)
        # ============================================================================
        if task in ['t2g', 'g2t']:  # í…ìŠ¤íŠ¸-ì œìŠ¤ì²˜ íƒœìŠ¤í¬ë“¤
            # ì‹¤ì œ BEAT2 í…ìŠ¤íŠ¸ ë˜ëŠ” ì œìŠ¤ì²˜ ì„¤ëª… ì‚¬ìš©
            if actual_text and actual_text.strip():
                prompt = actual_text  # ì‹¤ì œ BEAT2 í…ìŠ¤íŠ¸ ì‚¬ìš©
            else:
                prompt = np.random.choice(self.gesture_prompts)  # ë°±ì—…ìœ¼ë¡œ ë”ë¯¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            prompt2 = prompt  # ë‘ ì¸ì½”ë” ëª¨ë‘ì— ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            has_text = True  # í…ìŠ¤íŠ¸ ì‚¬ìš©
            has_gesture = True  # ì œìŠ¤ì²˜ ì‚¬ìš©
            has_audio = False  # ì˜¤ë””ì˜¤ ë¯¸ì‚¬ìš©
            
        elif task in ['a2g', 'g2a']:  # ì˜¤ë””ì˜¤-ì œìŠ¤ì²˜ íƒœìŠ¤í¬ë“¤
            # ì˜¤ë””ì˜¤-ì œìŠ¤ì²˜ íƒœìŠ¤í¬
            prompt = ""  # ìˆœìˆ˜ ì˜¤ë””ì˜¤-ì œìŠ¤ì²˜ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ì—†ìŒ
            prompt2 = ""
            has_text = False  # í…ìŠ¤íŠ¸ ë¯¸ì‚¬ìš©
            has_gesture = True  # ì œìŠ¤ì²˜ ì‚¬ìš©
            has_audio = True  # ì˜¤ë””ì˜¤ ì‚¬ìš©
            
        elif task == 't2a':  # í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ íƒœìŠ¤í¬ (OmniFlowì—ì„œ ê°€ì ¸ì˜´)
            # í…ìŠ¤íŠ¸-ì˜¤ë””ì˜¤ íƒœìŠ¤í¬ - ì‹¤ì œ BEAT2 í…ìŠ¤íŠ¸ ì‚¬ìš©
            if actual_text and actual_text.strip():
                prompt = actual_text  # ì‹¤ì œ BEAT2 í…ìŠ¤íŠ¸ ì‚¬ìš©
            else:
                prompt = "Speaking"  # ë°±ì—… ì˜¤ë””ì˜¤ ì„¤ëª…
            prompt2 = prompt
            has_text = True  # í…ìŠ¤íŠ¸ ì‚¬ìš©
            has_gesture = False  # ì œìŠ¤ì²˜ ë¯¸ì‚¬ìš©
            has_audio = True  # ì˜¤ë””ì˜¤ ì‚¬ìš©
            
        elif task == 'a2t':  # ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ íƒœìŠ¤í¬ (OmniFlowì—ì„œ ê°€ì ¸ì˜´)
            # ì˜¤ë””ì˜¤-í…ìŠ¤íŠ¸ íƒœìŠ¤í¬
            prompt = ""  # ì˜¤ë””ì˜¤ì—ì„œ ìƒì„±ë  í…ìŠ¤íŠ¸
            prompt2 = ""
            has_text = True  # í…ìŠ¤íŠ¸ ì‚¬ìš©
            has_gesture = False  # ì œìŠ¤ì²˜ ë¯¸ì‚¬ìš©
            has_audio = True  # ì˜¤ë””ì˜¤ ì‚¬ìš©
            if hasattr(self.beat_args, 'text_descriptions') and index < len(self.beat_args.text_descriptions):
                prompt = self.beat_args.text_descriptions[index]
            else:
                # ê¸°ë³¸ ì œìŠ¤ì²˜ ì„¤ëª… ì‚¬ìš©
                prompt = np.random.choice(self.gesture_prompts)
            prompt2 = prompt
        
        # ============================================================================
        # ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©)
        # ============================================================================
        gesture_sequence = self._process_gesture_data(pose, facial, trans, trans_v)  # ì œìŠ¤ì²˜ ë°ì´í„° ì²˜ë¦¬
        
        # ============================================================================
        # ì‹¤ì œ BEAT2 ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ìœ„í•œ ì˜¤ë””ì˜¤ ì²˜ë¦¬
        # ============================================================================
        audio_vae_input = torch.zeros(1, 1, 1024, 64)  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        audio_clip_input = torch.zeros(1, 3, 112, 1036)  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
        
        if has_audio:  # ì˜¤ë””ì˜¤ê°€ í•„ìš”í•œ ê²½ìš°
            try:
                # BEAT2 ë°ì´í„°ì—ì„œ ì‹¤ì œ ì˜¤ë””ì˜¤ íŠ¹ì§• ì‚¬ìš©
                if audio_features is not None and audio_features.shape[0] > 0:
                    # ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ VAE ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    if len(audio_features.shape) == 2:  # (T, audio_dim)
                        # í•„ìš”í•œ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ/íŒ¨ë”©
                        target_length = 1024
                        if audio_features.shape[0] < target_length:
                            # íŒ¨ë”©
                            pad_length = target_length - audio_features.shape[0]
                            audio_features = torch.cat([audio_features, torch.zeros(pad_length, audio_features.shape[1])], dim=0)
                        elif audio_features.shape[0] > target_length:
                            # íŠ¸ë ì¼€ì´íŠ¸
                            audio_features = audio_features[:target_length]
                        
                        # VAE ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜: (1, 1, 1024, audio_dim) -> (1, 1, 1024, 64)
                        if audio_features.shape[1] != 64:
                            if audio_features.shape[1] > 64:
                                audio_vae_input = audio_features[:, :64].unsqueeze(0).unsqueeze(0)
                            else:
                                # íŒ¨ë”©
                                pad_width = 64 - audio_features.shape[1]
                                padded_features = torch.cat([audio_features, torch.zeros(audio_features.shape[0], pad_width)], dim=1)
                                audio_vae_input = padded_features.unsqueeze(0).unsqueeze(0)
                        else:
                            audio_vae_input = audio_features.unsqueeze(0).unsqueeze(0)
                
                # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ CLIP ì˜¤ë””ì˜¤ ì²˜ë¦¬ë„ ì‹œë„
                if audio_name and os.path.exists(audio_name) and self.audio_processor_clip is not None:
                    audio_clip_input = self.audio_processor_clip([audio_name])['pixel_values']
                    
            except Exception as e:
                logger.warning(f"BEAT2 audio processing failed for {audio_name}: {e}")  # ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê²½ê³  ë¡œê·¸
                
        # ============================================================================
        # ìµœì¢… ë°ì´í„° ì•„ì´í…œ ë°˜í™˜ (ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©)
        # ============================================================================
        return {
            'gesture_sequence': gesture_sequence,    # ì‹¤ì œ BEAT2 ì œìŠ¤ì²˜ ë°ì´í„° (NPZ íŒŒì¼ì—ì„œ)
            'image': torch.zeros(3, self.size, self.size),  # í˜¸í™˜ì„±ì„ ìœ„í•œ ì œë¡œ ì´ë¯¸ì§€ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            'image_clip': torch.zeros(1, 3, 224, 224),      # í˜¸í™˜ì„±ì„ ìœ„í•œ ì œë¡œ ì´ë¯¸ì§€ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
            'caption': prompt,                      # ì‹¤ì œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (TextGridì—ì„œ ì¶”ì¶œ ë˜ëŠ” ìƒì„±)
            'caption2': prompt2,                    # ì‹¤ì œ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ 2
            'audio': audio_vae_input,              # ì‹¤ì œ BEAT2 ì˜¤ë””ì˜¤ íŠ¹ì§• (WAV íŒŒì¼ì—ì„œ)
            'audio_clip': audio_clip_input,        # ì‹¤ì œ BEAT2 ì˜¤ë””ì˜¤ CLIP íŠ¹ì§•
            'task': task,                          # íƒœìŠ¤í¬ íƒ€ì… (t2g, a2g, g2t, g2a, t2a, a2t)
            'has_gesture': has_gesture,            # ì œìŠ¤ì²˜ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
            'has_image': False,                    # ì´ë¯¸ì§€ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (ì œìŠ¤ì²˜ë¡œ ëŒ€ì²´)
            'has_audio': has_audio,                # ì˜¤ë””ì˜¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
            'has_caption': has_text,               # í…ìŠ¤íŠ¸ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
            'dataset': f'beat2_gesture_{task}',    # BEAT2 ë°ì´í„°ì…‹ ì‹ë³„ì
            'weight': [1.0, 1.0],                 # íƒœìŠ¤í¬ ê°€ì¤‘ì¹˜
            # BEAT2 ì›ë³¸ ë°ì´í„° ì¶”ê°€ ì •ë³´
            'beat2_metadata': {
                'audio_name': audio_name,
                'word_features_shape': word_features.shape if word_features is not None else None,
                'audio_features_shape': audio_features.shape if audio_features is not None else None,
                'pose_shape': pose.shape,
                'facial_shape': facial.shape,
                'trans_shape': trans.shape,
                'trans_v_shape': trans_v.shape,
            }
        }
        
    def _process_gesture_data(self, pose, facial, trans, trans_v):
        """
        ê²€ì¦ëœ ì ì‘í˜• ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì œìŠ¤ì²˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
        
        Args:
            pose: í¬ì¦ˆ ë°ì´í„° (T, pose_dim)
            facial: ì–¼êµ´ ë°ì´í„° (T, 100)
            trans: ë³€í™˜ ë°ì´í„° (T, 3)
            trans_v: ë³€í™˜ ì†ë„ ë°ì´í„° (T, 3)
        
        Returns:
            full_gesture: ì²˜ë¦¬ëœ ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ (T, 415)
        """
        # ============================================================================
        # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        # ============================================================================
        pose = pose.unsqueeze(0)      # (1, T, pose_dim) - ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        facial = facial.unsqueeze(0)  # (1, T, 100) - ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        trans = trans.unsqueeze(0)    # (1, T, 3) - ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        trans_v = trans_v.unsqueeze(0) # (1, T, 3) - ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        
        B, T, pose_dim = pose.shape  # ë°°ì¹˜, ì‹œí€€ìŠ¤ ê¸¸ì´, í¬ì¦ˆ ì°¨ì› ì¶”ì¶œ
        
        # ============================================================================
        # ê²€ì¦ëœ ì ì‘í˜• ë¶„í•  ë°©ë²• ì‚¬ìš©
        # ============================================================================
        upper_end = int(pose_dim * 0.4)  # ìƒì²´ ë ì¸ë±ìŠ¤ (í¬ì¦ˆ ì°¨ì›ì˜ 40%)
        hands_start = upper_end  # ì† ì‹œì‘ ì¸ë±ìŠ¤ (ìƒì²´ ëê³¼ ë™ì¼)
        hands_end = int(pose_dim * 0.8)  # ì† ë ì¸ë±ìŠ¤ (í¬ì¦ˆ ì°¨ì›ì˜ 80%)
        
        upper_pose = pose[:, :, :upper_end]  # ìƒì²´ í¬ì¦ˆ ì¶”ì¶œ
        hands_pose = pose[:, :, hands_start:hands_end]  # ì† í¬ì¦ˆ ì¶”ì¶œ
        lower_pose = pose[:, :, hands_end:]  # í•˜ì²´ í¬ì¦ˆ ì¶”ì¶œ
        
        # ============================================================================
        # í•˜ì²´ì™€ ë³€í™˜ ì†ë„ ê²°í•©
        # ============================================================================
        lower_trans = torch.cat([lower_pose, trans_v], dim=-1)  # í•˜ì²´ í¬ì¦ˆì™€ ë³€í™˜ ì†ë„ë¥¼ ë§ˆì§€ë§‰ ì°¨ì›ìœ¼ë¡œ ê²°í•©
        
        # ============================================================================
        # ì •í™•í•œ RVQVAE ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ íŒ¨ë”©
        # ============================================================================
        upper_pose = F.pad(upper_pose, (0, max(0, 78 - upper_pose.shape[-1])))[:, :, :78]  # ìƒì²´ë¥¼ 78ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        hands_pose = F.pad(hands_pose, (0, max(0, 180 - hands_pose.shape[-1])))[:, :, :180]  # ì†ì„ 180ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        lower_trans = F.pad(lower_trans, (0, max(0, 57 - lower_trans.shape[-1])))[:, :, :57]  # í•˜ì²´+ë³€í™˜ì„ 57ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        face_data = F.pad(facial, (0, max(0, 100 - facial.shape[-1])))[:, :, :100]  # ì–¼êµ´ì„ 100ì°¨ì›ìœ¼ë¡œ íŒ¨ë”©
        
        # ============================================================================
        # ëª¨ë“  ë¶€ìœ„ ê²°í•©
        # ============================================================================
        full_gesture = torch.cat([  # ëª¨ë“  ë¶€ìœ„ë¥¼ ë§ˆì§€ë§‰ ì°¨ì›ìœ¼ë¡œ ê²°í•©
            upper_pose,      # 78ì°¨ì› - ìƒì²´
            hands_pose,      # 180ì°¨ì› - ì†
            lower_trans,     # 57ì°¨ì› - í•˜ì²´ + ë³€í™˜
            face_data        # 100ì°¨ì› - ì–¼êµ´
        ], dim=-1)  # (1, T, 415) - ì´ 415ì°¨ì›
        
        return full_gesture.squeeze(0)  # (T, 415) - ë°°ì¹˜ ì°¨ì› ì œê±°í•˜ì—¬ ë°˜í™˜


def omniges_collate_fn(examples):
    """
    Omnigesì— ë§ê²Œ ì¡°ì •ëœ ë°°ì¹˜ ìˆ˜ì§‘ í•¨ìˆ˜
    """
    # ============================================================================
    # ì´ë¯¸ì§€ ëŒ€ì‹  ì œìŠ¤ì²˜ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    # ============================================================================
    gesture_sequences = [example["gesture_sequence"] for example in examples]  # ëª¨ë“  ì˜ˆì œì—ì„œ ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ ì¶”ì¶œ
    gesture_sequences = torch.stack([torch.nn.functional.pad(seq, (0, 0, 0, 128 - seq.shape[0])) for seq in gesture_sequences])  # 128 í”„ë ˆì„ìœ¼ë¡œ íŒ¨ë”©í•˜ì—¬ ìŠ¤íƒ
    
    # ============================================================================
    # OmniFlow ë¡œì§ê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ë”ë¯¸ ì´ë¯¸ì§€ ìœ ì§€
    # ============================================================================
    pixel_values = [example["image"] for example in examples]  # ëª¨ë“  ì˜ˆì œì—ì„œ ë”ë¯¸ ì´ë¯¸ì§€ ì¶”ì¶œ
    clip_values = torch.cat([example["image_clip"] for example in examples])  # CLIPìš© ë”ë¯¸ ì´ë¯¸ì§€ ì—°ê²°
    
    prompts = list([example["caption"] for example in examples])  # ëª¨ë“  ì˜ˆì œì—ì„œ í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
    prompts2 = list([example["caption2"] for example in examples])  # ëª¨ë“  ì˜ˆì œì—ì„œ í”„ë¡¬í”„íŠ¸2 ì¶”ì¶œ

    pixel_values = torch.stack(pixel_values)  # ë”ë¯¸ ì´ë¯¸ì§€ë“¤ì„ ìŠ¤íƒ
    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()  # ë©”ëª¨ë¦¬ í˜•ì‹ì„ ì—°ì†ìœ¼ë¡œ ë³€ê²½í•˜ê³  floatë¡œ ë³€í™˜

    audio = torch.cat([example["audio"] for example in examples])  # ëª¨ë“  ì˜ˆì œì—ì„œ ì˜¤ë””ì˜¤ ì—°ê²°
    audio_clip = torch.cat([example["audio_clip"] for example in examples])  # ëª¨ë“  ì˜ˆì œì—ì„œ CLIP ì˜¤ë””ì˜¤ ì—°ê²°
    
    # ============================================================================
    # ì²« ë²ˆì§¸ ì˜ˆì œì—ì„œ íƒœìŠ¤í¬ ê²°ì •
    # ============================================================================
    task_type = examples[0]['task']  # ì²« ë²ˆì§¸ ì˜ˆì œì˜ íƒœìŠ¤í¬ íƒ€ì… ê°€ì ¸ì˜¤ê¸°
    
    # ============================================================================
    # Omniges íƒœìŠ¤í¬ë¥¼ OmniFlow í˜¸í™˜ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘ (ë‚´ë¶€ ì²˜ë¦¬ìš©)
    # ============================================================================
    task_mapping = {
        't2g': 'text2img',   # í…ìŠ¤íŠ¸ â†’ ì œìŠ¤ì²˜ â†’ í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ (ë‚´ë¶€)
        'g2t': 'img2text',   # ì œìŠ¤ì²˜ â†’ í…ìŠ¤íŠ¸ â†’ ì´ë¯¸ì§€ â†’ í…ìŠ¤íŠ¸ (ë‚´ë¶€)
        'a2g': 'aud2img',    # ì˜¤ë””ì˜¤ â†’ ì œìŠ¤ì²˜ â†’ ì˜¤ë””ì˜¤ â†’ ì´ë¯¸ì§€ (ë‚´ë¶€)
        'g2a': 'img2aud',    # ì œìŠ¤ì²˜ â†’ ì˜¤ë””ì˜¤ â†’ ì´ë¯¸ì§€ â†’ ì˜¤ë””ì˜¤ (ë‚´ë¶€)
        't2a': 'text2aud',   # í…ìŠ¤íŠ¸ â†’ ì˜¤ë””ì˜¤ (ë™ì¼)
        'a2t': 'aud2text'    # ì˜¤ë””ì˜¤ â†’ í…ìŠ¤íŠ¸ (ë™ì¼)
    }
    
    task = task_mapping[task_type]  # íƒœìŠ¤í¬ íƒ€ì…ì„ OmniFlow í˜¸í™˜ ì´ë¦„ìœ¼ë¡œ ë³€í™˜
    
    # ============================================================================
    # ì œìŠ¤ì²˜ íƒœìŠ¤í¬ì— ë§ê²Œ ì¡°ì •ëœ ë“œë¡­ì•„ì›ƒ ë¡œì§
    # ============================================================================
    drop_img = drop_text = drop_aud = None  # ë“œë¡­ì•„ì›ƒ ì¸ë±ìŠ¤ ì´ˆê¸°í™”
    
    if task in ['text2img', 'text2aud']:  # t2g, t2a íƒœìŠ¤í¬
        drop_text = (np.random.rand(len(prompts)) < 0.15).nonzero()[0]  # 15% í™•ë¥ ë¡œ í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒ
    elif task in ['img2text', 'img2aud']:  # g2t, g2a íƒœìŠ¤í¬
        drop_img = (np.random.rand(len(prompts)) < 0.15).nonzero()[0]  # 15% í™•ë¥ ë¡œ ì´ë¯¸ì§€(ì œìŠ¤ì²˜) ë“œë¡­ì•„ì›ƒ
    elif task in ['aud2text', 'aud2img']:  # a2t, a2g íƒœìŠ¤í¬
        drop_aud = (np.random.rand(len(prompts)) < 0.15).nonzero()[0]  # 15% í™•ë¥ ë¡œ ì˜¤ë””ì˜¤ ë“œë¡­ì•„ì›ƒ
    
    # ============================================================================
    # ìµœì¢… ë°°ì¹˜ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±
    # ============================================================================
    batch = {
        "pixel_values": pixel_values,      # ë”ë¯¸ ì´ë¯¸ì§€ë“¤ (ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ë¡œ ëŒ€ì²´ë¨)
        "gesture_sequences": gesture_sequences,  # ìƒˆë¡œìš´: ì‹¤ì œ ì œìŠ¤ì²˜ ë°ì´í„°
        "prompts": prompts,  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        "prompts2": prompts2,  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸2ë“¤
        "task": task,  # OmniFlow í˜¸í™˜ íƒœìŠ¤í¬ ì´ë¦„
        "task_type": task_type,           # ìƒˆë¡œìš´: ì›ë³¸ íƒœìŠ¤í¬ íƒ€ì…
        "clip_values": clip_values,  # CLIPìš© ë”ë¯¸ ì´ë¯¸ì§€ë“¤
        "audio": audio,  # ì˜¤ë””ì˜¤ ë°ì´í„°
        "audio_clip": audio_clip,  # CLIPìš© ì˜¤ë””ì˜¤ ë°ì´í„°
        'drop_img': drop_img,  # ì´ë¯¸ì§€(ì œìŠ¤ì²˜) ë“œë¡­ì•„ì›ƒ ì¸ë±ìŠ¤
        'drop_aud': drop_aud,  # ì˜¤ë””ì˜¤ ë“œë¡­ì•„ì›ƒ ì¸ë±ìŠ¤
        "drop_text": drop_text,  # í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒ ì¸ë±ìŠ¤
        "name": examples[0]['dataset'],  # ë°ì´í„°ì…‹ ì´ë¦„
    }

    return batch  # ìµœì¢… ë°°ì¹˜ ë°˜í™˜


def prepare_omniges_inputs(
    transformer, args, text_encoder_one, text_encoder_two, text_encoder_three,
    device, batch, gesture_vae, tokenizer_three, text_encoders, tokenizers,
    tokenizer_one, tokenizer_two, weight_dtype, noise_scheduler_copy,
    noise_scheduler, audio_vae_factor, audiovae, text_vae_tokenizer,
    text_vae, audio_encoder, anchor=False, mm_encoder=None,
):
    """
    Omniges í›ˆë ¨ì„ ìœ„í•œ ì…ë ¥ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” í•¨ìˆ˜
    OmniFlowì˜ prepare_inputsë¥¼ ì œìŠ¤ì²˜ ì²˜ë¦¬ì— ë§ê²Œ ì ì‘
    
    Args:
        transformer: OmnigesFlowTransformerModel
        args: í›ˆë ¨ ì¸ìë“¤
        text_encoder_one/two/three: í…ìŠ¤íŠ¸ ì¸ì½”ë”ë“¤
        device: ê³„ì‚° ë””ë°”ì´ìŠ¤
        batch: ë°ì´í„° ë°°ì¹˜
        gesture_vae: ì œìŠ¤ì²˜ VAE
        tokenizer_three: í† í¬ë‚˜ì´ì €
        text_encoders: í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¦¬ìŠ¤íŠ¸
        tokenizers: í† í¬ë‚˜ì´ì € ë¦¬ìŠ¤íŠ¸
        tokenizer_one/two: ì¶”ê°€ í† í¬ë‚˜ì´ì €ë“¤
        weight_dtype: ê°€ì¤‘ì¹˜ ë°ì´í„° íƒ€ì…
        noise_scheduler_copy: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸
        noise_scheduler: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬
        audio_vae_factor: ì˜¤ë””ì˜¤ VAE íŒ©í„°
        audiovae: ì˜¤ë””ì˜¤ VAE
        text_vae_tokenizer: í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì €
        text_vae: í…ìŠ¤íŠ¸ VAE
        audio_encoder: ì˜¤ë””ì˜¤ ì¸ì½”ë”
        anchor: ì•µì»¤ í”Œë˜ê·¸
        mm_encoder: ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”
    
    Returns:
        tuple: í›ˆë ¨ì— í•„ìš”í•œ ëª¨ë“  ì…ë ¥ ë°ì´í„°ë“¤
    """
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ì¶”ë¡  ëª¨ë“œ)
        models_to_accumulate = [transformer]  # ëˆ„ì í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (í˜„ì¬ëŠ” transformerë§Œ)

        # ============================================================================
        # ë°°ì¹˜ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        # ============================================================================
        task = batch['task']  # OmniFlow í˜¸í™˜ íƒœìŠ¤í¬ ì´ë¦„ (text2img, img2text ë“±)
        task_type = batch['task_type']  # ì›ë³¸ Omniges íƒœìŠ¤í¬ íƒ€ì… (t2g, g2t ë“±)
        
        # ============================================================================
        # ì´ë¯¸ì§€ ë°ì´í„° ëŒ€ì‹  ì œìŠ¤ì²˜ ë°ì´í„° ì²˜ë¦¬
        # ============================================================================
        gesture_sequences = batch["gesture_sequences"]  # ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ (B, T, 415)
        prompts = np.array(batch["prompts"])  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        prompts2 = np.array(batch["prompts2"])  # í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸2ë“¤ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
        
        # ============================================================================
        # ë””ë²„ê·¸ ì¶œë ¥ - ë°ì´í„°ë¡œë” ì¶œë ¥ ì •ë³´
        # ============================================================================
        print(f"DEBUG: ========== DATALOADER OUTPUT ==========")  # ë°ì´í„°ë¡œë” ì¶œë ¥ ì„¹ì…˜ ì‹œì‘
        print(f"DEBUG: task: {task}, task_type: {task_type}")  # íƒœìŠ¤í¬ ì •ë³´ ì¶œë ¥
        print(f"DEBUG: gesture_sequences shape: {gesture_sequences.shape}")  # ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: prompts count: {len(prompts)}")  # í”„ë¡¬í”„íŠ¸ ê°œìˆ˜ ì¶œë ¥
        print(f"DEBUG: prompts2 count: {len(prompts2)}")  # í”„ë¡¬í”„íŠ¸2 ê°œìˆ˜ ì¶œë ¥
        if 'audio' in batch:  # ë°°ì¹˜ì— ì˜¤ë””ì˜¤ê°€ ìˆëŠ” ê²½ìš°
            print(f"DEBUG: audio shape: {batch['audio'].shape}")  # ì˜¤ë””ì˜¤ í˜•íƒœ ì¶œë ¥
        else:  # ë°°ì¹˜ì— ì˜¤ë””ì˜¤ê°€ ì—†ëŠ” ê²½ìš°
            print(f"DEBUG: No audio in batch")  # ì˜¤ë””ì˜¤ ì—†ìŒ ë©”ì‹œì§€ ì¶œë ¥
    
        bsz = len(prompts)  # ë°°ì¹˜ í¬ê¸°ë¥¼ í”„ë¡¬í”„íŠ¸ ê°œìˆ˜ë¡œ ì„¤ì •
        
        # ============================================================================
        # ì œìŠ¤ì²˜ VAEë¥¼ ì‚¬ìš©í•˜ì—¬ ì œìŠ¤ì²˜ë¥¼ ì ì¬ ë³€ìˆ˜ë¡œ ì¸ì½”ë”©
        # ============================================================================
        print(f"DEBUG: ========== GESTURE VAE ENCODING ==========")  # ì œìŠ¤ì²˜ VAE ì¸ì½”ë”© ì„¹ì…˜ ì‹œì‘
        gesture_latents_dist = gesture_vae.encode(gesture_sequences.to(device))  # ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™í•˜ì—¬ VAE ì¸ì½”ë”©
        model_input = gesture_latents_dist.sample()  # ë¶„í¬ì—ì„œ ìƒ˜í”Œë§í•˜ì—¬ (B, C, H, W) í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print(f"DEBUG: gesture VAE output shape: {model_input.shape}")  # ì œìŠ¤ì²˜ VAE ì¶œë ¥ í˜•íƒœ ì¶œë ¥
        
        # ============================================================================
        # GestureVAEëŠ” ì´ì œ (B, 512, T, 1)ì„ ì§ì ‘ ì¶œë ¥í•´ì•¼ í•¨ - ë¦¬ì…°ì´í•‘ ë¶ˆí•„ìš”
        # ============================================================================
        B, C, H, W = model_input.shape  # ë°°ì¹˜, ì±„ë„, ë†’ì´, ë„ˆë¹„ ì¶”ì¶œ
        print(f"DEBUG: Gesture VAE output - B:{B}, C:{C}, H:{H}, W:{W}")  # ì œìŠ¤ì²˜ VAE ì¶œë ¥ ì°¨ì› ì •ë³´ ì¶œë ¥
        
        # ============================================================================
        # ì˜ˆìƒ í˜•ì‹ ê²€ì¦
        # ============================================================================
        if C == 512 and W == 1:  # ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ ê²½ìš° (4ê°œ ë¶€ìœ„ ê²°í•©)
            print(f"DEBUG: âœ“ Correct format (B, 512, T, 1) - 4 parts concatenated")  # ì˜¬ë°”ë¥¸ í˜•ì‹ í™•ì¸ ë©”ì‹œì§€
        elif C == 128 and W == 4:  # ë ˆê±°ì‹œ í˜•ì‹ì¸ ê²½ìš° (4ê°œ ë¶€ìœ„ ìŠ¤íƒ)
            print(f"DEBUG: âš  Legacy format (B, 128, T, 4) - converting to concat format")  # ë ˆê±°ì‹œ í˜•ì‹ ë³€í™˜ ë©”ì‹œì§€
            # ë³€í™˜: (B, 128, T, 4) -> (B, T, 128, 4) -> (B, T, 512) -> (B, 512, T, 1)
            model_input = model_input.permute(0, 2, 1, 3)  # ì°¨ì› ìˆœì„œ ë³€ê²½ (B, T, 128, 4)
            model_input = model_input.reshape(B, H, C * W)  # í˜•íƒœ ë³€ê²½ (B, T, 512)
            model_input = model_input.permute(0, 2, 1).unsqueeze(-1)  # ì°¨ì› ìˆœì„œ ë³€ê²½ í›„ ë§ˆì§€ë§‰ ì°¨ì› ì¶”ê°€ (B, 512, T, 1)
            print(f"DEBUG: After conversion - model_input shape: {model_input.shape}")  # ë³€í™˜ í›„ í˜•íƒœ ì¶œë ¥
        else:  # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ì¸ ê²½ìš°
            print(f"DEBUG: âš  Unexpected format - C:{C}, W:{W}")  # ì˜ˆìƒì¹˜ ëª»í•œ í˜•ì‹ ê²½ê³ 
            print(f"DEBUG: Current model_input shape: {model_input.shape}")  # í˜„ì¬ í˜•íƒœ ì¶œë ¥
            
        model_input = model_input * gesture_vae.config.scaling_factor  # VAE ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì ìš©
        model_input = model_input.to(dtype=weight_dtype)  # ê°€ì¤‘ì¹˜ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        print(f"DEBUG: After scaling and dtype - model_input shape: {model_input.shape}")  # ìŠ¤ì¼€ì¼ë§ ë° ë°ì´í„° íƒ€ì… ë³€í™˜ í›„ í˜•íƒœ ì¶œë ¥

        # ============================================================================
        # ì˜¤ë””ì˜¤ ì…ë ¥ (OmniFlowì™€ ë™ì¼)
        # ============================================================================
        print(f"DEBUG: ========== AUDIO VAE ENCODING ==========")  # ì˜¤ë””ì˜¤ VAE ì¸ì½”ë”© ì„¹ì…˜ ì‹œì‘
        raw_audio_embeds = batch['audio'].to(model_input.device)  # ë°°ì¹˜ì˜ ì˜¤ë””ì˜¤ë¥¼ ëª¨ë¸ ì…ë ¥ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        print(f"DEBUG: Raw audio input shape: {raw_audio_embeds.shape}")  # ì›ë³¸ ì˜¤ë””ì˜¤ ì…ë ¥ í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: Raw audio input dtype: {raw_audio_embeds.dtype}")  # ì›ë³¸ ì˜¤ë””ì˜¤ ì…ë ¥ ë°ì´í„° íƒ€ì… ì¶œë ¥
        print(f"DEBUG: Raw audio input device: {raw_audio_embeds.device}")  # ì›ë³¸ ì˜¤ë””ì˜¤ ì…ë ¥ ë””ë°”ì´ìŠ¤ ì¶œë ¥
        
        # ============================================================================
        # ì˜¤ë””ì˜¤ VAE ì¸ì½”ë”©
        # ============================================================================
        audio_latent_dist = audiovae.encode(raw_audio_embeds.float())  # ì˜¤ë””ì˜¤ë¥¼ floatë¡œ ë³€í™˜í•˜ì—¬ VAE ì¸ì½”ë”©
        print(f"DEBUG: Audio VAE latent_dist type: {type(audio_latent_dist)}")  # ì˜¤ë””ì˜¤ VAE ì ì¬ ë¶„í¬ íƒ€ì… ì¶œë ¥
        
        # ============================================================================
        # AutoencoderKLOutputì„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬
        # ============================================================================
        if hasattr(audio_latent_dist, 'latents'):  # latents ì†ì„±ì´ ìˆëŠ” ê²½ìš°
            # AutoencoderKLOutputì˜ ê°€ì¥ ì¼ë°˜ì ì¸ ê²½ìš°
            raw_audio_embeds = audio_latent_dist.latents  # latents ì†ì„±ì—ì„œ ì ì¬ ë³€ìˆ˜ ì¶”ì¶œ
            print(f"DEBUG: Audio VAE latents shape (via .latents): {raw_audio_embeds.shape}")  # latentsë¥¼ í†µí•œ í˜•íƒœ ì¶œë ¥
        elif hasattr(audio_latent_dist, 'latent_dist'):  # latent_dist ì†ì„±ì´ ìˆëŠ” ê²½ìš°
            raw_audio_embeds = audio_latent_dist.latent_dist.sample()  # ì ì¬ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§
            print(f"DEBUG: Audio VAE sample shape (via latent_dist): {raw_audio_embeds.shape}")  # latent_distë¥¼ í†µí•œ í˜•íƒœ ì¶œë ¥
        elif hasattr(audio_latent_dist, 'sample'):  # sample ë©”ì„œë“œê°€ ìˆëŠ” ê²½ìš°
            raw_audio_embeds = audio_latent_dist.sample()  # ì§ì ‘ ìƒ˜í”Œë§
            print(f"DEBUG: Audio VAE sample shape (direct): {raw_audio_embeds.shape}")  # ì§ì ‘ ìƒ˜í”Œë§ í˜•íƒœ ì¶œë ¥
        else:  # ìœ„ì˜ ê²½ìš°ê°€ ëª¨ë‘ ì•„ë‹Œ ê²½ìš°
            # í´ë°± - ì´ë¯¸ ì ì¬ í…ì„œë¼ê³  ê°€ì •
            raw_audio_embeds = audio_latent_dist  # ì§ì ‘ ì‚¬ìš©
            print(f"DEBUG: Audio VAE direct tensor shape: {raw_audio_embeds.shape}")  # ì§ì ‘ í…ì„œ í˜•íƒœ ì¶œë ¥
            
        print(f"DEBUG: Audio latent attributes: {[attr for attr in dir(audio_latent_dist) if not attr.startswith('_')]}")  # ì˜¤ë””ì˜¤ ì ì¬ ë¶„í¬ì˜ ëª¨ë“  ì†ì„± ì¶œë ¥
        
        # ============================================================================
        # ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì ìš©
        # ============================================================================
        raw_audio_embeds = raw_audio_embeds.mul_(audiovae.config.scaling_factor)  # ì˜¤ë””ì˜¤ VAE ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì ìš© (in-place)
        print(f"DEBUG: Audio VAE scaling factor: {audiovae.config.scaling_factor}")  # ì˜¤ë””ì˜¤ VAE ìŠ¤ì¼€ì¼ë§ íŒ©í„° ì¶œë ¥
        print(f"DEBUG: After scaling - audio shape: {raw_audio_embeds.shape}")  # ìŠ¤ì¼€ì¼ë§ í›„ ì˜¤ë””ì˜¤ í˜•íƒœ ì¶œë ¥
        
        raw_audio_embeds = raw_audio_embeds.to(model_input)  # ëª¨ë¸ ì…ë ¥ê³¼ ë™ì¼í•œ ë””ë°”ì´ìŠ¤/ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        print(f"DEBUG: Final audio embeds shape: {raw_audio_embeds.shape}")  # ìµœì¢… ì˜¤ë””ì˜¤ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: Final audio embeds dtype: {raw_audio_embeds.dtype}")  # ìµœì¢… ì˜¤ë””ì˜¤ ì„ë² ë”© ë°ì´í„° íƒ€ì… ì¶œë ¥
        
        # ============================================================================
        # ì„œë¡œ ë‹¤ë¥¸ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ìœ„í•œ ë…¸ì´ì¦ˆ ìƒ˜í”Œë§
        # ============================================================================
        bsz = model_input.shape[0]  # ë°°ì¹˜ í¬ê¸°ë¥¼ ëª¨ë¸ ì…ë ¥ì—ì„œ ì¶”ì¶œ
        add_token_embed = True  # í† í° ì„ë² ë”© ì¶”ê°€ í”Œë˜ê·¸ í™œì„±í™”
        
        # ============================================================================
        # 3ê°œ ëª¨ë‹¬ë¦¬í‹°ë¥¼ ìœ„í•œ íƒ€ì„ìŠ¤í… ìƒ˜í”Œë§
        # ============================================================================
        print(f"DEBUG: ========== TIMESTEP SAMPLING ==========")  # íƒ€ì„ìŠ¤í… ìƒ˜í”Œë§ ì„¹ì…˜ ì‹œì‘
        print(f"DEBUG: batch size: {bsz}, total timesteps to sample: {bsz * 3}")  # ë°°ì¹˜ í¬ê¸°ì™€ ì´ ìƒ˜í”Œë§í•  íƒ€ì„ìŠ¤í… ìˆ˜ ì¶œë ¥
        u = compute_density_for_timestep_sampling(  # íƒ€ì„ìŠ¤í… ìƒ˜í”Œë§ì„ ìœ„í•œ ë°€ë„ ê³„ì‚°
            weighting_scheme=args.weighting_scheme,  # ê°€ì¤‘ì¹˜ ìŠ¤í‚¤ë§ˆ
            batch_size=bsz * 3,  # ë°°ì¹˜ í¬ê¸° (3ê°œ ëª¨ë‹¬ë¦¬í‹°)
            logit_mean=args.logit_mean,  # ë¡œì§“ í‰ê· 
            logit_std=args.logit_std,  # ë¡œì§“ í‘œì¤€í¸ì°¨
            mode_scale=args.mode_scale,  # ëª¨ë“œ ìŠ¤ì¼€ì¼
        )
        indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()  # ë°€ë„ë¥¼ íƒ€ì„ìŠ¤í… ì¸ë±ìŠ¤ë¡œ ë³€í™˜
        if args.uniform_flow:  # ê· ë“± í”Œë¡œìš°ê°€ í™œì„±í™”ëœ ê²½ìš°
            indices = torch.randint(  # ê· ë“± ë¶„í¬ì—ì„œ ëœë¤ ì¸ë±ìŠ¤ ìƒì„±
                0, noise_scheduler.config.num_train_timesteps, (bsz*3,), device='cpu', dtype=torch.long
            )
        timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)  # ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” íƒ€ì„ìŠ¤í…ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        print(f"DEBUG: Raw timesteps shape: {timesteps.shape}")  # ì›ë³¸ íƒ€ì„ìŠ¤í… í˜•íƒœ ì¶œë ¥
        timesteps, timesteps_text, timesteps_audio = timesteps.chunk(3)  # íƒ€ì„ìŠ¤í…ì„ 3ê°œ ëª¨ë‹¬ë¦¬í‹°ë¡œ ë¶„í• 
        print(f"DEBUG: Split timesteps - gesture: {timesteps.shape}, text: {timesteps_text.shape}, audio: {timesteps_audio.shape}")  # ë¶„í• ëœ íƒ€ì„ìŠ¤í… í˜•íƒœ ì¶œë ¥
        
        # ============================================================================
        # ê° ëª¨ë‹¬ë¦¬í‹°ì— ëŒ€í•œ ì‹œê·¸ë§ˆ ê°’ ê°€ì ¸ì˜¤ê¸°
        # ============================================================================
        sigmas = n_get_sigmas(noise_scheduler_copy, device, timesteps, n_dim=model_input.ndim, dtype=model_input.dtype)  # ì œìŠ¤ì²˜ìš© ì‹œê·¸ë§ˆ
        sigma_text = n_get_sigmas(noise_scheduler_copy, device, timesteps_text, n_dim=model_input.ndim, dtype=model_input.dtype)  # í…ìŠ¤íŠ¸ìš© ì‹œê·¸ë§ˆ
        sigmas_audio = n_get_sigmas(noise_scheduler_copy, device, timesteps_audio, n_dim=model_input.ndim, dtype=model_input.dtype)  # ì˜¤ë””ì˜¤ìš© ì‹œê·¸ë§ˆ
        print(f"DEBUG: Sigmas shapes - gesture: {sigmas.shape}, text: {sigma_text.shape}, audio: {sigmas_audio.shape}")  # ê° ëª¨ë‹¬ë¦¬í‹° ì‹œê·¸ë§ˆ í˜•íƒœ ì¶œë ¥
        
        # ============================================================================
        # ì„œë¡œ ë‹¤ë¥¸ íƒœìŠ¤í¬ì— ëŒ€í•œ ì†ì‹¤ íŒ©í„°
        # ============================================================================
        loss_text_factor = 1  # í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°
        loss_aud_factor = 1  # ì˜¤ë””ì˜¤ ì†ì‹¤ íŒ©í„°
        loss_gesture_factor = 1  # ì œìŠ¤ì²˜ ì†ì‹¤ íŒ©í„° (ê¸°ì¡´ loss_img_factorì—ì„œ ì´ë¦„ ë³€ê²½)
        
        # ============================================================================
        # íƒœìŠ¤í¬ì— ë”°ë¥¸ ì ì ˆí•œ ì¡°ê±´í™” ì„¤ì •
        # ============================================================================
        can_generate_text = True  # í…ìŠ¤íŠ¸ ìƒì„± ê°€ëŠ¥ í”Œë˜ê·¸
        if np.random.rand() < 0.1:  # 10% í™•ë¥ ë¡œ í…ìŠ¤íŠ¸ ìƒì„± ë¹„í™œì„±í™”
            can_generate_text = False
            
        # ============================================================================
        # íƒœìŠ¤í¬ë³„ ì¡°ê±´í™” (ì œìŠ¤ì²˜ì— ë§ê²Œ ì ì‘)
        # ============================================================================
        if task in ['text2img', 'text2aud']:  # t2g, t2a íƒœìŠ¤í¬
            loss_text_factor = 0  # í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            if np.random.rand() < 0.8:  # 80% í™•ë¥ ë¡œ í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆì™€ íƒ€ì„ìŠ¤í…ì„ 0ìœ¼ë¡œ ì„¤ì •
                sigma_text = sigma_text * 0  # í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                timesteps_text = timesteps_text * 0  # í…ìŠ¤íŠ¸ íƒ€ì„ìŠ¤í…ì„ 0ìœ¼ë¡œ ì„¤ì •
        
        if task in ['img2aud', 'aud2img']:  # g2a, a2g íƒœìŠ¤í¬
            loss_text_factor = 0  # í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            sigma_text = sigma_text * 0 + 1  # í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆë¥¼ 1ë¡œ ì„¤ì •
            timesteps_text = timesteps_text * 0 + 1000  # í…ìŠ¤íŠ¸ íƒ€ì„ìŠ¤í…ì„ 1000ìœ¼ë¡œ ì„¤ì •
            
        if batch['drop_text'] is not None:  # í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
            timesteps_text[batch['drop_text']] = 1000  # ë“œë¡­ëœ í…ìŠ¤íŠ¸ì˜ íƒ€ì„ìŠ¤í…ì„ 1000ìœ¼ë¡œ ì„¤ì •
            sigma_text[batch['drop_text']] = 1  # ë“œë¡­ëœ í…ìŠ¤íŠ¸ì˜ ì‹œê·¸ë§ˆë¥¼ 1ë¡œ ì„¤ì •
        
        if batch['drop_aud'] is not None:  # ì˜¤ë””ì˜¤ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
            timesteps_audio[batch['drop_aud']] = 1000  # ë“œë¡­ëœ ì˜¤ë””ì˜¤ì˜ íƒ€ì„ìŠ¤í…ì„ 1000ìœ¼ë¡œ ì„¤ì •
            sigmas_audio[batch['drop_aud']] = 1  # ë“œë¡­ëœ ì˜¤ë””ì˜¤ì˜ ì‹œê·¸ë§ˆë¥¼ 1ë¡œ ì„¤ì •
            
        if batch['drop_img'] is not None:  # ì´ë¯¸ì§€(ì œìŠ¤ì²˜) ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
            timesteps[batch['drop_img']] = 1000  # ë“œë¡­ëœ ì œìŠ¤ì²˜ì˜ íƒ€ì„ìŠ¤í…ì„ 1000ìœ¼ë¡œ ì„¤ì •
            sigmas[batch['drop_img']] = 1  # ë“œë¡­ëœ ì œìŠ¤ì²˜ì˜ ì‹œê·¸ë§ˆë¥¼ 1ë¡œ ì„¤ì •
            
        if task in ['img2text', 'img2aud']:  # g2t, g2a íƒœìŠ¤í¬
            loss_gesture_factor = 0  # ì œìŠ¤ì²˜ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            if np.random.rand() < 0.8:  # 80% í™•ë¥ ë¡œ ì œìŠ¤ì²˜ ì‹œê·¸ë§ˆì™€ íƒ€ì„ìŠ¤í…ì„ 0ìœ¼ë¡œ ì„¤ì •
                sigmas = sigmas * 0  # ì œìŠ¤ì²˜ ì‹œê·¸ë§ˆë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                timesteps = timesteps * 0  # ì œìŠ¤ì²˜ íƒ€ì„ìŠ¤í…ì„ 0ìœ¼ë¡œ ì„¤ì •
                
        if task in ['text2aud', 'aud2text']:  # t2a, a2t íƒœìŠ¤í¬
            loss_gesture_factor = 0  # ì œìŠ¤ì²˜ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            sigmas = sigmas * 0 + 1  # ì œìŠ¤ì²˜ ì‹œê·¸ë§ˆë¥¼ 1ë¡œ ì„¤ì •
            timesteps = timesteps * 0 + 1000  # ì œìŠ¤ì²˜ íƒ€ì„ìŠ¤í…ì„ 1000ìœ¼ë¡œ ì„¤ì •
              
        if task in ['aud2text', 'aud2img']:  # a2t, a2g íƒœìŠ¤í¬
            loss_aud_factor = 0  # ì˜¤ë””ì˜¤ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            if np.random.rand() < 0.8:  # 80% í™•ë¥ ë¡œ ì˜¤ë””ì˜¤ ì‹œê·¸ë§ˆì™€ íƒ€ì„ìŠ¤í…ì„ 0ìœ¼ë¡œ ì„¤ì •
                sigmas_audio = sigmas_audio * 0  # ì˜¤ë””ì˜¤ ì‹œê·¸ë§ˆë¥¼ 0ìœ¼ë¡œ ì„¤ì •
                timesteps_audio = timesteps_audio * 0  # ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤í…ì„ 0ìœ¼ë¡œ ì„¤ì •
            
        if task in ['text2img', 'img2text']:  # t2g, g2t íƒœìŠ¤í¬
            loss_aud_factor = 0  # ì˜¤ë””ì˜¤ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            sigmas_audio = sigmas_audio * 0 + 1  # ì˜¤ë””ì˜¤ ì‹œê·¸ë§ˆë¥¼ 1ë¡œ ì„¤ì •
            timesteps_audio = timesteps_audio * 0 + 1000  # ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤í…ì„ 1000ìœ¼ë¡œ ì„¤ì •
        
        # ============================================================================
        # í’€ë§ ëª¨ë“œ ê²°ì •
        # ============================================================================
        if task in ['img2text', 'img2aud']:  # g2t, g2a íƒœìŠ¤í¬
            pool_mode = 'gesture'  # ì œìŠ¤ì²˜ ì„ë² ë”© ì‚¬ìš©
        elif task in ['aud2img', 'aud2text']:  # a2g, a2t íƒœìŠ¤í¬
            pool_mode = 'aud'  # ì˜¤ë””ì˜¤ ì„ë² ë”© ì‚¬ìš©
        else:  # ê¸°íƒ€ íƒœìŠ¤í¬
            pool_mode = 'text'  # í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©
            
        if not can_generate_text:  # í…ìŠ¤íŠ¸ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
            loss_text_factor = loss_text_factor * 0  # í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°ë¥¼ 0ìœ¼ë¡œ ì„¤ì •

        # ============================================================================
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”© (OmniFlowì™€ ë™ì¼)
        # ============================================================================
        print(f"DEBUG: ========== TEXT ENCODING ==========")  # í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„¹ì…˜ ì‹œì‘
        prompts = prompts.tolist()  # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        target_labels = tokenize_prompt(tokenizer_three, prompts)  # í”„ë¡¬í”„íŠ¸ë¥¼ í† í¬ë‚˜ì´ì €ë¡œ í† í¬í™”
        target_labels = target_labels.to(device)  # íƒ€ê²Ÿ ë¼ë²¨ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        print(f"DEBUG: target_labels shape: {target_labels.shape}")  # íƒ€ê²Ÿ ë¼ë²¨ í˜•íƒœ ì¶œë ¥
        
        prompt_embeds, pooled_prompt_embeds = n_compute_text_embeddings(  # í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚°
            device, prompts, text_encoders, tokenizers, add_token_embed=add_token_embed, train=False
        )
        print(f"DEBUG: prompt_embeds shape: {prompt_embeds.shape}")  # í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: pooled_prompt_embeds shape: {pooled_prompt_embeds.shape}")  # í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        
        # ============================================================================
        # í…ìŠ¤íŠ¸ VAE ì¸ì½”ë”© (ë°ì´í„° íƒ€ì… ì¼ì¹˜ ë³´ì¥)
        # ============================================================================
        print(f"DEBUG: ========== TEXT VAE ENCODING ==========")  # í…ìŠ¤íŠ¸ VAE ì¸ì½”ë”© ì„¹ì…˜ ì‹œì‘
        
        # ë°ì´í„° íƒ€ì… ì¼ì¹˜ë¥¼ ìœ„í•´ text_vaeë¥¼ float32ë¡œ ì„¤ì •
        original_dtype = next(text_vae.parameters()).dtype
        print(f"DEBUG: text_vae original dtype: {original_dtype}")
        if original_dtype != torch.float32:
            text_vae = text_vae.float()  # text_vaeë¥¼ float32ë¡œ ë³€í™˜
            print(f"DEBUG: text_vae converted to float32")
        
        prompt_embeds_vae = text_vae.encode(prompts, input_ids=None, tokenizer=tokenizer_three, sample=True)  # ì¡°ê±´ë¶€ VAE ì¸ì½”ë”©
        prompt_embeds_vae_uncond = text_vae.encode(prompts, input_ids=None, tokenizer=tokenizer_three, drop=True)  # ë¬´ì¡°ê±´ë¶€ VAE ì¸ì½”ë”©
        print(f"DEBUG: prompt_embeds_vae shape: {prompt_embeds_vae.shape}")  # ì¡°ê±´ë¶€ VAE ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: prompt_embeds_vae_uncond shape: {prompt_embeds_vae_uncond.shape}")  # ë¬´ì¡°ê±´ë¶€ VAE ì„ë² ë”© í˜•íƒœ ì¶œë ¥

        if not can_generate_text:  # í…ìŠ¤íŠ¸ ìƒì„±ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°
            prompt_embeds_vae *= 0  # VAE ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì„¤ì •
            print(f"DEBUG: Text generation disabled - zeroed VAE embeddings")  # í…ìŠ¤íŠ¸ ìƒì„± ë¹„í™œì„±í™” ë©”ì‹œì§€

        l_vae = prompt_embeds_vae.shape[1]  # VAE ì‹œí€€ìŠ¤ ê¸¸ì´ ì¶”ì¶œ
        print(f"DEBUG: l_vae (VAE sequence length): {l_vae}")  # VAE ì‹œí€€ìŠ¤ ê¸¸ì´ ì¶œë ¥
        
        # ============================================================================
        # í”„ë¡¬í”„íŠ¸ ì„ë² ë”© ì¤€ë¹„
        # ============================================================================
        print(f"DEBUG: ========== TEXT EMBEDDINGS PREPARATION ==========")  # í…ìŠ¤íŠ¸ ì„ë² ë”© ì¤€ë¹„ ì„¹ì…˜ ì‹œì‘
        prompt_embeds = cat_and_pad([prompt_embeds_vae], max_dim=4096)  # ì¡°ê±´ë¶€ VAE ì„ë² ë”©ì„ ì—°ê²°í•˜ê³  íŒ¨ë”©
        prompt_embeds_uncond = cat_and_pad([prompt_embeds_vae_uncond], max_dim=4096)  # ë¬´ì¡°ê±´ë¶€ VAE ì„ë² ë”©ì„ ì—°ê²°í•˜ê³  íŒ¨ë”©
        print(f"DEBUG: After cat_and_pad - prompt_embeds shape: {prompt_embeds.shape}")  # ì—°ê²° ë° íŒ¨ë”© í›„ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: After cat_and_pad - prompt_embeds_uncond shape: {prompt_embeds_uncond.shape}")  # ì—°ê²° ë° íŒ¨ë”© í›„ ë¬´ì¡°ê±´ë¶€ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥

        # ============================================================================
        # í…ìŠ¤íŠ¸ ë””ì½”ë”ë¥¼ ìœ„í•œ íƒ€ê²Ÿ
        # ============================================================================
        targets = encode_prompt_for_decoder(prompts, text_vae_tokenizer, device=transformer.device)  # ë””ì½”ë”ìš© í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©
        target_labels = targets['labels']  # íƒ€ê²Ÿ ë¼ë²¨ ì¶”ì¶œ
        print(f"DEBUG: Text decoder targets shape: {target_labels.shape}")  # í…ìŠ¤íŠ¸ ë””ì½”ë” íƒ€ê²Ÿ í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: Target labels sample: {targets.keys()}")  # íƒ€ê²Ÿ ë¼ë²¨ ìƒ˜í”Œ ì¶œë ¥

        # ============================================================================
        # í’€ë§ ëª¨ë“œì— ë”°ë¥¸ í’€ë§ëœ ì„ë² ë”©
        # ============================================================================
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
            if pool_mode == 'gesture':  # ì œìŠ¤ì²˜ í’€ë§ ëª¨ë“œì¸ ê²½ìš°
                # ì œìŠ¤ì²˜ ì„ë² ë”© ì‚¬ìš© (í˜„ì¬ëŠ” ë”ë¯¸)
                pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)  # í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                if batch['drop_img'] is not None:  # ì œìŠ¤ì²˜ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
                    pooled_prompt_embeds[batch['drop_img']] = 0  # ë“œë¡­ëœ ì œìŠ¤ì²˜ì˜ í’€ë§ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì„¤ì •
            elif pool_mode == 'aud':  # ì˜¤ë””ì˜¤ í’€ë§ ëª¨ë“œì¸ ê²½ìš°
                audio_embeds = audio_encoder.get_image_features(  # ì˜¤ë””ì˜¤ ì¸ì½”ë”ì—ì„œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
                    pixel_values=batch['audio_clip'].to(audio_encoder.dtype).to(audio_encoder.device)  # ì˜¤ë””ì˜¤ í´ë¦½ì„ ì¸ì½”ë” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                )
                pooled_prompt_embeds = torch.zeros_like(pooled_prompt_embeds)  # í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”
                pooled_prompt_embeds[..., :audio_embeds.shape[-1]] = audio_embeds  # ì˜¤ë””ì˜¤ ì„ë² ë”©ì„ í’€ë§ëœ ì„ë² ë”©ì— í• ë‹¹
                if batch['drop_aud'] is not None:  # ì˜¤ë””ì˜¤ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
                    pooled_prompt_embeds[batch['drop_aud']] = 0  # ë“œë¡­ëœ ì˜¤ë””ì˜¤ì˜ í’€ë§ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì„¤ì •
            else:  # í…ìŠ¤íŠ¸ í’€ë§ ëª¨ë“œì¸ ê²½ìš°
                if batch['drop_text'] is not None:  # í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
                    pooled_prompt_embeds[batch['drop_text']] = 0  # ë“œë¡­ëœ í…ìŠ¤íŠ¸ì˜ í’€ë§ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì„¤ì •
                    
        pooled_prompt_embeds = pooled_prompt_embeds.detach()  # í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ ê·¸ë˜ë””ì–¸íŠ¸ì—ì„œ ë¶„ë¦¬
        
        # ============================================================================
        # í’€ë§ëœ ì„ë² ë”©ì— ë“œë¡­ì•„ì›ƒ ì ìš©
        # ============================================================================
        drop_pool = (torch.rand(pooled_prompt_embeds.shape[0]) < 0.85).view(-1, 1).to(pooled_prompt_embeds)  # 85% í™•ë¥ ë¡œ ë“œë¡­ì•„ì›ƒ ë§ˆìŠ¤í¬ ìƒì„±
        pooled_prompt_embeds = pooled_prompt_embeds * drop_pool  # ë“œë¡­ì•„ì›ƒ ë§ˆìŠ¤í¬ë¥¼ í’€ë§ëœ ì„ë² ë”©ì— ì ìš©
        
        # ============================================================================
        # ì‹œê·¸ë§ˆ í…ìŠ¤íŠ¸ í˜•íƒœ ë³€ê²½
        # ============================================================================
        sigma_text = sigma_text.view(-1, 1, 1)  # ì‹œê·¸ë§ˆ í…ìŠ¤íŠ¸ë¥¼ 3ì°¨ì›ìœ¼ë¡œ í˜•íƒœ ë³€ê²½
        
        # ============================================================================
        # ë…¸ì´ì¦ˆ ìƒì„±
        # ============================================================================
        print(f"DEBUG: ========== NOISE GENERATION ==========")  # ë…¸ì´ì¦ˆ ìƒì„± ì„¹ì…˜ ì‹œì‘
        noise = torch.randn_like(model_input)  # ëª¨ë¸ ì…ë ¥ê³¼ ë™ì¼í•œ í˜•íƒœì˜ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±
        noise_text = torch.randn_like(prompt_embeds)  # í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ê³¼ ë™ì¼í•œ í˜•íƒœì˜ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±
        print(f"DEBUG: noise shape: {noise.shape}")  # ì œìŠ¤ì²˜ ë…¸ì´ì¦ˆ í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: noise_text shape: {noise_text.shape}")  # í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ í˜•íƒœ ì¶œë ¥
        
        # ============================================================================
        # ì…ë ¥ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        # ============================================================================
        noisy_model_input = sigmas * noise + (1.0 - sigmas) * model_input  # ì œìŠ¤ì²˜ ì…ë ¥ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        noisy_prompt_embeds = sigma_text * noise_text + (1.0 - sigma_text) * prompt_embeds  # í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        print(f"DEBUG: noisy_model_input shape: {noisy_model_input.shape}")  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ëª¨ë¸ ì…ë ¥ í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: noisy_prompt_embeds shape: {noisy_prompt_embeds.shape}")  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ ì¶œë ¥

        noise_audio = torch.randn_like(raw_audio_embeds)  # ì˜¤ë””ì˜¤ ì„ë² ë”©ê³¼ ë™ì¼í•œ í˜•íƒœì˜ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ìƒì„±
        sigmas_audio = sigmas_audio.view(-1, 1, 1, 1)  # ì˜¤ë””ì˜¤ ì‹œê·¸ë§ˆë¥¼ 4ì°¨ì›ìœ¼ë¡œ í˜•íƒœ ë³€ê²½
        noisy_audio_embeds = sigmas_audio * noise_audio + (1.0 - sigmas_audio) * raw_audio_embeds  # ì˜¤ë””ì˜¤ ì„ë² ë”©ì— ë…¸ì´ì¦ˆ ì¶”ê°€
        print(f"DEBUG: noise_audio shape: {noise_audio.shape}")  # ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ í˜•íƒœ ì¶œë ¥
        print(f"DEBUG: noisy_audio_embeds shape: {noisy_audio_embeds.shape}")  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì˜¤ë””ì˜¤ ì„ë² ë”© í˜•íƒœ ì¶œë ¥

        # ============================================================================
        # í…ìŠ¤íŠ¸ ì„ë² ë”© ì •ë¦¬
        # ============================================================================
        noisy_prompt_embeds[:, -l_vae:, prompt_embeds_vae.shape[-1]:] = 0  # VAE ë¶€ë¶„ ì´í›„ì˜ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ 0ìœ¼ë¡œ ì„¤ì •
        noisy_prompt_embeds = noisy_prompt_embeds.detach()  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ì„ ê·¸ë˜ë””ì–¸íŠ¸ì—ì„œ ë¶„ë¦¬
        
        # ============================================================================
        # ìµœì¢… ë°˜í™˜ê°’
        # ============================================================================
        return (  # í›ˆë ¨ì— í•„ìš”í•œ ëª¨ë“  ì…ë ¥ ë°ì´í„°ë“¤ì„ íŠœí”Œë¡œ ë°˜í™˜
            noisy_model_input, timesteps, timesteps_text, timesteps_audio, noisy_prompt_embeds,  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì…ë ¥ë“¤
            noisy_audio_embeds, sigma_text, prompt_embeds, pooled_prompt_embeds, targets, prompt_embeds_uncond,  # ì„ë² ë”© ë° íƒ€ê²Ÿë“¤
            sigmas, sigmas_audio, model_input,  # ì‹œê·¸ë§ˆ ê°’ë“¤ê³¼ ì›ë³¸ ëª¨ë¸ ì…ë ¥
            loss_gesture_factor,  # ì œìŠ¤ì²˜ ì†ì‹¤ íŒ©í„° (ê¸°ì¡´ loss_img_factorì—ì„œ ì´ë¦„ ë³€ê²½)
            loss_text_factor,  # í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°
            loss_aud_factor,  # ì˜¤ë””ì˜¤ ì†ì‹¤ íŒ©í„°
            noise_scheduler_copy,  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸
            raw_audio_embeds,  # ì›ë³¸ ì˜¤ë””ì˜¤ ì„ë² ë”©
            task, task_type,  # íƒœìŠ¤í¬ ì´ë¦„ë“¤ (OmniFlow í˜¸í™˜ ì´ë¦„ê³¼ ì›ë³¸ ì´ë¦„ ëª¨ë‘ í¬í•¨)
            prompts,  # ì›ë³¸ í”„ë¡¬í”„íŠ¸ë“¤
            noise,  # ì œìŠ¤ì²˜ ë…¸ì´ì¦ˆ
            noise_text,  # í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ
            noise_audio,  # ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ
            target_labels,  # íƒ€ê²Ÿ ë¼ë²¨ë“¤
            prompt_embeds_vae_uncond,  # ë¬´ì¡°ê±´ë¶€ VAE í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
            gesture_sequences  # ìƒˆë¡œìš´: ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ ì¶”ê°€
        )


def compute_omniges_loss(
    transformer, noisy_model_input, timesteps, timesteps_text, timesteps_audio, noisy_prompt_embeds,
    noisy_audio_embeds, sigma_text, prompt_embeds, pooled_prompt_embeds, targets, prompt_embeds_uncond,
    sigmas, sigmas_audio, model_input, loss_gesture_factor, loss_text_factor, loss_aud_factor,
    noise_scheduler_copy, last_lr, raw_audio_embeds, task, task_type, prompts,
    noise, noise_text, noise_audio, text_vae, target_labels, do_decode,
    prompt_embeds_vae_uncond, precondition_text_outputs=False, anchor=False, batch=None,
    gesture_sequences=None
):
    """
    Omniges í›ˆë ¨ì„ ìœ„í•œ ì†ì‹¤ì„ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    OmniFlowë¥¼ ì œìŠ¤ì²˜ ì²˜ë¦¬ì— ë§ê²Œ ì ì‘
    
    Args:
        transformer: OmnigesFlowTransformerModel
        noisy_model_input: ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì œìŠ¤ì²˜ ì…ë ¥
        timesteps: ì œìŠ¤ì²˜ íƒ€ì„ìŠ¤í…
        timesteps_text: í…ìŠ¤íŠ¸ íƒ€ì„ìŠ¤í…
        timesteps_audio: ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤í…
        noisy_prompt_embeds: ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í…ìŠ¤íŠ¸ ì„ë² ë”©
        noisy_audio_embeds: ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì˜¤ë””ì˜¤ ì„ë² ë”©
        sigma_text: í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆ ê°’
        prompt_embeds: ì›ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”©
        pooled_prompt_embeds: í’€ë§ëœ í…ìŠ¤íŠ¸ ì„ë² ë”©
        targets: í…ìŠ¤íŠ¸ ë””ì½”ë” íƒ€ê²Ÿ
        prompt_embeds_uncond: ë¬´ì¡°ê±´ë¶€ í…ìŠ¤íŠ¸ ì„ë² ë”©
        sigmas: ì œìŠ¤ì²˜ ì‹œê·¸ë§ˆ ê°’
        sigmas_audio: ì˜¤ë””ì˜¤ ì‹œê·¸ë§ˆ ê°’
        model_input: ì›ë³¸ ì œìŠ¤ì²˜ ì…ë ¥
        loss_gesture_factor: ì œìŠ¤ì²˜ ì†ì‹¤ íŒ©í„°
        loss_text_factor: í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°
        loss_aud_factor: ì˜¤ë””ì˜¤ ì†ì‹¤ íŒ©í„°
        noise_scheduler_copy: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸
        last_lr: ë§ˆì§€ë§‰ í•™ìŠµë¥ 
        raw_audio_embeds: ì›ë³¸ ì˜¤ë””ì˜¤ ì„ë² ë”©
        task: OmniFlow í˜¸í™˜ íƒœìŠ¤í¬ ì´ë¦„
        task_type: ì›ë³¸ Omniges íƒœìŠ¤í¬ íƒ€ì…
        prompts: ì›ë³¸ í”„ë¡¬í”„íŠ¸ë“¤
        noise: ì œìŠ¤ì²˜ ë…¸ì´ì¦ˆ
        noise_text: í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ
        noise_audio: ì˜¤ë””ì˜¤ ë…¸ì´ì¦ˆ
        text_vae: í…ìŠ¤íŠ¸ VAE
        target_labels: íƒ€ê²Ÿ ë¼ë²¨ë“¤
        do_decode: ë””ì½”ë”© ì—¬ë¶€
        prompt_embeds_vae_uncond: ë¬´ì¡°ê±´ë¶€ VAE í…ìŠ¤íŠ¸ ì„ë² ë”©
        precondition_text_outputs: í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬ ì—¬ë¶€
        anchor: ì•µì»¤ í”Œë˜ê·¸
        batch: ë°°ì¹˜ ë°ì´í„°
        gesture_sequences: ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤
    
    Returns:
        tuple: ì†ì‹¤, ë””ì½”ë”© ì†ì‹¤, ë¡œê·¸, íƒœìŠ¤í¬ íƒ€ì…, ì˜ˆì¸¡ê°’ë“¤
    """
    
    # ============================================================================
    # OmnigesFlowë¥¼ í†µí•œ ìˆœì „íŒŒ
    # ============================================================================
    print(f"DEBUG: ========== MODEL FORWARD PASS ==========")  # ëª¨ë¸ ìˆœì „íŒŒ ì„¹ì…˜ ì‹œì‘
    print(f"DEBUG: Forward pass inputs:")  # ìˆœì „íŒŒ ì…ë ¥ ì •ë³´ ì¶œë ¥
    print(f"DEBUG:   noisy_model_input shape: {noisy_model_input.shape}")  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì œìŠ¤ì²˜ ì…ë ¥ í˜•íƒœ
    print(f"DEBUG:   timesteps shape: {timesteps.shape}")  # ì œìŠ¤ì²˜ íƒ€ì„ìŠ¤í… í˜•íƒœ
    print(f"DEBUG:   timesteps_text shape: {timesteps_text.shape}")  # í…ìŠ¤íŠ¸ íƒ€ì„ìŠ¤í… í˜•íƒœ
    print(f"DEBUG:   timesteps_audio shape: {timesteps_audio.shape}")  # ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤í… í˜•íƒœ
    print(f"DEBUG:   noisy_prompt_embeds shape: {noisy_prompt_embeds.shape}")  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ í…ìŠ¤íŠ¸ ì„ë² ë”© í˜•íƒœ
    print(f"DEBUG:   noisy_audio_embeds shape: {noisy_audio_embeds.shape}")  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì˜¤ë””ì˜¤ ì„ë² ë”© í˜•íƒœ
    print(f"DEBUG:   pooled_prompt_embeds shape: {pooled_prompt_embeds.shape}")  # í’€ë§ëœ í”„ë¡¬í”„íŠ¸ ì„ë² ë”© í˜•íƒœ
    
    output_dict = transformer(  # OmnigesFlow íŠ¸ëœìŠ¤í¬ë¨¸ì— ìˆœì „íŒŒ
        hidden_states=noisy_model_input,              # ì œìŠ¤ì²˜ ì ì¬ ë³€ìˆ˜
        timestep=timesteps,                           # ì œìŠ¤ì²˜ íƒ€ì„ìŠ¤í…
        timestep_text=timesteps_text,                 # í…ìŠ¤íŠ¸ íƒ€ì„ìŠ¤í…
        timestep_audio=timesteps_audio,               # ì˜¤ë””ì˜¤ íƒ€ì„ìŠ¤í…
        encoder_hidden_states=noisy_prompt_embeds,    # í…ìŠ¤íŠ¸ ì„ë² ë”©
        audio_hidden_states=noisy_audio_embeds,       # ì˜¤ë””ì˜¤ ì„ë² ë”©
        sigma_text=sigma_text,  # í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆ ê°’
        target_prompt_embeds=prompt_embeds,  # íƒ€ê²Ÿ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
        pooled_projections=pooled_prompt_embeds,  # í’€ë§ëœ íˆ¬ì˜
        targets=targets,  # í…ìŠ¤íŠ¸ ë””ì½”ë” íƒ€ê²Ÿ
        return_dict=False,  # ë”•ì…”ë„ˆë¦¬ ë°˜í™˜ ë¹„í™œì„±í™”
        use_text_output=True,  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì‚¬ìš©
        prompt_embeds_uncond=None if np.random.rand() < 0.5 else prompt_embeds_uncond,  # 50% í™•ë¥ ë¡œ ë¬´ì¡°ê±´ë¶€ ì„ë² ë”© ì‚¬ìš©
        detach_logits=not anchor,  # ì•µì»¤ê°€ ì•„ë‹Œ ê²½ìš° ë¡œì§“ ë¶„ë¦¬
        split_cond=False,  # ì¡°ê±´ ë¶„í•  ë¹„í™œì„±í™”
        text_vae=text_vae,  # í…ìŠ¤íŠ¸ VAE
        text_x0=precondition_text_outputs,  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬
        decode_text=True,  # í…ìŠ¤íŠ¸ ë””ì½”ë”© í™œì„±í™”
        # íƒœìŠ¤í¬ë³„ ë“œë¡­ì•„ì›ƒ ë¡œì§
        # ì…ë ¥ ëª¨ë‹¬ë¦¬í‹°: ì ˆëŒ€ ë“œë¡­í•˜ì§€ ì•ŠìŒ
        # ì¶œë ¥ ëª¨ë‹¬ë¦¬í‹°: ë°°ì¹˜ ë“œë¡­ì•„ì›ƒ ì„¤ì • ì‚¬ìš©
        drop_gesture=(task in ['text2img', 'aud2img'] and batch['drop_img'] is not None),  # T2G, A2G íƒœìŠ¤í¬ì—ì„œë§Œ ì œìŠ¤ì²˜ ë“œë¡­
        drop_text=(task in ['img2text', 'aud2text'] and batch['drop_text'] is not None),   # G2T, A2T íƒœìŠ¤í¬ì—ì„œë§Œ í…ìŠ¤íŠ¸ ë“œë¡­
        drop_audio=(task in ['text2aud', 'img2aud'] and batch['drop_aud'] is not None)     # T2A, G2A íƒœìŠ¤í¬ì—ì„œë§Œ ì˜¤ë””ì˜¤ ë“œë¡­
    )
    
    # ============================================================================
    # ì˜ˆì¸¡ê°’ ì¶”ì¶œ
    # ============================================================================
    print(f"DEBUG: ========== MODEL OUTPUT ==========")  # ëª¨ë¸ ì¶œë ¥ ì„¹ì…˜ ì‹œì‘
    model_pred = output_dict['output']              # ì œìŠ¤ì²˜ ì¶œë ¥
    model_pred_audio = output_dict['audio_hidden_states']  # ì˜¤ë””ì˜¤ ì¶œë ¥
    model_pred_text = output_dict['model_pred_text']       # í…ìŠ¤íŠ¸ ì¶œë ¥
    logits = output_dict['logits']  # í…ìŠ¤íŠ¸ ë””ì½”ë”© ë¡œì§“
    logits_labels = output_dict['logits_labels']  # í…ìŠ¤íŠ¸ ë¼ë²¨ ë¡œì§“
    
    print(f"DEBUG: Model outputs:")  # ëª¨ë¸ ì¶œë ¥ ì •ë³´ ì¶œë ¥
    print(f"DEBUG:   model_pred shape: {model_pred.shape if model_pred is not None else None}")  # ì œìŠ¤ì²˜ ì˜ˆì¸¡ í˜•íƒœ
    print(f"DEBUG:   model_pred_audio shape: {model_pred_audio.shape if model_pred_audio is not None else None}")  # ì˜¤ë””ì˜¤ ì˜ˆì¸¡ í˜•íƒœ
    print(f"DEBUG:   model_pred_text shape: {model_pred_text.shape if model_pred_text is not None else None}")  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ í˜•íƒœ
    print(f"DEBUG:   logits shape: {logits.shape if logits is not None else None}")  # ë¡œì§“ í˜•íƒœ
    print(f"DEBUG:   logits_labels shape: {logits_labels.shape if logits_labels is not None else None}")  # ë¼ë²¨ ë¡œì§“ í˜•íƒœ
    
    # ============================================================================
    # ì†ë„ íƒ€ê²Ÿ ê³„ì‚°
    # ============================================================================
    print(f"DEBUG: ========== VELOCITY TARGETS ==========")  # ì†ë„ íƒ€ê²Ÿ ì„¹ì…˜ ì‹œì‘
    v_theta = noise - model_input                    # ì œìŠ¤ì²˜ ì†ë„ (ë…¸ì´ì¦ˆ - ì›ë³¸ ì…ë ¥)
    v_theta_audio = noise_audio - raw_audio_embeds   # ì˜¤ë””ì˜¤ ì†ë„ (ë…¸ì´ì¦ˆ - ì›ë³¸ ì˜¤ë””ì˜¤)
    print(f"DEBUG: v_theta shape: {v_theta.shape}")  # ì œìŠ¤ì²˜ ì†ë„ í˜•íƒœ ì¶œë ¥
    print(f"DEBUG: v_theta_audio shape: {v_theta_audio.shape}")  # ì˜¤ë””ì˜¤ ì†ë„ í˜•íƒœ ì¶œë ¥
    
    print(f"DEBUG: Loss input comparison:")  # ì†ì‹¤ ì…ë ¥ ë¹„êµ ì •ë³´ ì¶œë ¥
    print(f"DEBUG:   model_pred shape: {model_pred.shape if model_pred is not None else None}")  # ëª¨ë¸ ì˜ˆì¸¡ í˜•íƒœ
    print(f"DEBUG:   v_theta shape: {v_theta.shape}")  # ì œìŠ¤ì²˜ ì†ë„ í˜•íƒœ
    print(f"DEBUG:   Are shapes compatible? {model_pred.shape == v_theta.shape if model_pred is not None else 'model_pred is None'}")  # í˜•íƒœ í˜¸í™˜ì„± í™•ì¸
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ ì„ë² ë”© ì²˜ë¦¬ (ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œ model_pred_textê°€ Noneì¼ ìˆ˜ ìˆìŒ)
    # ============================================================================
    if model_pred_text is not None:  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        raw_text_embeds = prompt_embeds[..., :model_pred_text.shape[-1]]  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ í¬ê¸°ì— ë§ê²Œ ì›ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ìŠ¬ë¼ì´ì‹±
        noise_text = noise_text[..., :model_pred_text.shape[-1]]  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ í¬ê¸°ì— ë§ê²Œ í…ìŠ¤íŠ¸ ë…¸ì´ì¦ˆ ìŠ¬ë¼ì´ì‹±
    else:  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ì´ ì—†ëŠ” ê²½ìš°
        raw_text_embeds = prompt_embeds  # ì›ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©
        # noise_textëŠ” ì´ë¯¸ ì˜¬ë°”ë¥¸ í¬ê¸°

    # ============================================================================
    # ì†ì‹¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    # ============================================================================
    weighting = compute_loss_weighting_for_sd3(weighting_scheme=args.weighting_scheme, sigmas=sigmas)  # ì œìŠ¤ì²˜ ì†ì‹¤ ê°€ì¤‘ì¹˜
    weighting_text = compute_loss_weighting_for_sd3(weighting_scheme=args.weighting_scheme, sigmas=sigma_text)  # í…ìŠ¤íŠ¸ ì†ì‹¤ ê°€ì¤‘ì¹˜
    weighting_audio = compute_loss_weighting_for_sd3(weighting_scheme=args.weighting_scheme, sigmas=sigmas_audio)  # ì˜¤ë””ì˜¤ ì†ì‹¤ ê°€ì¤‘ì¹˜
    
    # ============================================================================
    # ê°€ì¤‘ì¹˜ì— ë“œë¡­ì•„ì›ƒ ì ìš©
    # ============================================================================
    if batch['drop_img'] is not None:  # ì œìŠ¤ì²˜ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
        weighting[batch['drop_img']] = 0  # ë“œë¡­ëœ ì œìŠ¤ì²˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •

    # ============================================================================
    # ì œìŠ¤ì²˜ ì†ì‹¤ ê³„ì‚° (ì´ë¯¸ì§€ ì†ì‹¤ì—ì„œ ì ì‘)
    # ============================================================================
    print(f"DEBUG: ========== LOSS CALCULATION ==========")  # ì†ì‹¤ ê³„ì‚° ì„¹ì…˜ ì‹œì‘
    print(f"DEBUG: Gesture loss inputs:")  # ì œìŠ¤ì²˜ ì†ì‹¤ ì…ë ¥ ì •ë³´ ì¶œë ¥
    print(f"DEBUG:   weighting shape: {weighting.shape}")  # ê°€ì¤‘ì¹˜ í˜•íƒœ
    print(f"DEBUG:   model_pred shape: {model_pred.shape if model_pred is not None else None}")  # ëª¨ë¸ ì˜ˆì¸¡ í˜•íƒœ
    print(f"DEBUG:   v_theta shape: {v_theta.shape}")  # ì œìŠ¤ì²˜ ì†ë„ í˜•íƒœ
    
    if model_pred is not None and v_theta is not None:  # ëª¨ë¸ ì˜ˆì¸¡ê³¼ ì†ë„ê°€ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        loss_gesture = (weighting.float() * (model_pred - v_theta.float()) ** 2).mean()  # ì œìŠ¤ì²˜ ì†ì‹¤ ê³„ì‚° (MSE)
        print(f"DEBUG:   gesture loss value: {loss_gesture.item()}")  # ì œìŠ¤ì²˜ ì†ì‹¤ ê°’ ì¶œë ¥
    else:  # ëª¨ë¸ ì˜ˆì¸¡ ë˜ëŠ” ì†ë„ê°€ Noneì¸ ê²½ìš°
        loss_gesture = torch.tensor(0.0, device=weighting.device)  # ì œìŠ¤ì²˜ ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
        print(f"DEBUG:   gesture loss set to 0 (None inputs)")  # ì œìŠ¤ì²˜ ì†ì‹¤ 0 ì„¤ì • ë©”ì‹œì§€

    # ============================================================================
    # í…ìŠ¤íŠ¸ ì†ì‹¤ ê³„ì‚° (OmniFlowì™€ ë™ì¼) - None ì²˜ë¦¬ í¬í•¨
    # ============================================================================
    print(f"DEBUG: Text loss inputs:")  # í…ìŠ¤íŠ¸ ì†ì‹¤ ì…ë ¥ ì •ë³´ ì¶œë ¥
    print(f"DEBUG:   weighting_text shape: {weighting_text.shape}")  # í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ í˜•íƒœ
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        weighting_text = weighting_text.view(-1, 1, 1)  # í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ë¥¼ 3ì°¨ì›ìœ¼ë¡œ í˜•íƒœ ë³€ê²½
        if batch['drop_text'] is not None:  # í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
            weighting_text[batch['drop_text']] = 0  # ë“œë¡­ëœ í…ìŠ¤íŠ¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            print(f"DEBUG:   Applied text dropout")  # í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒ ì ìš© ë©”ì‹œì§€
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ ì¶œë ¥ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ ì†ì‹¤ ì²˜ë¦¬
    # ============================================================================
    if model_pred_text is not None and loss_text_factor > 0:  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ì´ ì¡´ì¬í•˜ê³  í…ìŠ¤íŠ¸ íŒ©í„°ê°€ 0ë³´ë‹¤ í° ê²½ìš°
        if precondition_text_outputs:  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬ê°€ í™œì„±í™”ëœ ê²½ìš°
            loss_text = (weighting_text.float() * (model_pred_text.float() - raw_text_embeds.float().detach()) ** 2).mean()  # ì „ì²˜ë¦¬ í…ìŠ¤íŠ¸ ì†ì‹¤
            norm_1 = F.normalize(model_pred_text, dim=-1, eps=1e-4).float()  # ëª¨ë¸ ì˜ˆì¸¡ ì •ê·œí™”
            norm_2 = F.normalize(raw_text_embeds, dim=-1, eps=1e-4).float().detach()  # ì›ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”© ì •ê·œí™”
            loss_text_norm = (weighting_text.float() * (norm_1 - norm_2) ** 2).mean()  # ì •ê·œí™” í…ìŠ¤íŠ¸ ì†ì‹¤
            loss_text_norm = loss_text_norm * 0.1  # ì •ê·œí™” ì†ì‹¤ì— 0.1 ê°€ì¤‘ì¹˜ ì ìš©
        else:  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°
            v_theta_text = noise_text - raw_text_embeds  # í…ìŠ¤íŠ¸ ì†ë„ ê³„ì‚°
            loss_text = (weighting_text.float() * (model_pred_text.float() - v_theta_text.float()) ** 2).mean()  # ì¼ë°˜ í…ìŠ¤íŠ¸ ì†ì‹¤
            loss_text_norm = 0  # ì •ê·œí™” ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
    else:  # í…ìŠ¤íŠ¸ ì¶œë ¥ì´ ì—†ê±°ë‚˜ í…ìŠ¤íŠ¸ íŒ©í„°ê°€ 0ì¸ ê²½ìš°
        # í…ìŠ¤íŠ¸ ì¶œë ¥ì´ ì—†ê±°ë‚˜ í…ìŠ¤íŠ¸ íŒ©í„°ê°€ 0
        loss_text = torch.tensor(0.0, device=model_input.device)  # í…ìŠ¤íŠ¸ ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
        loss_text_norm = 0  # ì •ê·œí™” ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
        
    # ============================================================================
    # ì˜¤ë””ì˜¤ ì†ì‹¤ ê³„ì‚° (OmniFlowì™€ ë™ì¼)
    # ============================================================================
    print(f"DEBUG: Audio loss inputs:")  # ì˜¤ë””ì˜¤ ì†ì‹¤ ì…ë ¥ ì •ë³´ ì¶œë ¥
    print(f"DEBUG:   weighting_audio shape before view: {weighting_audio.shape}")  # view ì „ ì˜¤ë””ì˜¤ ê°€ì¤‘ì¹˜ í˜•íƒœ
    weighting_audio = weighting_audio.view(-1, 1, 1, 1)  # ì˜¤ë””ì˜¤ ê°€ì¤‘ì¹˜ë¥¼ 4ì°¨ì›ìœ¼ë¡œ í˜•íƒœ ë³€ê²½
    print(f"DEBUG:   weighting_audio shape after view: {weighting_audio.shape}")  # view í›„ ì˜¤ë””ì˜¤ ê°€ì¤‘ì¹˜ í˜•íƒœ
    print(f"DEBUG:   model_pred_audio shape: {model_pred_audio.shape if model_pred_audio is not None else None}")  # ì˜¤ë””ì˜¤ ì˜ˆì¸¡ í˜•íƒœ
    print(f"DEBUG:   v_theta_audio shape: {v_theta_audio.shape}")  # ì˜¤ë””ì˜¤ ì†ë„ í˜•íƒœ
    
    if model_pred_audio is not None:  # ì˜¤ë””ì˜¤ ì˜ˆì¸¡ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        loss_audio = (weighting_audio.float() * (model_pred_audio - v_theta_audio.float()) ** 2).mean()  # ì˜¤ë””ì˜¤ ì†ì‹¤ ê³„ì‚° (MSE)
        print(f"DEBUG:   audio loss value: {loss_audio.item()}")  # ì˜¤ë””ì˜¤ ì†ì‹¤ ê°’ ì¶œë ¥
    else:  # ì˜¤ë””ì˜¤ ì˜ˆì¸¡ì´ Noneì¸ ê²½ìš°
        loss_audio = torch.tensor(0.0, device=weighting_audio.device)  # ì˜¤ë””ì˜¤ ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
        print(f"DEBUG:   audio loss set to 0 (None model_pred_audio)")  # ì˜¤ë””ì˜¤ ì†ì‹¤ 0 ì„¤ì • ë©”ì‹œì§€

    # ============================================================================
    # í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ ë””ì½”ë”© ì†ì‹¤ (OmniFlowì™€ ë™ì¼)
    # ============================================================================
    if anchor:  # ì•µì»¤ ëª¨ë“œì¸ ê²½ìš°
        from train import WeightedLabelSmoother, compute_decode_loss_weight  # í•„ìš”í•œ ëª¨ë“ˆ ì„í¬íŠ¸
        label_smoother = WeightedLabelSmoother(epsilon=0.0, ignore_index=-100)  # ë¼ë²¨ ìŠ¤ë¬´ë” ì´ˆê¸°í™”
        decode_loss_tgt_weight = torch.ones(len(timesteps_text)).to(logits)  # íƒ€ê²Ÿ ë””ì½”ë”© ì†ì‹¤ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        if anchor:  # ì•µì»¤ ëª¨ë“œì¸ ê²½ìš°
            decode_loss_weight = torch.ones(len(timesteps_text)).to(logits)  # ë””ì½”ë”© ì†ì‹¤ ê°€ì¤‘ì¹˜ë¥¼ 1ë¡œ ì´ˆê¸°í™”
        else:  # ì•µì»¤ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°
            decode_loss_weight = compute_decode_loss_weight(timesteps_text, noise_scheduler_copy.config.num_train_timesteps)  # ë””ì½”ë”© ì†ì‹¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
        if batch['drop_text'] is not None:  # í…ìŠ¤íŠ¸ ë“œë¡­ì•„ì›ƒì´ ì„¤ì •ëœ ê²½ìš°
            decode_loss_weight[batch['drop_text']] = 0  # ë“œë¡­ëœ í…ìŠ¤íŠ¸ì˜ ë””ì½”ë”© ì†ì‹¤ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            decode_loss_tgt_weight[batch['drop_text']] = 0  # ë“œë¡­ëœ í…ìŠ¤íŠ¸ì˜ íƒ€ê²Ÿ ë””ì½”ë”© ì†ì‹¤ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
        decode_loss_pred = label_smoother([logits], target_labels, shift_labels=True, sample_weight=decode_loss_weight)  # ì˜ˆì¸¡ ë””ì½”ë”© ì†ì‹¤ ê³„ì‚°
        decode_loss_tgt = label_smoother([logits_labels], target_labels, shift_labels=True, sample_weight=decode_loss_tgt_weight)  # íƒ€ê²Ÿ ë””ì½”ë”© ì†ì‹¤ ê³„ì‚°
        decode_loss = None  # ë””ì½”ë”© ì†ì‹¤ì„ Noneìœ¼ë¡œ ì„¤ì •
    else:  # ì•µì»¤ ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°
        decode_loss_pred = 0  # ì˜ˆì¸¡ ë””ì½”ë”© ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
        decode_loss_tgt = 0  # íƒ€ê²Ÿ ë””ì½”ë”© ì†ì‹¤ì„ 0ìœ¼ë¡œ ì„¤ì •
        decode_loss = None  # ë””ì½”ë”© ì†ì‹¤ì„ Noneìœ¼ë¡œ ì„¤ì •

    # ============================================================================
    # ì´ ì†ì‹¤ ê³„ì‚°
    # ============================================================================
    loss = (loss_gesture * loss_gesture_factor +  # ì œìŠ¤ì²˜ ì†ì‹¤ì— ì œìŠ¤ì²˜ íŒ©í„° ê³±í•˜ê¸°
            (loss_text + loss_text_norm) * loss_text_factor +  # í…ìŠ¤íŠ¸ ì†ì‹¤ì— í…ìŠ¤íŠ¸ íŒ©í„° ê³±í•˜ê¸°
            loss_audio * loss_aud_factor +  # ì˜¤ë””ì˜¤ ì†ì‹¤ì— ì˜¤ë””ì˜¤ íŒ©í„° ê³±í•˜ê¸°
            (decode_loss_tgt + decode_loss_pred) * loss_text_factor * 0.1)  # ë””ì½”ë”© ì†ì‹¤ì— í…ìŠ¤íŠ¸ íŒ©í„°ì™€ 0.1 ê°€ì¤‘ì¹˜ ê³±í•˜ê¸°

    # ============================================================================
    # ë¡œê¹…
    # ============================================================================
    logs = {  # ë¡œê·¸ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
        "loss": loss.detach().item(),  # ì´ ì†ì‹¤
        "lr": last_lr,  # ë§ˆì§€ë§‰ í•™ìŠµë¥ 
        "loss_aud_factor": loss_aud_factor,  # ì˜¤ë””ì˜¤ ì†ì‹¤ íŒ©í„°
        "loss_gesture_factor": loss_gesture_factor,  # ì œìŠ¤ì²˜ ì†ì‹¤ íŒ©í„° (ì´ë¦„ ë³€ê²½ë¨)
        "loss_text_factor": loss_text_factor,  # í…ìŠ¤íŠ¸ ì†ì‹¤ íŒ©í„°
        "task_type": task_type  # ì›ë³¸ íƒœìŠ¤í¬ íƒ€ì… ë¡œê¹…
    }
    
    if loss_text_factor > 0 and model_pred_text is not None:  # í…ìŠ¤íŠ¸ íŒ©í„°ê°€ 0ë³´ë‹¤ í¬ê³  í…ìŠ¤íŠ¸ ì˜ˆì¸¡ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
        logs.update({  # í…ìŠ¤íŠ¸ ê´€ë ¨ ë¡œê·¸ ì¶”ê°€
            "loss_text": loss_text.detach().item(),  # í…ìŠ¤íŠ¸ ì†ì‹¤
            "loss_text_norm": loss_text_norm.detach().item() if isinstance(loss_text_norm, torch.Tensor) else loss_text_norm,  # ì •ê·œí™” í…ìŠ¤íŠ¸ ì†ì‹¤
        })
        with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
            if raw_text_embeds is not None:  # ì›ë³¸ í…ìŠ¤íŠ¸ ì„ë² ë”©ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                logs.update({  # í…ìŠ¤íŠ¸ ì„ë² ë”© í†µê³„ ì¶”ê°€
                    "text_embed_mean": raw_text_embeds.mean().item(),  # í…ìŠ¤íŠ¸ ì„ë² ë”© í‰ê· 
                    "text_embed_std": raw_text_embeds.std().item(),  # í…ìŠ¤íŠ¸ ì„ë² ë”© í‘œì¤€í¸ì°¨
                })
        if anchor:  # ì•µì»¤ ëª¨ë“œì¸ ê²½ìš°
            logs.update({  # ë””ì½”ë”© ì†ì‹¤ ë¡œê·¸ ì¶”ê°€
                "decode_loss_tgt": decode_loss_tgt.detach().item(),  # íƒ€ê²Ÿ ë””ì½”ë”© ì†ì‹¤
                "decode_loss": decode_loss_pred.detach().item(),  # ì˜ˆì¸¡ ë””ì½”ë”© ì†ì‹¤
            })
            
    if loss_gesture_factor > 0:  # ì œìŠ¤ì²˜ íŒ©í„°ê°€ 0ë³´ë‹¤ í° ê²½ìš°
        logs.update({  # ì œìŠ¤ì²˜ ê´€ë ¨ ë¡œê·¸ ì¶”ê°€
            "loss_gesture": loss_gesture.detach().item(),  # ì œìŠ¤ì²˜ ì†ì‹¤
        })
        
    if loss_aud_factor > 0:  # ì˜¤ë””ì˜¤ íŒ©í„°ê°€ 0ë³´ë‹¤ í° ê²½ìš°
        logs.update({  # ì˜¤ë””ì˜¤ ê´€ë ¨ ë¡œê·¸ ì¶”ê°€
            "loss_audio": loss_audio.detach().item(),  # ì˜¤ë””ì˜¤ ì†ì‹¤
        })
        
    # ============================================================================
    # ì˜ˆì¸¡ê°’ ê³„ì‚°
    # ============================================================================
    with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
        model_pred = model_pred * (-sigmas) + noisy_model_input  # ì œìŠ¤ì²˜ ì˜ˆì¸¡ê°’ ê³„ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
        model_pred_audio = model_pred_audio * (-sigmas_audio) + noisy_audio_embeds  # ì˜¤ë””ì˜¤ ì˜ˆì¸¡ê°’ ê³„ì‚° (ë…¸ì´ì¦ˆ ì œê±°)
        target = model_input  # íƒ€ê²Ÿì„ ì›ë³¸ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì„¤ì •
        
    # ============================================================================
    # ìµœì¢… ë°˜í™˜ê°’
    # ============================================================================
    return (  # ëª¨ë“  ê²°ê³¼ë¥¼ íŠœí”Œë¡œ ë°˜í™˜
        loss, decode_loss, logs, task_type, model_pred, logits, target, prompts,  # ê¸°ë³¸ ê²°ê³¼ë“¤
        model_pred_audio, model_pred_audio, raw_audio_embeds, model_pred_text, raw_text_embeds  # ëª¨ë‹¬ë¦¬í‹°ë³„ ê²°ê³¼ë“¤
    )


def omniges_forward_pass(
    transformer, args, text_encoder_one, text_encoder_two, text_encoder_three,
    accelerator, batch, gesture_vae, tokenizer_three, text_encoders, tokenizers,
    tokenizer_one, tokenizer_two, weight_dtype, noise_scheduler_copy,
    noise_scheduler, audio_vae_factor, audiovae, text_vae_tokenizer,
    last_lr, text_vae, audio_encoder, do_decode=False,
    precondition_text_outputs=False, anchor=False, mm_encoder=None
):
    """
    Omniges í›ˆë ¨ì„ ìœ„í•œ ì™„ì „í•œ ìˆœì „íŒŒ í•¨ìˆ˜
    ì…ë ¥ ì¤€ë¹„ì™€ ì†ì‹¤ ê³„ì‚°ì„ í¬í•¨í•œ ì „ì²´ í›ˆë ¨ ìŠ¤í…ì„ ìˆ˜í–‰
    
    Args:
        transformer: OmnigesFlowTransformerModel
        args: í›ˆë ¨ ì¸ìë“¤
        text_encoder_one: ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë” (CLIP)
        text_encoder_two: ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë” (CLIP)
        text_encoder_three: ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë” (T5)
        accelerator: Accelerate ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì†ê¸°
        batch: ë°°ì¹˜ ë°ì´í„°
        gesture_vae: ì œìŠ¤ì²˜ VAE
        tokenizer_three: T5 í† í¬ë‚˜ì´ì €
        text_encoders: í…ìŠ¤íŠ¸ ì¸ì½”ë”ë“¤
        tokenizers: í† í¬ë‚˜ì´ì €ë“¤
        tokenizer_one: CLIP í† í¬ë‚˜ì´ì €
        tokenizer_two: CLIP í† í¬ë‚˜ì´ì €
        weight_dtype: ê°€ì¤‘ì¹˜ ë°ì´í„° íƒ€ì…
        noise_scheduler_copy: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸
        noise_scheduler: ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬
        audio_vae_factor: ì˜¤ë””ì˜¤ VAE íŒ©í„°
        audiovae: ì˜¤ë””ì˜¤ VAE
        text_vae_tokenizer: í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì €
        last_lr: ë§ˆì§€ë§‰ í•™ìŠµë¥ 
        text_vae: í…ìŠ¤íŠ¸ VAE
        audio_encoder: ì˜¤ë””ì˜¤ ì¸ì½”ë”
        do_decode: ë””ì½”ë”© ì—¬ë¶€
        precondition_text_outputs: í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬ ì—¬ë¶€
        anchor: ì•µì»¤ í”Œë˜ê·¸
        mm_encoder: ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”
    
    Returns:
        tuple: ì†ì‹¤, ë””ì½”ë”© ì†ì‹¤, ë¡œê·¸, íƒœìŠ¤í¬ íƒ€ì…, ì˜ˆì¸¡ê°’ë“¤
    """
    
    # ============================================================================
    # ì…ë ¥ ì¤€ë¹„
    # ============================================================================
    (noisy_model_input, timesteps, timesteps_text, timesteps_audio, noisy_prompt_embeds,  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ëª¨ë¸ ì…ë ¥, íƒ€ì„ìŠ¤í…ë“¤
     noisy_audio_embeds, sigma_text, prompt_embeds, pooled_prompt_embeds, targets, prompt_embeds_uncond,  # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì˜¤ë””ì˜¤ ì„ë² ë”©, í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆ, í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ë“¤
     sigmas, sigmas_audio, model_input, loss_gesture_factor, loss_text_factor, loss_aud_factor,  # ì‹œê·¸ë§ˆë“¤, ëª¨ë¸ ì…ë ¥, ì†ì‹¤ íŒ©í„°ë“¤
     noise_scheduler_copy, raw_audio_embeds, task, task_type, prompts, noise, noise_text, noise_audio,  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬, ì›ë³¸ ì˜¤ë””ì˜¤ ì„ë² ë”©, íƒœìŠ¤í¬ ì •ë³´, ë…¸ì´ì¦ˆë“¤
     target_labels, prompt_embeds_vae_uncond, gesture_sequences) = prepare_omniges_inputs(  # íƒ€ê²Ÿ ë¼ë²¨, ë¬´ì¡°ê±´ë¶€ VAE ì„ë² ë”©, ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤
        transformer, args, text_encoder_one, text_encoder_two, text_encoder_three,  # íŠ¸ëœìŠ¤í¬ë¨¸, ì¸ìë“¤, í…ìŠ¤íŠ¸ ì¸ì½”ë”ë“¤
        accelerator, batch, gesture_vae, tokenizer_three, text_encoders, tokenizers,  # ê°€ì†ê¸°, ë°°ì¹˜, ì œìŠ¤ì²˜ VAE, í† í¬ë‚˜ì´ì €ë“¤
        tokenizer_one, tokenizer_two, weight_dtype, noise_scheduler_copy,  # CLIP í† í¬ë‚˜ì´ì €ë“¤, ê°€ì¤‘ì¹˜ íƒ€ì…, ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬
        noise_scheduler, audio_vae_factor, audiovae, text_vae_tokenizer,  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬, ì˜¤ë””ì˜¤ VAE íŒ©í„°, ì˜¤ë””ì˜¤ VAE, í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì €
        text_vae, audio_encoder, anchor, mm_encoder=mm_encoder  # í…ìŠ¤íŠ¸ VAE, ì˜¤ë””ì˜¤ ì¸ì½”ë”, ì•µì»¤, ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë”
    )
    
    # ============================================================================
    # ì†ì‹¤ ê³„ì‚°
    # ============================================================================
    loss, decode_loss, logs, task_type, model_pred, logits, target, prompts, model_pred_audio, model_pred_audio, raw_audio_embeds, model_pred_text, raw_text_embeds = compute_omniges_loss(  # ëª¨ë“  ì†ì‹¤ê³¼ ì˜ˆì¸¡ê°’ë“¤
        transformer, noisy_model_input, timesteps, timesteps_text, timesteps_audio, noisy_prompt_embeds,  # íŠ¸ëœìŠ¤í¬ë¨¸, ë…¸ì´ì¦ˆ ì…ë ¥, íƒ€ì„ìŠ¤í…ë“¤, ë…¸ì´ì¦ˆ í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
        noisy_audio_embeds, sigma_text, prompt_embeds, pooled_prompt_embeds, targets, prompt_embeds_uncond,  # ë…¸ì´ì¦ˆ ì˜¤ë””ì˜¤ ì„ë² ë”©, í…ìŠ¤íŠ¸ ì‹œê·¸ë§ˆ, í”„ë¡¬í”„íŠ¸ ì„ë² ë”©ë“¤
        sigmas, sigmas_audio, model_input, loss_gesture_factor, loss_text_factor, loss_aud_factor,  # ì‹œê·¸ë§ˆë“¤, ëª¨ë¸ ì…ë ¥, ì†ì‹¤ íŒ©í„°ë“¤
        noise_scheduler_copy, last_lr, raw_audio_embeds, task, task_type, prompts,  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬, ë§ˆì§€ë§‰ í•™ìŠµë¥ , ì›ë³¸ ì˜¤ë””ì˜¤ ì„ë² ë”©, íƒœìŠ¤í¬ ì •ë³´
        noise, noise_text, noise_audio, text_vae, target_labels, do_decode,  # ë…¸ì´ì¦ˆë“¤, í…ìŠ¤íŠ¸ VAE, íƒ€ê²Ÿ ë¼ë²¨, ë””ì½”ë”© ì—¬ë¶€
        prompt_embeds_vae_uncond, precondition_text_outputs=precondition_text_outputs,  # ë¬´ì¡°ê±´ë¶€ VAE ì„ë² ë”©, í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬
        anchor=anchor, batch=batch, gesture_sequences=gesture_sequences  # ì•µì»¤, ë°°ì¹˜, ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤
    )
    
    # ============================================================================
    # ê²°ê³¼ ë°˜í™˜ (í…ìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ë“¤ì€ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ë¦¬)
    # ============================================================================
    return loss, decode_loss, logs, task_type, model_pred, logits, target, prompts, model_pred_audio, model_pred_audio, raw_audio_embeds, model_pred_text.detach() if model_pred_text is not None else None, raw_text_embeds.detach() if raw_text_embeds is not None else None  # í…ìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ë“¤ì„ detachí•˜ì—¬ ê·¸ë˜ë””ì–¸íŠ¸ ë¶„ë¦¬


@torch.no_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™” (ê²€ì¦ ì‹œì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ê°€ í•„ìš” ì—†ìŒ)
def log_omniges_validation(
    pipeline, args, accelerator, pipeline_args, global_step,
    is_final_validation=False, prefix='', do_gesture=True, do_audio=True, do_text=True,
):
    """
    Omniges ê²€ì¦ ë¡œê¹… í•¨ìˆ˜
    ì§€ì›í•˜ëŠ” ëª¨ë“  íƒœìŠ¤í¬ë¥¼ í…ŒìŠ¤íŠ¸: t2g, g2t, a2g, g2a, t2a, a2t
    ê° íƒœìŠ¤í¬ë³„ë¡œ ìƒ˜í”Œì„ ìƒì„±í•˜ê³  wandbì— ë¡œê¹…
    
    Args:
        pipeline: OmnigesPipeline
        args: í›ˆë ¨ ì¸ìë“¤
        accelerator: Accelerate ê°€ì†ê¸°
        pipeline_args: íŒŒì´í”„ë¼ì¸ ì¸ìë“¤
        global_step: í˜„ì¬ ê¸€ë¡œë²Œ ìŠ¤í…
        is_final_validation: ìµœì¢… ê²€ì¦ ì—¬ë¶€
        prefix: ë¡œê¹… ì ‘ë‘ì‚¬
        do_gesture: ì œìŠ¤ì²˜ íƒœìŠ¤í¬ ì‹¤í–‰ ì—¬ë¶€
        do_audio: ì˜¤ë””ì˜¤ íƒœìŠ¤í¬ ì‹¤í–‰ ì—¬ë¶€
        do_text: í…ìŠ¤íŠ¸ íƒœìŠ¤í¬ ì‹¤í–‰ ì—¬ë¶€
    """
    logger.info(f"Running Omniges validation... Generating samples for all tasks")  # ê²€ì¦ ì‹œì‘ ë¡œê·¸
    pipeline = pipeline.to(accelerator.device)  # íŒŒì´í”„ë¼ì¸ì„ ê°€ì†ê¸° ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    
    generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None  # ì‹œë“œê°€ ìˆìœ¼ë©´ ì œë„ˆë ˆì´í„° ìƒì„±, ì—†ìœ¼ë©´ None
    autocast_ctx = nullcontext()  # ìë™ ìºìŠ¤íŒ… ì»¨í…ìŠ¤íŠ¸ (í˜¼í•© ì •ë°€ë„ ë¹„í™œì„±í™”)
    
    with autocast_ctx:  # ìë™ ìºìŠ¤íŒ… ì»¨í…ìŠ¤íŠ¸ ë‚´ì—ì„œ ì‹¤í–‰
        phase_name = f"test_{prefix}" if is_final_validation else f"validation_{prefix}"  # ê²€ì¦ ë‹¨ê³„ ì´ë¦„ ì„¤ì • (ìµœì¢… ê²€ì¦ì´ë©´ 'test', ì•„ë‹ˆë©´ 'validation')
        
        # ============================================================================
        # í…ìŠ¤íŠ¸ì—ì„œ ì œìŠ¤ì²˜ë¡œ ë³€í™˜ í…ŒìŠ¤íŠ¸ (t2g)
        # ============================================================================
        if do_gesture:  # ì œìŠ¤ì²˜ íƒœìŠ¤í¬ê°€ í™œì„±í™”ëœ ê²½ìš°
            try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                gesture_results = []  # ì œìŠ¤ì²˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                test_prompts = ["A person waving hello", "Someone clapping hands", "Dancing movements"]  # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
                for prompt in test_prompts:  # ê° í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´
                    result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                        prompt=prompt,  # í”„ë¡¬í”„íŠ¸
                        task='t2g',  # íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ì—ì„œ ì œìŠ¤ì²˜ë¡œ
                        seq_length=128,  # ì‹œí€€ìŠ¤ ê¸¸ì´
                        guidance_scale=7.0,  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                        generator=generator  # ì œë„ˆë ˆì´í„°
                    )
                    gesture_results.append(result)  # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                    
                # ============================================================================
                # wandbì— ë¡œê¹…
                # ============================================================================
                for tracker in accelerator.trackers:  # ëª¨ë“  íŠ¸ë˜ì»¤ì— ëŒ€í•´
                    if tracker.name == "wandb":  # wandb íŠ¸ë˜ì»¤ì¸ ê²½ìš°
                        # ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ë¥¼ numpy ë°°ì—´ë¡œ ë¡œê¹…
                        gesture_data = []  # ì œìŠ¤ì²˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                        for i, result in enumerate(gesture_results):  # ê° ê²°ê³¼ì— ëŒ€í•´
                            if hasattr(result, 'gestures'):  # ê²°ê³¼ì— gestures ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                                gesture_np = result.gestures.cpu().numpy()  # ì œìŠ¤ì²˜ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                                gesture_data.append({  # ì œìŠ¤ì²˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬ ì¶”ê°€
                                    'prompt': test_prompts[i],  # í”„ë¡¬í”„íŠ¸
                                    'gesture_shape': str(gesture_np.shape),  # ì œìŠ¤ì²˜ í˜•íƒœ
                                    'gesture_mean': float(gesture_np.mean()),  # ì œìŠ¤ì²˜ í‰ê· 
                                    'gesture_std': float(gesture_np.std())  # ì œìŠ¤ì²˜ í‘œì¤€í¸ì°¨
                                })
                        
                        df = pd.DataFrame(gesture_data)  # pandas DataFrame ìƒì„±
                        html = wandb.Html(df.to_html(), inject=True)  # HTML í…Œì´ë¸” ìƒì„±
                        tracker.log({f"t2g_{phase_name}": html})  # wandbì— ë¡œê¹…
                        
            except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                logger.warning(f"T2G validation failed: {e}")  # ê²½ê³  ë¡œê·¸ ì¶œë ¥
        
        # ============================================================================
        # ì˜¤ë””ì˜¤ì—ì„œ ì œìŠ¤ì²˜ë¡œ ë³€í™˜ í…ŒìŠ¤íŠ¸ (a2g)
        # ============================================================================
        if do_gesture and do_audio:  # ì œìŠ¤ì²˜ì™€ ì˜¤ë””ì˜¤ íƒœìŠ¤í¬ê°€ ëª¨ë‘ í™œì„±í™”ëœ ê²½ìš°
            try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                for ref_audio in ['assets/car engine.mp3']:  # ì°¸ì¡° ì˜¤ë””ì˜¤ íŒŒì¼ë“¤
                    if os.path.exists(ref_audio):  # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                        result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                            input_aud=ref_audio,  # ì…ë ¥ ì˜¤ë””ì˜¤
                            task='a2g',  # íƒœìŠ¤í¬: ì˜¤ë””ì˜¤ì—ì„œ ì œìŠ¤ì²˜ë¡œ
                            seq_length=128,  # ì‹œí€€ìŠ¤ ê¸¸ì´
                            guidance_scale=7.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                        )
                        
                        # ============================================================================
                        # wandbì— ë¡œê¹…
                        # ============================================================================
                        for tracker in accelerator.trackers:  # ëª¨ë“  íŠ¸ë˜ì»¤ì— ëŒ€í•´
                            if tracker.name == "wandb":  # wandb íŠ¸ë˜ì»¤ì¸ ê²½ìš°
                                if hasattr(result, 'gestures'):  # ê²°ê³¼ì— gestures ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                                    gesture_np = result.gestures.cpu().numpy()  # ì œìŠ¤ì²˜ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                                    gesture_info = {  # ì œìŠ¤ì²˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬
                                        'audio_file': ref_audio,  # ì˜¤ë””ì˜¤ íŒŒì¼ëª…
                                        'gesture_shape': str(gesture_np.shape),  # ì œìŠ¤ì²˜ í˜•íƒœ
                                        'gesture_mean': float(gesture_np.mean()),  # ì œìŠ¤ì²˜ í‰ê· 
                                        'gesture_std': float(gesture_np.std())  # ì œìŠ¤ì²˜ í‘œì¤€í¸ì°¨
                                    }
                                    tracker.log({f"a2g_{phase_name}": gesture_info})  # wandbì— ë¡œê¹…
                                    
            except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                logger.warning(f"A2G validation failed: {e}")  # ê²½ê³  ë¡œê·¸ ì¶œë ¥
        
        # ============================================================================
        # ì œìŠ¤ì²˜ì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í…ŒìŠ¤íŠ¸ (g2t) - ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©
        # ============================================================================
        if do_text:  # í…ìŠ¤íŠ¸ íƒœìŠ¤í¬ê°€ í™œì„±í™”ëœ ê²½ìš°
            try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                # ì‹¤ì œ í›ˆë ¨ ë°ì´í„°ì—ì„œ ì œìŠ¤ì²˜ ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
                sample_batch = next(iter(train_dataloader))  # í›ˆë ¨ ë°ì´í„°ë¡œë”ì—ì„œ ì²« ë²ˆì§¸ ë°°ì¹˜ ê°€ì ¸ì˜¤ê¸°
                real_gesture = sample_batch['gesture_sequence'][:1].to(accelerator.device)  # ì²« ë²ˆì§¸ ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©
                
                result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                    input_gesture=real_gesture,  # ì‹¤ì œ BEAT2 ì œìŠ¤ì²˜ ì…ë ¥
                    task='g2t',  # íƒœìŠ¤í¬: ì œìŠ¤ì²˜ì—ì„œ í…ìŠ¤íŠ¸ë¡œ
                    guidance_scale=2.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                )
                
                if isinstance(result, tuple) and len(result) >= 2:  # ê²°ê³¼ê°€ íŠœí”Œì´ê³  ê¸¸ì´ê°€ 2 ì´ìƒì¸ ê²½ìš°
                    generated_text = result[0][0] if result[0] else "No text generated"  # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    
                    # ============================================================================
                    # wandbì— ë¡œê¹… - BEAT2 ë©”íƒ€ë°ì´í„° í¬í•¨
                    # ============================================================================
                    for tracker in accelerator.trackers:  # ëª¨ë“  íŠ¸ë˜ì»¤ì— ëŒ€í•´
                        if tracker.name == "wandb":  # wandb íŠ¸ë˜ì»¤ì¸ ê²½ìš°
                            tracker.log({  # wandbì— ë¡œê¹…
                                f"g2t_{phase_name}": {  # ì œìŠ¤ì²˜ì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ ê²°ê³¼
                                    'generated_text': generated_text,  # ìƒì„±ëœ í…ìŠ¤íŠ¸
                                    'gesture_input_shape': str(real_gesture.shape),  # ì‹¤ì œ ì…ë ¥ ì œìŠ¤ì²˜ í˜•íƒœ
                                    'beat2_data_source': str(sample_batch.get('beat2_metadata', {}).get('audio_name', 'unknown'))  # BEAT2 ë°ì´í„° ì†ŒìŠ¤
                                }
                            })
                            
            except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                logger.warning(f"G2T validation failed: {e}")  # ê²½ê³  ë¡œê·¸ ì¶œë ¥
        
        # ============================================================================
        # í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜ í…ŒìŠ¤íŠ¸ (t2a) - OmniFlowì—ì„œ ê°€ì ¸ì˜´
        # ============================================================================
        if do_audio:  # ì˜¤ë””ì˜¤ íƒœìŠ¤í¬ê°€ í™œì„±í™”ëœ ê²½ìš°
            try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                spec, _ = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ìŠ¤í™íŠ¸ë¡œê·¸ë¨ê³¼ ê¸°íƒ€ ê²°ê³¼)
                    prompt="Music playing softly",  # í”„ë¡¬í”„íŠ¸
                    task='t2a',  # íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ë¡œ
                    guidance_scale=4.0,  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                    num_inference_steps=28  # ì¶”ë¡  ìŠ¤í… ìˆ˜
                )
                
                # ============================================================================
                # wandbì— ë¡œê¹…
                # ============================================================================
                for tracker in accelerator.trackers:  # ëª¨ë“  íŠ¸ë˜ì»¤ì— ëŒ€í•´
                    if tracker.name == "wandb":  # wandb íŠ¸ë˜ì»¤ì¸ ê²½ìš°
                        # ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì •ë³´ ë¡œê¹…
                        tracker.log({  # wandbì— ë¡œê¹…
                            f"t2a_{phase_name}": {  # í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜ ê²°ê³¼
                                'spec_shape': str(spec.shape) if hasattr(spec, 'shape') else 'No shape',  # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í˜•íƒœ
                                'spec_mean': float(np.mean(spec)) if spec is not None else 0,  # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í‰ê· 
                                'spec_std': float(np.std(spec)) if spec is not None else 0  # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í‘œì¤€í¸ì°¨
                            }
                        })
                        
            except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                logger.warning(f"T2A validation failed: {e}")  # ê²½ê³  ë¡œê·¸ ì¶œë ¥

    # ============================================================================
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    # ============================================================================
    del pipeline  # íŒŒì´í”„ë¼ì¸ ì‚­ì œ
    if torch.cuda.is_available():  # CUDAê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
        torch.cuda.empty_cache()  # CUDA ìºì‹œ ë¹„ìš°ê¸°

    return None  # None ë°˜í™˜


def parse_omniges_args(input_args=None):
    """
    Omniges í›ˆë ¨ì„ ìœ„í•œ ì¸ì íŒŒì‹± í•¨ìˆ˜
    ëª¨ë“  í›ˆë ¨ ê´€ë ¨ ì¸ìë“¤ì„ ì •ì˜í•˜ê³  íŒŒì‹±
    
    Args:
        input_args: ì…ë ¥ ì¸ì ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ sys.argv ì‚¬ìš©)
    
    Returns:
        args: íŒŒì‹±ëœ ì¸ì ê°ì²´
    """
    parser = argparse.ArgumentParser(description="Omniges multi-modal training script.")  # ì¸ì íŒŒì„œ ìƒì„±
    
    # ============================================================================
    # ê¸°ë³¸ ëª¨ë¸ ì¸ìë“¤
    # ============================================================================
    parser.add_argument(  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ
        "--pretrained_model_name_or_path",
        type=str,
        default=None,
        required=True,  # í•„ìˆ˜ ì¸ì
        help="Path to pretrained OmniFlow model",  # ì‚¬ì „ í›ˆë ¨ëœ OmniFlow ëª¨ë¸ ê²½ë¡œ
    )
    
    parser.add_argument(  # BEAT ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        "--beat_config_path",
        type=str,
        default="configs/shortcut_rvqvae_128.yaml",  # ê¸°ë³¸ ì„¤ì • íŒŒì¼
        help="Path to BEAT dataset configuration",  # BEAT ë°ì´í„°ì…‹ ì„¤ì • íŒŒì¼ ê²½ë¡œ
    )
    
    parser.add_argument(  # RVQVAE ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        "--rvqvae_checkpoints",
        type=str,
        default="./ckpt/",  # ê¸°ë³¸ ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        help="Directory containing RVQVAE checkpoints",  # RVQVAE ì²´í¬í¬ì¸íŠ¸ê°€ í¬í•¨ëœ ë””ë ‰í† ë¦¬
    )
    
    parser.add_argument(  # í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì € ê²½ë¡œ
        "--tokenizer",
        type=str,
        default='./checkpoint/OmniFlow-v0.5/vae_tokenizer',  # OmniFlow-v0.5ì˜ í† í¬ë‚˜ì´ì € ê²½ë¡œ
        help="Path to tokenizer for text VAE",  # í…ìŠ¤íŠ¸ VAEìš© í† í¬ë‚˜ì´ì € ê²½ë¡œ
    )
    
    # ============================================================================
    # í›ˆë ¨ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--output_dir", type=str, default="omniges-training", help="Output directory")  # ì¶œë ¥ ë””ë ‰í† ë¦¬
    parser.add_argument("--seed", type=int, default=None, help="Training seed")  # í›ˆë ¨ ì‹œë“œ
    parser.add_argument("--resolution", type=int, default=512, help="Resolution for compatibility")  # í˜¸í™˜ì„±ì„ ìœ„í•œ í•´ìƒë„
    parser.add_argument("--seq_length", type=int, default=128, help="Gesture sequence length")  # ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ ê¸¸ì´
    parser.add_argument("--train_batch_size", type=int, default=2, help="Batch size per device - GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ê°ì†Œ")  # ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°
    parser.add_argument("--num_train_epochs", type=int, default=100, help="Number of epochs")  # ì—í¬í¬ ìˆ˜
    parser.add_argument("--max_train_steps", type=int, default=None, help="Maximum training steps")  # ìµœëŒ€ í›ˆë ¨ ìŠ¤í…
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate")  # í•™ìŠµë¥ 
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="Gradient accumulation")  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
    parser.add_argument("--gradient_checkpointing", action="store_true", help="Use gradient checkpointing")  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ì‚¬ìš©
    parser.add_argument("--mixed_precision", type=str, default="bf16", choices=["no", "fp16", "bf16"])  # í˜¼í•© ì •ë°€ë„
    parser.add_argument("--use_ema", action="store_true", help="Use EMA")  # EMA ì‚¬ìš©
    parser.add_argument("--ema_momentum", type=float, default=0.9999, help="EMA momentum")  # EMA ëª¨ë©˜í…€
    
    # ============================================================================
    # ê²€ì¦ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--validation_prompt", type=str, default="A person waving", help="Validation prompt")  # ê²€ì¦ í”„ë¡¬í”„íŠ¸
    parser.add_argument("--num_validation_images", type=int, default=4, help="Number of validation samples")  # ê²€ì¦ ìƒ˜í”Œ ìˆ˜
    parser.add_argument("--val_every", type=int, default=500, help="Validation frequency")  # ê²€ì¦ ë¹ˆë„
    
    # ============================================================================
    # BEAT2 ë°ì´í„°ì…‹ ê´€ë ¨ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--beat2_data_root", type=str, default="./datasets/BEAT_SMPL/", help="Root directory for BEAT2 dataset")  # BEAT2 ë°ì´í„°ì…‹ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
    parser.add_argument("--beat2_wav_dir", type=str, default="wave16k", help="WAV files subdirectory name in BEAT2")  # BEAT2 WAV íŒŒì¼ í•˜ìœ„ ë””ë ‰í† ë¦¬ ì´ë¦„
    parser.add_argument("--beat2_gesture_dir", type=str, default="speakers_1234_smplx_neutral_npz", help="Gesture NPZ files subdirectory name in BEAT2")  # BEAT2 ì œìŠ¤ì²˜ NPZ íŒŒì¼ í•˜ìœ„ ë””ë ‰í† ë¦¬ ì´ë¦„
    parser.add_argument("--beat2_text_dir", type=str, default="word", help="TextGrid files subdirectory name in BEAT2")  # BEAT2 TextGrid íŒŒì¼ í•˜ìœ„ ë””ë ‰í† ë¦¬ ì´ë¦„
    parser.add_argument("--use_beat2_cache", action="store_true", help="Use cached BEAT2 data for faster loading")  # ë” ë¹ ë¥¸ ë¡œë”©ì„ ìœ„í•´ ìºì‹œëœ BEAT2 ë°ì´í„° ì‚¬ìš©
    parser.add_argument("--beat2_cache_dir", type=str, default="./datasets/beat_cache/", help="Directory to store BEAT2 cache files")  # BEAT2 ìºì‹œ íŒŒì¼ ì €ì¥ ë””ë ‰í† ë¦¬
    
    # ============================================================================
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--lr_scheduler", type=str, default="constant", help="LR scheduler type")  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
    parser.add_argument("--lr_warmup_steps", type=int, default=500, help="Warmup steps")  # ì›Œë°ì—… ìŠ¤í… ìˆ˜
    
    # ============================================================================
    # ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--weighting_scheme", type=str, default="logit_normal", choices=["sigma_sqrt", "logit_normal", "mode", "cosmap"])  # ê°€ì¤‘ì¹˜ ìŠ¤í‚´
    parser.add_argument("--logit_mean", type=float, default=0.0, help="Logit normal mean")  # ë¡œì§“ ì •ê·œ ë¶„í¬ í‰ê· 
    parser.add_argument("--logit_std", type=float, default=1.0, help="Logit normal std")  # ë¡œì§“ ì •ê·œ ë¶„í¬ í‘œì¤€í¸ì°¨
    parser.add_argument("--mode_scale", type=float, default=1.29, help="Mode scale")  # ëª¨ë“œ ìŠ¤ì¼€ì¼
    parser.add_argument("--uniform_flow", action="store_true", help="Use uniform flow matching")  # ê· ë“± í”Œë¡œìš° ë§¤ì¹­ ì‚¬ìš©
    
    # ============================================================================
    # ì²´í¬í¬ì¸íŠ¸ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--checkpointing_steps", type=int, default=500, help="Checkpoint frequency")  # ì²´í¬í¬ì¸íŠ¸ ë¹ˆë„
    parser.add_argument("--checkpoints_total_limit", type=int, default=5, help="Max checkpoints")  # ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ ìˆ˜
    parser.add_argument("--resume_from_checkpoint", type=str, default=None, help="Resume from checkpoint")  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ
    
    # ============================================================================
    # ì˜µí‹°ë§ˆì´ì € ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--optimizer", type=str, default="AdamW", help="Optimizer type")  # ì˜µí‹°ë§ˆì´ì € íƒ€ì…
    parser.add_argument("--adam_beta1", type=float, default=0.9, help="Adam beta1")  # Adam beta1
    parser.add_argument("--adam_beta2", type=float, default=0.999, help="Adam beta2")  # Adam beta2
    parser.add_argument("--adam_weight_decay", type=float, default=0, help="Weight decay")  # ê°€ì¤‘ì¹˜ ê°ì‡ 
    parser.add_argument("--adam_epsilon", type=float, default=1e-08, help="Adam epsilon")  # Adam epsilon
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="Max gradient norm")  # ìµœëŒ€ ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„
    
    # ============================================================================
    # ë¡œê¹… ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--report_to", type=str, default="wandb", help="Reporting backend")  # ë³´ê³  ë°±ì—”ë“œ
    parser.add_argument("--logging_dir", type=str, default="logs", help="Logging directory")  # ë¡œê¹… ë””ë ‰í† ë¦¬
    
    # ============================================================================
    # ê³ ê¸‰ ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--allow_tf32", action="store_true", help="Allow TF32")  # TF32 í—ˆìš©
    parser.add_argument("--dataloader_num_workers", type=int, default=0, help="Dataloader workers")  # ë°ì´í„°ë¡œë” ì›Œì»¤ ìˆ˜
    parser.add_argument("--local_rank", type=int, default=-1, help="Local rank for distributed training")  # ë¶„ì‚° í›ˆë ¨ìš© ë¡œì»¬ ë­í¬
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ VAE ì¸ìë“¤
    # ============================================================================
    parser.add_argument("--text_vae", type=str, default="./checkpoint/OmniFlow-v0.5/text_vae", help="Path to text VAE model")  # í…ìŠ¤íŠ¸ VAE ëª¨ë¸ ê²½ë¡œ
    parser.add_argument("--precondition_text_outputs", action="store_true", help="Precondition text outputs")  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬
    parser.add_argument("--anchor", action="store_true", help="Use anchor loss")  # ì•µì»¤ ì†ì‹¤ ì‚¬ìš©
    
    # ============================================================================
    # ì¸ì íŒŒì‹±
    # ============================================================================
    if input_args is not None:  # ì…ë ¥ ì¸ìê°€ ì œê³µëœ ê²½ìš°
        args = parser.parse_args(input_args)  # ì œê³µëœ ì¸ìë¡œ íŒŒì‹±
    else:  # ì…ë ¥ ì¸ìê°€ ì—†ëŠ” ê²½ìš°
        args = parser.parse_args()  # sys.argvë¡œ íŒŒì‹±
    
    # ============================================================================
    # ì¸ì ê²€ì¦
    # ============================================================================
    env_local_rank = int(os.environ.get("LOCAL_RANK", -1))  # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¡œì»¬ ë­í¬ ê°€ì ¸ì˜¤ê¸°
    if env_local_rank != -1 and env_local_rank != args.local_rank:  # í™˜ê²½ ë³€ìˆ˜ ë­í¬ê°€ ìˆê³  ì¸ì ë­í¬ì™€ ë‹¤ë¥¸ ê²½ìš°
        args.local_rank = env_local_rank  # í™˜ê²½ ë³€ìˆ˜ ë­í¬ë¡œ ì—…ë°ì´íŠ¸
        
    return args  # íŒŒì‹±ëœ ì¸ì ë°˜í™˜


def tokenize_prompt(tokenizer, prompt):
    """
    í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
    
    Args:
        tokenizer: í† í¬ë‚˜ì´ì €
        prompt: í† í¬ë‚˜ì´ì§•í•  í”„ë¡¬í”„íŠ¸
    
    Returns:
        text_input_ids: í† í¬ë‚˜ì´ì§•ëœ ì…ë ¥ IDë“¤
    """
    text_inputs = tokenizer(  # í† í¬ë‚˜ì´ì €ë¡œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        prompt,  # í”„ë¡¬í”„íŠ¸
        padding="max_length",  # ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©
        max_length=77,  # ìµœëŒ€ ê¸¸ì´ 77
        truncation=True,  # ì˜ë¼ë‚´ê¸° í™œì„±í™”
        return_tensors="pt",  # PyTorch í…ì„œë¡œ ë°˜í™˜
    )
    text_input_ids = text_inputs.input_ids  # ì…ë ¥ ID ì¶”ì¶œ
    return text_input_ids  # í† í¬ë‚˜ì´ì§•ëœ ì…ë ¥ ID ë°˜í™˜


def load_safe_tensors(fp, model):
    """
    í˜•íƒœ ê²€ì‚¬ë¥¼ í†µí•œ ì•ˆì „í•œ í…ì„œ ë¡œë”© í•¨ìˆ˜
    ëª¨ë¸ê³¼ ë¡œë“œí•  í…ì„œì˜ í˜•íƒœê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš° í•´ë‹¹ í‚¤ë¥¼ ì œê±°
    
    Args:
        fp: ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        model: í…ì„œë¥¼ ë¡œë“œí•  ëª¨ë¸
    """
    tensors = torch.load(fp, map_location='cpu')  # CPUì—ì„œ í…ì„œ ë¡œë“œ
    
    model_dict = model.state_dict()  # ëª¨ë¸ì˜ ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ê°€ì ¸ì˜¤ê¸°
    keys_to_pop = []  # ì œê±°í•  í‚¤ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    for k, v in tensors.items():  # ë¡œë“œëœ í…ì„œì˜ ê° í‚¤-ê°’ ìŒì— ëŒ€í•´
        if k in model_dict and model_dict[k].shape != v.shape:  # ëª¨ë¸ì— í‚¤ê°€ ìˆê³  í˜•íƒœê°€ ë‹¤ë¥¸ ê²½ìš°
            print(f"SIZE MISMATCH {k}: {model_dict[k].shape} vs {v.shape}")  # í˜•íƒœ ë¶ˆì¼ì¹˜ ì¶œë ¥
            keys_to_pop.append(k)  # ì œê±°í•  í‚¤ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    for k in keys_to_pop:  # ì œê±°í•  í‚¤ë“¤ì— ëŒ€í•´
        tensors.pop(k)  # í…ì„œ ë”•ì…”ë„ˆë¦¬ì—ì„œ í•´ë‹¹ í‚¤ ì œê±°
        
    res = model.load_state_dict(tensors, strict=False)  # ëª¨ë¸ì— í…ì„œ ë¡œë“œ (ì—„ê²©í•˜ì§€ ì•Šê²Œ)
    print(f"Loaded {fp}: {res}")  # ë¡œë“œ ê²°ê³¼ ì¶œë ¥
    del tensors  # í…ì„œ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ
    torch.cuda.empty_cache()  # CUDA ìºì‹œ ë¹„ìš°ê¸°


def load_safe_tensors_ema(fp, model):
    """
    EMA ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë”© í•¨ìˆ˜
    EMA (Exponential Moving Average) ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œ
    
    Args:
        fp: ë¡œë“œí•  EMA íŒŒì¼ ê²½ë¡œ
        model: EMA ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí•  ëª¨ë¸
    """
    tensors = torch.load(fp, map_location='cpu')  # CPUì—ì„œ EMA í…ì„œ ë¡œë“œ
    res = model.load_state_dict(tensors)  # ëª¨ë¸ì— EMA ê°€ì¤‘ì¹˜ ë¡œë“œ (ì—„ê²©í•˜ê²Œ)
    print(f"Loaded EMA {fp}: {res}")  # EMA ë¡œë“œ ê²°ê³¼ ì¶œë ¥
    del tensors  # í…ì„œ ë”•ì…”ë„ˆë¦¬ ì‚­ì œ
    torch.cuda.empty_cache()  # CUDA ìºì‹œ ë¹„ìš°ê¸°


def main(args):
    """
    Omniges í›ˆë ¨ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜
    ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ì´ˆê¸°í™”í•˜ê³  í›ˆë ¨ ë£¨í”„ë¥¼ ì‹œì‘
    
    Args:
        args: íŒŒì‹±ëœ í›ˆë ¨ ì¸ìë“¤
    """
    
    # ============================================================================
    # ê°€ì†ê¸° ì„¤ì •
    # ============================================================================
    logging_dir = Path(args.output_dir, args.logging_dir)  # ë¡œê¹… ë””ë ‰í† ë¦¬ ê²½ë¡œ ìƒì„±
    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)  # í”„ë¡œì íŠ¸ ì„¤ì • ìƒì„±
    kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)  # ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¸ì (ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° í—ˆìš©)
    accelerator = Accelerator(  # Accelerate ê°€ì†ê¸° ì´ˆê¸°í™”
        gradient_accumulation_steps=args.gradient_accumulation_steps,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
        mixed_precision=args.mixed_precision,  # í˜¼í•© ì •ë°€ë„ ì„¤ì •
        log_with=args.report_to,  # ë¡œê¹… ë°±ì—”ë“œ (wandb ë“±)
        project_config=accelerator_project_config,  # í”„ë¡œì íŠ¸ ì„¤ì •
        kwargs_handlers=[kwargs],  # í‚¤ì›Œë“œ ì¸ì í•¸ë“¤ëŸ¬
    )
    
    # ============================================================================
    # ë¡œê¹… ì„¤ì •
    # ============================================================================
    logging.basicConfig(  # ê¸°ë³¸ ë¡œê¹… ì„¤ì •
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",  # ë¡œê·¸ í¬ë§·
        datefmt="%m/%d/%Y %H:%M:%S",  # ë‚ ì§œ í¬ë§·
        level=logging.INFO,  # ë¡œê·¸ ë ˆë²¨
    )
    logger.info(accelerator.state, main_process_only=False)  # ê°€ì†ê¸° ìƒíƒœ ë¡œê¹… (ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì—ì„œ)
    
    if accelerator.is_local_main_process:  # ë¡œì»¬ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
        transformers.utils.logging.set_verbosity_warning()  # Transformers ë¡œê¹…ì„ ê²½ê³  ë ˆë²¨ë¡œ ì„¤ì •
        diffusers.utils.logging.set_verbosity_info()  # Diffusers ë¡œê¹…ì„ ì •ë³´ ë ˆë²¨ë¡œ ì„¤ì •
    else:  # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
        transformers.utils.logging.set_verbosity_error()  # Transformers ë¡œê¹…ì„ ì—ëŸ¬ ë ˆë²¨ë¡œ ì„¤ì •
        diffusers.utils.logging.set_verbosity_error()  # Diffusers ë¡œê¹…ì„ ì—ëŸ¬ ë ˆë²¨ë¡œ ì„¤ì •

    if args.seed is not None:  # ì‹œë“œê°€ ì„¤ì •ëœ ê²½ìš°
        set_seed(args.seed)  # ì‹œë“œ ì„¤ì •

    # ============================================================================
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    # ============================================================================
    if accelerator.is_main_process:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
        os.makedirs(args.output_dir, exist_ok=True)  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)

    # ============================================================================
    # í† í¬ë‚˜ì´ì € ë¡œë“œ (OmniFlowì™€ ë™ì¼)
    # ============================================================================
    tokenizer_one = CLIPTokenizer.from_pretrained('laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K')  # ì²« ë²ˆì§¸ CLIP í† í¬ë‚˜ì´ì €
    tokenizer_two = CLIPTokenizer.from_pretrained(  # ë‘ ë²ˆì§¸ CLIP í† í¬ë‚˜ì´ì €
        args.pretrained_model_name_or_path, subfolder="tokenizer_2"  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ë¡œë“œ
    )
    tokenizer_three = T5TokenizerFast.from_pretrained('google/flan-t5-large')  # T5 í† í¬ë‚˜ì´ì €

    # ============================================================================
    # ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œë“œ (OmniFlowì™€ ë™ì¼)
    # ============================================================================
    noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œë“œ
        args.pretrained_model_name_or_path, subfolder="scheduler", shift=1  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ë¡œë“œ, shift=1
    )
    noise_scheduler_copy = copy.deepcopy(noise_scheduler)  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸ (í›ˆë ¨ìš©)
    noise_scheduler_pipeline = copy.deepcopy(noise_scheduler)  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸ (íŒŒì´í”„ë¼ì¸ìš©)
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¡œë“œ (OmniFlowì™€ ë™ì¼)
    # ============================================================================
    text_encoder_one = CLIPTextModelWithProjection.from_pretrained(  # ì²« ë²ˆì§¸ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”
        'laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K', projection_dim=768  # íˆ¬ì˜ ì°¨ì› 768
    )
    text_encoder_two = CLIPTextModelWithProjection.from_pretrained(  # ë‘ ë²ˆì§¸ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë”
        args.pretrained_model_name_or_path, subfolder="text_encoder_2"  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì—ì„œ ë¡œë“œ
    )
    text_encoder_three = T5EncoderModel.from_pretrained('google/flan-t5-large')  # T5 ì¸ì½”ë”

    # ============================================================================
    # ê¸°íƒ€ ì¸ì½”ë” ë¡œë“œ
    # ============================================================================
    audio_encoder = LanguageBindAudio.from_pretrained('LanguageBind/LanguageBind_Audio_FT')  # ì˜¤ë””ì˜¤ ì¸ì½”ë”
    audio_encoder.text_model = nn.Identity()  # í…ìŠ¤íŠ¸ ëª¨ë¸ì„ í•­ë“± í•¨ìˆ˜ë¡œ ì„¤ì • (ì˜¤ë””ì˜¤ë§Œ ì‚¬ìš©)
    
    image_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-base-patch32")  # ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ (í˜¸í™˜ì„±ì„ ìœ„í•´)
    audio_processor_clip = LanguageBindAudioProcessor(audio_encoder.config)  # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ
    
    # ============================================================================
    # ì¸ì½”ë”ë¥¼ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    # ============================================================================
    text_encoder_one.eval()  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    text_encoder_two.eval()  # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    text_encoder_three.eval()  # ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    
    # ============================================================================
    # VAE ë¡œë“œ
    # ============================================================================
    audiovae, audio_processor = load_audio_vae()  # ì˜¤ë””ì˜¤ VAEì™€ í”„ë¡œì„¸ì„œ ë¡œë“œ
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ VAE ë¡œë“œ
    # ============================================================================
    text_vae_tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)  # í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì € ë¡œë“œ
    text_vae_tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # íŒ¨ë”© í† í° ì¶”ê°€
    
    config = AutoConfig.from_pretrained(args.text_vae)  # í…ìŠ¤íŠ¸ VAE ì„¤ì • ë¡œë“œ
    text_vae = LLamaForLatentConnector._from_config(config, torch_dtype=torch.bfloat16)  # í…ìŠ¤íŠ¸ VAE ëª¨ë¸ ìƒì„± (bfloat16)
    text_vae.prepare_tokenizer(text_vae_tokenizer)  # í† í¬ë‚˜ì´ì € ì¤€ë¹„
    text_vae.set_encoder(text_encoder_three)  # T5 ì¸ì½”ë” ì„¤ì •
    
    # ============================================================================
    # ì œìŠ¤ì²˜ VAE ìƒì„±
    # ============================================================================
    rvqvae_checkpoints = {  # RVQVAE ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë“¤
        'upper': os.path.join(args.rvqvae_checkpoints, 'net_300000_upper.pth'),  # ìƒì²´ ì²´í¬í¬ì¸íŠ¸
        'hands': os.path.join(args.rvqvae_checkpoints, 'net_300000_hands.pth'),  # ì† ì²´í¬í¬ì¸íŠ¸
        'lower_trans': os.path.join(args.rvqvae_checkpoints, 'net_300000_lower.pth'),  # í•˜ì²´+ì´ë™ ì²´í¬í¬ì¸íŠ¸
        'face': os.path.join(args.rvqvae_checkpoints, 'net_300000_face.pth')  # ì–¼êµ´ ì²´í¬í¬ì¸íŠ¸
    }
    gesture_vae = OmnigesGestureVAE(rvqvae_checkpoints)  # ì œìŠ¤ì²˜ VAE ìƒì„±
    
    # ============================================================================
    # ìƒˆë¡œìš´ Omniges íŠ¸ëœìŠ¤í¬ë¨¸ ìƒì„± - OmniFlow ì°¨ì›ì— ë§ì¶¤
    # ============================================================================
    transformer = OmnigesFlowTransformerModel(  # OmnigesFlow íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ìƒì„±
        seq_length=args.seq_length,  # ì‹œí€€ìŠ¤ ê¸¸ì´
        gesture_latent_dim=512,      # ì œìŠ¤ì²˜ ì ì¬ ì°¨ì› (128 * 4 parts)
        num_layers=24,               # OmniFlow ì‹¤ì œ ë ˆì´ì–´ ìˆ˜ (ë” í° ëª¨ë¸)
        num_attention_heads=24,      # OmniFlow ì‹¤ì œ head ìˆ˜
        attention_head_dim=64,  # ì–´í…ì…˜ í—¤ë“œ ì°¨ì›
        joint_attention_dim=4096,  # ê³µë™ ì–´í…ì…˜ ì°¨ì›
        caption_projection_dim=1536, # OmniFlow ì‹¤ì œ ì°¨ì› 1536
        pooled_projection_dim=2048,  # í’€ë§ íˆ¬ì˜ ì°¨ì›
        audio_input_dim=8,  # ì˜¤ë””ì˜¤ ì…ë ¥ ì°¨ì›
        gesture_output_dim=512,  # ì œìŠ¤ì²˜ ì¶œë ¥ ì°¨ì›
        add_audio=True,  # ì˜¤ë””ì˜¤ ì¶”ê°€
        use_audio_mae=False,  # ì˜¤ë””ì˜¤ MAE ì‚¬ìš© ì•ˆí•¨
        drop_gesture=False,  # ì œìŠ¤ì²˜ ë“œë¡­ ì•ˆí•¨
        drop_text=False,  # í…ìŠ¤íŠ¸ ë“œë¡­ ì•ˆí•¨
        drop_audio=False  # ì˜¤ë””ì˜¤ ë“œë¡­ ì•ˆí•¨
    )
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ ë””ì½”ë” ì„¤ì •
    # ============================================================================
    transformer.set_text_decoder(text_vae)  # íŠ¸ëœìŠ¤í¬ë¨¸ì— í…ìŠ¤íŠ¸ ë””ì½”ë” ì„¤ì •
    
    # ============================================================================
    # OmniFlow ê°€ì¤‘ì¹˜ ë¡œë“œ (ê°€ëŠ¥í•œ ê²½ìš°)
    # ============================================================================
    if args.pretrained_model_name_or_path:  # ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°
        fp = os.path.join(args.pretrained_model_name_or_path, 'transformer/diffusion_pytorch_model.bin')  # íŠ¸ëœìŠ¤í¬ë¨¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ
        if os.path.exists(fp):  # íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            try:  # ì‹œë„
                load_safe_tensors(fp, transformer)  # ì•ˆì „í•œ í…ì„œ ë¡œë”©
                logger.info("Loaded OmniFlow weights successfully")  # ì„±ê³µ ë¡œê·¸
            except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                logger.warning(f"Could not load OmniFlow weights: {e}")  # ê²½ê³  ë¡œê·¸
    
    # ============================================================================
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ì„¤ì •
    # ============================================================================
    transformer.requires_grad_(True)  # íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° í™œì„±í™”
    text_vae.requires_grad_(False)  # í…ìŠ¤íŠ¸ VAEëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    audio_encoder.requires_grad_(False)  # ì˜¤ë””ì˜¤ ì¸ì½”ë”ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    audiovae.requires_grad_(False)  # ì˜¤ë””ì˜¤ VAEëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    gesture_vae.requires_grad_(False)  # ì œìŠ¤ì²˜ VAEëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    text_encoder_one.requires_grad_(False)  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    text_encoder_two.requires_grad_(False)  # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    text_encoder_three.requires_grad_(False)  # ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    
    # ============================================================================
    # ê°€ì¤‘ì¹˜ ë°ì´í„° íƒ€ì… ì„¤ì •
    # ============================================================================
    weight_dtype = torch.float32  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ë°ì´í„° íƒ€ì…
    if accelerator.mixed_precision == "fp16":  # í˜¼í•© ì •ë°€ë„ê°€ fp16ì¸ ê²½ìš°
        weight_dtype = torch.float16  # float16ìœ¼ë¡œ ì„¤ì •
    elif accelerator.mixed_precision == "bf16":  # í˜¼í•© ì •ë°€ë„ê°€ bf16ì¸ ê²½ìš°
        weight_dtype = torch.bfloat16  # bfloat16ìœ¼ë¡œ ì„¤ì •
        
    # ============================================================================
    # EMA ì„¤ì •
    # ============================================================================
    if args.use_ema and accelerator.is_main_process:  # EMA ì‚¬ìš©ì´ í™œì„±í™”ë˜ê³  ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
        ema_transformer = EMAModel(transformer.parameters(), decay=args.ema_momentum)  # EMA ëª¨ë¸ ìƒì„±
        
    # ============================================================================
    # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    # ============================================================================
    gesture_vae.to(accelerator.device, dtype=weight_dtype)  # ì œìŠ¤ì²˜ VAEë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    audiovae.to(accelerator.device, dtype=torch.float32)  # ì˜¤ë””ì˜¤ VAEë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™ (float32)
    text_vae.to(accelerator.device)  # í…ìŠ¤íŠ¸ VAEë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    text_encoder_one.to(accelerator.device, dtype=weight_dtype)  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    text_encoder_two.to(accelerator.device, dtype=weight_dtype)  # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    text_encoder_three.to(accelerator.device, dtype=weight_dtype)  # ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    audio_encoder.to(accelerator.device, dtype=weight_dtype)  # ì˜¤ë””ì˜¤ ì¸ì½”ë”ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
    
    # ============================================================================
    # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
    # ============================================================================
    if args.gradient_checkpointing:  # ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì´ í™œì„±í™”ëœ ê²½ìš°
        transformer.enable_gradient_checkpointing()  # íŠ¸ëœìŠ¤í¬ë¨¸ì— ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”
        # ëª¨ë“  í…ìŠ¤íŠ¸ ëª¨ë¸ë“¤ì˜ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… ë¹„í™œì„±í™” (í˜¸í™˜ì„± ë¬¸ì œ)
        if hasattr(text_encoder_one, 'gradient_checkpointing_enable'):
            text_encoder_one.gradient_checkpointing_disable()
        if hasattr(text_encoder_two, 'gradient_checkpointing_enable'):
            text_encoder_two.gradient_checkpointing_disable()
        if hasattr(text_encoder_three, 'gradient_checkpointing_enable'):
            text_encoder_three.gradient_checkpointing_disable()
        if hasattr(text_vae, 'gradient_checkpointing_enable'):
            text_vae.gradient_checkpointing_disable()
        # Llama ëª¨ë¸ì˜ gradient checkpointing ë¹„í™œì„±í™”
        if hasattr(text_vae.model, 'gradient_checkpointing_enable'):
            text_vae.model.gradient_checkpointing_disable()
        
    # ============================================================================
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    # ============================================================================
    optimizer = torch.optim.AdamW(  # AdamW ì˜µí‹°ë§ˆì´ì € ìƒì„±
        transformer.parameters(),  # íŠ¸ëœìŠ¤í¬ë¨¸ íŒŒë¼ë¯¸í„°
        lr=args.learning_rate,  # í•™ìŠµë¥ 
        betas=(args.adam_beta1, args.adam_beta2),  # Adam ë² íƒ€ ê°’ë“¤
        weight_decay=args.adam_weight_decay,  # ê°€ì¤‘ì¹˜ ê°ì‡ 
        eps=args.adam_epsilon,  # Adam epsilon
    )
    
    # ============================================================================
    # BEAT2 ë°ì´í„°ì…‹ ìƒì„± (ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©, ë”ë¯¸ ë°ì´í„° ì œê±°)
    # ============================================================================
    logger.info("ğŸ”§ Creating BEAT2 dataset for Omniges training...")
    logger.info(f"  â€¢ BEAT2 data root: {args.beat2_data_root}")
    logger.info(f"  â€¢ BEAT2 config: {args.beat_config_path}")
    logger.info(f"  â€¢ WAV directory: {args.beat2_wav_dir}")
    logger.info(f"  â€¢ Gesture directory: {args.beat2_gesture_dir}")
    logger.info(f"  â€¢ TextGrid directory: {args.beat2_text_dir}")
    logger.info(f"  â€¢ Using cache: {args.use_beat2_cache}")
    
    train_dataset = OmnigesDataset(  # Omniges ë°ì´í„°ì…‹ ìƒì„± - ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©
        beat_config_path=args.beat_config_path,  # BEAT2 ì„¤ì • íŒŒì¼ ê²½ë¡œ
        task_weights=[1/6] * 6,  # ëª¨ë“  íƒœìŠ¤í¬ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ (t2g, g2t, a2g, g2a, t2a, a2t)
        size=args.resolution,  # í•´ìƒë„ (í˜¸í™˜ì„±ìš©)
        is_train=True,  # í›ˆë ¨ ëª¨ë“œ
        image_processor=image_processor,  # ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ)
        audio_processor=audio_processor,  # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ (BEAT2 WAV íŒŒì¼ìš©)
        audio_processor_clip=audio_processor_clip,  # CLIP ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ (BEAT2 WAV íŒŒì¼ìš©)
        # BEAT2 ë°ì´í„°ì…‹ ì¶”ê°€ ì„¤ì •
        beat2_data_root=args.beat2_data_root,  # BEAT2 ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        beat2_wav_dir=args.beat2_wav_dir,      # WAV íŒŒì¼ ë””ë ‰í† ë¦¬
        beat2_gesture_dir=args.beat2_gesture_dir,  # ì œìŠ¤ì²˜ NPZ íŒŒì¼ ë””ë ‰í† ë¦¬
        beat2_text_dir=args.beat2_text_dir,    # TextGrid íŒŒì¼ ë””ë ‰í† ë¦¬
        use_beat2_cache=args.use_beat2_cache,  # ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        beat2_cache_dir=args.beat2_cache_dir   # ìºì‹œ ë””ë ‰í† ë¦¬
    )
    
    # ============================================================================
    # ë°ì´í„°ë¡œë” ìƒì„±
    # ============================================================================
    train_dataloader = torch.utils.data.DataLoader(  # í›ˆë ¨ ë°ì´í„°ë¡œë” ìƒì„±
        train_dataset,  # í›ˆë ¨ ë°ì´í„°ì…‹
        batch_size=args.train_batch_size,  # ë°°ì¹˜ í¬ê¸°
        shuffle=True,  # ì…”í”Œ í™œì„±í™”
        collate_fn=omniges_collate_fn,  # ì»¤ìŠ¤í…€ ì½œë ˆì´íŠ¸ í•¨ìˆ˜
        num_workers=args.dataloader_num_workers,  # ì›Œì»¤ ìˆ˜
    )
    
    # ============================================================================
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    # ============================================================================
    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)  # ì—í¬í¬ë‹¹ ì—…ë°ì´íŠ¸ ìŠ¤í… ìˆ˜ ê³„ì‚°
    if args.max_train_steps is None:  # ìµœëŒ€ í›ˆë ¨ ìŠ¤í…ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°
        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch  # ì—í¬í¬ ìˆ˜ë¡œ ê³„ì‚°
    else:  # ìµœëŒ€ í›ˆë ¨ ìŠ¤í…ì´ ì§€ì •ëœ ê²½ìš°
        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)  # ì—í¬í¬ ìˆ˜ ì¬ê³„ì‚°
    
    lr_scheduler = get_scheduler(  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
        args.lr_scheduler,  # ìŠ¤ì¼€ì¤„ëŸ¬ íƒ€ì…
        optimizer=optimizer,  # ì˜µí‹°ë§ˆì´ì €
        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,  # ì›Œë°ì—… ìŠ¤í… ìˆ˜ (í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³±í•˜ê¸°)
        num_training_steps=args.max_train_steps * accelerator.num_processes,  # í›ˆë ¨ ìŠ¤í… ìˆ˜ (í”„ë¡œì„¸ìŠ¤ ìˆ˜ ê³±í•˜ê¸°)
    )
    
    # ============================================================================
    # ê°€ì†ê¸°ë¡œ ì¤€ë¹„
    # ============================================================================
    transformer, optimizer, lr_scheduler = accelerator.prepare(transformer, optimizer, lr_scheduler)  # ê°€ì†ê¸°ë¡œ ëª¨ë¸, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ì¤€ë¹„
    
    # ============================================================================
    # í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¦¬ìŠ¤íŠ¸ ìƒì„±
    # ============================================================================
    tokenizers = [tokenizer_one, tokenizer_two, tokenizer_three]  # í† í¬ë‚˜ì´ì € ë¦¬ìŠ¤íŠ¸
    text_encoders = [text_encoder_one, text_encoder_two, text_encoder_three]  # í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¦¬ìŠ¤íŠ¸
    
    # ============================================================================
    # í›ˆë ¨ ì •ë³´
    # ============================================================================
    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps  # ì´ ë°°ì¹˜ í¬ê¸° ê³„ì‚°
    
    logger.info("***** Running Omniges Training *****")  # í›ˆë ¨ ì‹œì‘ ë¡œê·¸
    logger.info(f"  Num examples = {len(train_dataset)}")  # ì˜ˆì œ ìˆ˜
    logger.info(f"  Num batches each epoch = {len(train_dataloader)}")  # ì—í¬í¬ë‹¹ ë°°ì¹˜ ìˆ˜
    logger.info(f"  Num Epochs = {args.num_train_epochs}")  # ì—í¬í¬ ìˆ˜
    logger.info(f"  Instantaneous batch size per device = {args.train_batch_size}")  # ë””ë°”ì´ìŠ¤ë‹¹ ì¦‰ì‹œ ë°°ì¹˜ í¬ê¸°
    logger.info(f"  Total train batch size = {total_batch_size}")  # ì´ í›ˆë ¨ ë°°ì¹˜ í¬ê¸°
    logger.info(f"  Gradient Accumulation steps = {args.gradient_accumulation_steps}")  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
    logger.info(f"  Total optimization steps = {args.max_train_steps}")  # ì´ ìµœì í™” ìŠ¤í…
    logger.info(f"  Supported tasks: t2g, g2t, a2g, g2a, t2a, a2t")  # ì§€ì›í•˜ëŠ” íƒœìŠ¤í¬ë“¤
    
    global_step = 0  # ê¸€ë¡œë²Œ ìŠ¤í… ì´ˆê¸°í™”
    first_epoch = 0  # ì²« ë²ˆì§¸ ì—í¬í¬ ì´ˆê¸°í™”
    
    # ============================================================================
    # ì§„í–‰ë¥  ë°”
    # ============================================================================
    progress_bar = tqdm(  # tqdm ì§„í–‰ë¥  ë°” ìƒì„±
        range(0, args.max_train_steps * args.gradient_accumulation_steps),  # ì§„í–‰ ë²”ìœ„
        initial=0,  # ì´ˆê¸°ê°’
        desc="Steps",  # ì„¤ëª…
        disable=not accelerator.is_local_main_process,  # ë¡œì»¬ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì•„ë‹ˆë©´ ë¹„í™œì„±í™”
    )
    
    # ============================================================================
    # ì¶”ì  ì´ˆê¸°í™”
    # ============================================================================
    if accelerator.is_main_process:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
        accelerator.init_trackers("omniges-training", config=vars(args))  # ì¶”ì ê¸° ì´ˆê¸°í™” (wandb ë“±)
    
    # ============================================================================
    # í›ˆë ¨ ë£¨í”„
    # ============================================================================
    for epoch in range(first_epoch, args.num_train_epochs):  # ì—í¬í¬ ë£¨í”„ (ì²« ë²ˆì§¸ ì—í¬í¬ë¶€í„° ì„¤ì •ëœ ì—í¬í¬ ìˆ˜ê¹Œì§€)
        transformer.train()  # íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        
        for step, batch in enumerate(train_dataloader):  # ë°°ì¹˜ ë£¨í”„ (ë°ì´í„°ë¡œë”ì˜ ê° ë°°ì¹˜ì— ëŒ€í•´)
            with accelerator.accumulate([transformer]):  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì»¨í…ìŠ¤íŠ¸ (íŠ¸ëœìŠ¤í¬ë¨¸ë§Œ ëˆ„ì )
                
                # ============================================================================
                # ìˆœì „íŒŒ
                # ============================================================================
                loss, decode_loss, logs, task_type, model_pred, logits, target, prompts, model_pred_audio, model_pred_audio, raw_audio_embeds, model_pred_text, raw_text_embeds = transformer(  # íŠ¸ëœìŠ¤í¬ë¨¸ì— ìˆœì „íŒŒí•˜ì—¬ ëª¨ë“  ê²°ê³¼ ë°›ê¸°
                    kkwargs={  # í‚¤ì›Œë“œ ì¸ì ë”•ì…”ë„ˆë¦¬
                        'args': args,  # í›ˆë ¨ ì¸ìë“¤
                        'text_encoder_one': text_encoder_one,  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”
                        'text_encoder_two': text_encoder_two,  # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”
                        'text_encoder_three': text_encoder_three,  # ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”
                        'accelerator': accelerator.device,  # ê°€ì†ê¸° ë””ë°”ì´ìŠ¤
                        'batch': batch,  # ë°°ì¹˜ ë°ì´í„°
                        'gesture_vae': gesture_vae,  # ì œìŠ¤ì²˜ VAE ì‚¬ìš© (ì´ë¯¸ì§€ VAE ëŒ€ì‹ )
                        'tokenizer_three': tokenizer_three,  # T5 í† í¬ë‚˜ì´ì €
                        'text_encoders': text_encoders,  # í…ìŠ¤íŠ¸ ì¸ì½”ë” ë¦¬ìŠ¤íŠ¸
                        'tokenizers': tokenizers,  # í† í¬ë‚˜ì´ì € ë¦¬ìŠ¤íŠ¸
                        'tokenizer_one': tokenizer_one,  # ì²« ë²ˆì§¸ CLIP í† í¬ë‚˜ì´ì €
                        'tokenizer_two': tokenizer_two,  # ë‘ ë²ˆì§¸ CLIP í† í¬ë‚˜ì´ì €
                        'weight_dtype': weight_dtype,  # ê°€ì¤‘ì¹˜ ë°ì´í„° íƒ€ì…
                        'noise_scheduler_copy': noise_scheduler_copy,  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì‚¬ë³¸
                        'noise_scheduler': noise_scheduler,  # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬
                        'audio_vae_factor': 1,  # ì˜¤ë””ì˜¤ VAE íŒ©í„°
                        'audiovae': audiovae,  # ì˜¤ë””ì˜¤ VAE
                        'text_vae_tokenizer': text_vae_tokenizer,  # í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì €
                        'last_lr': lr_scheduler.get_last_lr()[0],  # ë§ˆì§€ë§‰ í•™ìŠµë¥ 
                        'text_vae': text_vae,  # í…ìŠ¤íŠ¸ VAE
                        'audio_encoder': audio_encoder,  # ì˜¤ë””ì˜¤ ì¸ì½”ë”
                        'do_decode': False,  # ë””ì½”ë”© ë¹„í™œì„±í™”
                        'precondition_text_outputs': args.precondition_text_outputs,  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬
                        'anchor': args.anchor,  # ì•µì»¤ í”Œë˜ê·¸
                        'mm_encoder': None,  # ë©€í‹°ëª¨ë‹¬ ì¸ì½”ë” (None)
                    },
                    forward_function=omniges_forward_pass  # ìˆœì „íŒŒ í•¨ìˆ˜ ì§€ì •
                )

                # ============================================================================
                # ì—­ì „íŒŒ
                # ============================================================================
                accelerator.backward(loss)  # ê°€ì†ê¸°ë¥¼ ì‚¬ìš©í•œ ì—­ì „íŒŒ
                
                # ============================================================================
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                # ============================================================================
                if accelerator.sync_gradients:  # ê·¸ë˜ë””ì–¸íŠ¸ ë™ê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
                    params_to_clip = transformer.parameters()  # í´ë¦¬í•‘í•  íŒŒë¼ë¯¸í„°ë“¤ (íŠ¸ëœìŠ¤í¬ë¨¸ë§Œ)
                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)  # ê·¸ë˜ë””ì–¸íŠ¸ ë…¸ë¦„ í´ë¦¬í•‘
                
                # ============================================================================
                # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                # ============================================================================
                optimizer.step()  # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
                lr_scheduler.step()  # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
                optimizer.zero_grad()  # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                
                # ============================================================================
                # EMA ì—…ë°ì´íŠ¸
                # ============================================================================
                if accelerator.sync_gradients:  # ê·¸ë˜ë””ì–¸íŠ¸ ë™ê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
                    if args.use_ema and accelerator.is_main_process:  # EMA ì‚¬ìš©ì´ í™œì„±í™”ë˜ê³  ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
                        if global_step % 100 == 0:  # 100 ìŠ¤í…ë§ˆë‹¤ EMA ì—…ë°ì´íŠ¸
                            ema_transformer.step(transformer.parameters())  # EMA ëª¨ë¸ ì—…ë°ì´íŠ¸

            # ============================================================================
            # ì§„í–‰ë¥  ì¶”ì 
            # ============================================================================
            progress_bar.update(1)  # ì§„í–‰ë¥  ë°” ì—…ë°ì´íŠ¸
            if accelerator.sync_gradients:  # ê·¸ë˜ë””ì–¸íŠ¸ ë™ê¸°í™”ê°€ í•„ìš”í•œ ê²½ìš°
                global_step += 1  # ê¸€ë¡œë²Œ ìŠ¤í… ì¦ê°€
                
                # ============================================================================
                # ë©”íŠ¸ë¦­ ë¡œê¹…
                # ============================================================================
                progress_bar.set_postfix(**logs)  # ì§„í–‰ë¥  ë°”ì— ë¡œê·¸ ì •ë³´ í‘œì‹œ
                accelerator.log(logs, step=global_step)  # ê°€ì†ê¸°ì— ë¡œê·¸ ê¸°ë¡
                
                # ============================================================================
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                # ============================================================================
                if accelerator.is_main_process and global_step % args.checkpointing_steps == 0:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì´ê³  ì²´í¬í¬ì¸íŠ¸ ìŠ¤í…ì¸ ê²½ìš°
                    save_path = os.path.join(args.output_dir, f"checkpoint-{global_step}")  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
                    accelerator.save_state(save_path)  # ê°€ì†ê¸° ìƒíƒœ ì €ì¥
                    
                    # EMA ë³„ë„ ì €ì¥
                    if args.use_ema:  # EMA ì‚¬ìš©ì´ í™œì„±í™”ëœ ê²½ìš°
                        ema_path = os.path.join(save_path, "ema_transformer.pt")  # EMA ëª¨ë¸ ì €ì¥ ê²½ë¡œ
                        torch.save(ema_transformer.state_dict(), ema_path)  # EMA ëª¨ë¸ ìƒíƒœ ì €ì¥
                    
                    logger.info(f"Saved checkpoint to {save_path}")  # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë¡œê·¸
                
                # ============================================================================
                # ê²€ì¦ - ëª¨ë“  íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸
                # ============================================================================
                if global_step % args.val_every == 0 and global_step > 0:  # ê²€ì¦ ë¹ˆë„ì— ë„ë‹¬í•˜ê³  ê¸€ë¡œë²Œ ìŠ¤í…ì´ 0ë³´ë‹¤ í° ê²½ìš°
                    transformer.eval()  # íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
                    
                    # ============================================================================
                    # ê²€ì¦ íŒŒì´í”„ë¼ì¸ ìƒì„±
                    # ============================================================================
                    pipeline = OmnigesPipeline(  # Omniges íŒŒì´í”„ë¼ì¸ ìƒì„±
                        transformer=accelerator.unwrap_model(transformer),  # ê°€ì†ê¸°ì—ì„œ ì–¸ë˜í•‘ëœ íŠ¸ëœìŠ¤í¬ë¨¸
                        scheduler=noise_scheduler_pipeline,  # íŒŒì´í”„ë¼ì¸ìš© ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ëŸ¬
                        gesture_vae=gesture_vae,  # ì œìŠ¤ì²˜ VAE
                        text_encoder=accelerator.unwrap_model(text_encoder_one),  # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”
                        text_encoder_2=accelerator.unwrap_model(text_encoder_two),  # ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”
                        text_encoder_3=accelerator.unwrap_model(text_encoder_three),  # ì„¸ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì¸ì½”ë”
                        tokenizer=tokenizer_one,  # ì²« ë²ˆì§¸ í† í¬ë‚˜ì´ì €
                        tokenizer_2=tokenizer_two,  # ë‘ ë²ˆì§¸ í† í¬ë‚˜ì´ì €
                        tokenizer_3=tokenizer_three,  # ì„¸ ë²ˆì§¸ í† í¬ë‚˜ì´ì €
                        audio_vae=audiovae,  # ì˜¤ë””ì˜¤ VAE
                        audio_processor=audio_processor,  # ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ
                        audio_processor_clip=audio_processor_clip,  # CLIP ì˜¤ë””ì˜¤ í”„ë¡œì„¸ì„œ
                        audio_encoder=accelerator.unwrap_model(audio_encoder),  # ì˜¤ë””ì˜¤ ì¸ì½”ë”
                        text_vae=text_vae,  # í…ìŠ¤íŠ¸ VAE
                        text_vae_tokenizer=text_vae_tokenizer,  # í…ìŠ¤íŠ¸ VAE í† í¬ë‚˜ì´ì €
                        text_x0=args.precondition_text_outputs,  # í…ìŠ¤íŠ¸ ì¶œë ¥ ì „ì²˜ë¦¬
                    )
                    
                    # ============================================================================
                    # ğŸ¯ ëª¨ë“  íƒœìŠ¤í¬ë³„ ê²€ì¦ (OmniFlow ë°©ì‹ í™•ì¥)
                    # ============================================================================
                    validation_results = {}  # ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
                    
                    # ============================================================================
                    # 1. í…ìŠ¤íŠ¸ì—ì„œ ì œìŠ¤ì²˜ë¡œ ë³€í™˜ (t2g)
                    # ============================================================================
                    try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                        t2g_result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                            prompt=args.validation_prompt,  # ê²€ì¦ í”„ë¡¬í”„íŠ¸
                            task='t2g',  # íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ì—ì„œ ì œìŠ¤ì²˜ë¡œ
                            seq_length=128,  # ì‹œí€€ìŠ¤ ê¸¸ì´
                            guidance_scale=7.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                        )
                        if hasattr(t2g_result, 'gestures'):  # ê²°ê³¼ì— gestures ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                            gesture_np = t2g_result.gestures.cpu().numpy()  # ì œìŠ¤ì²˜ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                            validation_results['t2g'] = {  # T2G ê²€ì¦ ê²°ê³¼ ì €ì¥
                                'shape': gesture_np.shape,  # ì œìŠ¤ì²˜ í˜•íƒœ
                                'mean': float(gesture_np.mean()),  # ì œìŠ¤ì²˜ í‰ê· 
                                'std': float(gesture_np.std())  # ì œìŠ¤ì²˜ í‘œì¤€í¸ì°¨
                            }
                            logger.info(f"  âœ… T2G validation: {gesture_np.shape}")  # ì„±ê³µ ë¡œê·¸
                    except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                        logger.warning(f"  âš ï¸ T2G validation failed: {e}")  # ê²½ê³  ë¡œê·¸
                    
                    # ============================================================================
                    # 2. ì œìŠ¤ì²˜ì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (g2t) - ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©
                    # ============================================================================
                    try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                        # í˜„ì¬ ë°°ì¹˜ì—ì„œ ì‹¤ì œ ì œìŠ¤ì²˜ ë°ì´í„° ì‚¬ìš©
                        real_gesture = batch['gesture_sequence'][:1].to(accelerator.device)  # ì²« ë²ˆì§¸ ì œìŠ¤ì²˜ ì‹œí€€ìŠ¤ë§Œ ì‚¬ìš©
                        g2t_result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                            input_gesture=real_gesture,  # ì‹¤ì œ BEAT2 ì œìŠ¤ì²˜ ì…ë ¥
                            task='g2t',  # íƒœìŠ¤í¬: ì œìŠ¤ì²˜ì—ì„œ í…ìŠ¤íŠ¸ë¡œ
                            guidance_scale=2.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                        )
                        if isinstance(g2t_result, tuple) and len(g2t_result) >= 2:  # ê²°ê³¼ê°€ íŠœí”Œì´ê³  ê¸¸ì´ê°€ 2 ì´ìƒì¸ ê²½ìš°
                            generated_text = g2t_result[0][0] if g2t_result[0] else "No text"  # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            validation_results['g2t'] = {  # G2T ê²€ì¦ ê²°ê³¼ ì €ì¥
                                'text': generated_text,  # ìƒì„±ëœ í…ìŠ¤íŠ¸
                                'length': len(generated_text.split()),  # í…ìŠ¤íŠ¸ ê¸¸ì´
                                'beat2_source': str(batch.get('beat2_metadata', {}).get('audio_name', ['unknown'])[0] if isinstance(batch.get('beat2_metadata', {}).get('audio_name', 'unknown'), list) else batch.get('beat2_metadata', {}).get('audio_name', 'unknown'))  # BEAT2 ë°ì´í„° ì†ŒìŠ¤
                            }
                            logger.info(f"  âœ… G2T validation (BEAT2): '{generated_text[:30]}...'")  # ì„±ê³µ ë¡œê·¸
                    except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                        logger.warning(f"  âš ï¸ G2T validation failed: {e}")  # ê²½ê³  ë¡œê·¸
                    
                    # ============================================================================
                    # 3. ì˜¤ë””ì˜¤ì—ì„œ ì œìŠ¤ì²˜ë¡œ ë³€í™˜ (a2g) - ì˜¤ë””ì˜¤ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°
                    # ============================================================================
                    if os.path.exists('./assets/car engine.mp3'):  # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                        try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                            a2g_result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                                input_aud='./assets/car engine.mp3',  # ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
                                task='a2g',  # íƒœìŠ¤í¬: ì˜¤ë””ì˜¤ì—ì„œ ì œìŠ¤ì²˜ë¡œ
                                seq_length=128,  # ì‹œí€€ìŠ¤ ê¸¸ì´
                                guidance_scale=7.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                            )
                            if hasattr(a2g_result, 'gestures'):  # ê²°ê³¼ì— gestures ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                                gesture_np = a2g_result.gestures.cpu().numpy()  # ì œìŠ¤ì²˜ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
                                validation_results['a2g'] = {  # A2G ê²€ì¦ ê²°ê³¼ ì €ì¥
                                    'shape': gesture_np.shape,  # ì œìŠ¤ì²˜ í˜•íƒœ
                                    'mean': float(gesture_np.mean())  # ì œìŠ¤ì²˜ í‰ê· 
                                }
                                logger.info(f"  âœ… A2G validation: {gesture_np.shape}")  # ì„±ê³µ ë¡œê·¸
                        except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                            logger.warning(f"  âš ï¸ A2G validation failed: {e}")  # ê²½ê³  ë¡œê·¸
                    
                    # ============================================================================
                    # 4. ì œìŠ¤ì²˜ì—ì„œ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜ (g2a) - ì‹¤ì œ BEAT2 ë°ì´í„° ì‚¬ìš©
                    # ============================================================================
                    try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                        g2a_result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                            input_gesture=real_gesture,  # ì‹¤ì œ BEAT2 ì œìŠ¤ì²˜ ì…ë ¥ (ì´ì „ì— ì‚¬ìš©ëœ ì‹¤ì œ ì œìŠ¤ì²˜)
                            task='g2a',  # íƒœìŠ¤í¬: ì œìŠ¤ì²˜ì—ì„œ ì˜¤ë””ì˜¤ë¡œ
                            guidance_scale=4.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                        )
                        if isinstance(g2a_result, tuple) and len(g2a_result) >= 1:  # ê²°ê³¼ê°€ íŠœí”Œì´ê³  ê¸¸ì´ê°€ 1 ì´ìƒì¸ ê²½ìš°
                            audio_spec = g2a_result[0]  # ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶”ì¶œ
                            validation_results['g2a'] = {  # G2A ê²€ì¦ ê²°ê³¼ ì €ì¥
                                'audio_shape': str(audio_spec.shape) if hasattr(audio_spec, 'shape') else 'No shape',  # ì˜¤ë””ì˜¤ í˜•íƒœ
                                'audio_mean': float(np.mean(audio_spec)) if audio_spec is not None else 0  # ì˜¤ë””ì˜¤ í‰ê· 
                            }
                            logger.info(f"  âœ… G2A validation: audio generated")  # ì„±ê³µ ë¡œê·¸
                    except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                        logger.warning(f"  âš ï¸ G2A validation failed: {e}")  # ê²½ê³  ë¡œê·¸
                    
                    # ============================================================================
                    # 5. í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜ (t2a) - OmniFlow ë°©ì‹
                    # ============================================================================
                    try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                        t2a_result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                            prompt="Music playing",  # í”„ë¡¬í”„íŠ¸
                            task='t2a',  # íƒœìŠ¤í¬: í…ìŠ¤íŠ¸ì—ì„œ ì˜¤ë””ì˜¤ë¡œ
                            guidance_scale=4.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                        )
                        if isinstance(t2a_result, tuple) and len(t2a_result) >= 1:  # ê²°ê³¼ê°€ íŠœí”Œì´ê³  ê¸¸ì´ê°€ 1 ì´ìƒì¸ ê²½ìš°
                            audio_spec = t2a_result[0]  # ì˜¤ë””ì˜¤ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶”ì¶œ
                            validation_results['t2a'] = {  # T2A ê²€ì¦ ê²°ê³¼ ì €ì¥
                                'audio_shape': str(audio_spec.shape) if hasattr(audio_spec, 'shape') else 'No shape'  # ì˜¤ë””ì˜¤ í˜•íƒœ
                            }
                            logger.info(f"  âœ… T2A validation: audio generated")  # ì„±ê³µ ë¡œê·¸
                    except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                        logger.warning(f"  âš ï¸ T2A validation failed: {e}")  # ê²½ê³  ë¡œê·¸
                    
                    # ============================================================================
                    # 6. ì˜¤ë””ì˜¤ì—ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (a2t) - OmniFlow ë°©ì‹
                    # ============================================================================
                    if os.path.exists('./assets/car engine.mp3'):  # ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°
                        try:  # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œì‘
                            a2t_result = pipeline(  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                                input_aud='./assets/car engine.mp3',  # ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
                                task='a2t',  # íƒœìŠ¤í¬: ì˜¤ë””ì˜¤ì—ì„œ í…ìŠ¤íŠ¸ë¡œ
                                guidance_scale=2.0  # ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼
                            )
                            if isinstance(a2t_result, tuple) and len(a2t_result) >= 2:  # ê²°ê³¼ê°€ íŠœí”Œì´ê³  ê¸¸ì´ê°€ 2 ì´ìƒì¸ ê²½ìš°
                                generated_text = a2t_result[0][0] if a2t_result[0] else "No text"  # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                                validation_results['a2t'] = {  # A2T ê²€ì¦ ê²°ê³¼ ì €ì¥
                                    'text': generated_text,  # ìƒì„±ëœ í…ìŠ¤íŠ¸
                                    'length': len(generated_text.split())  # í…ìŠ¤íŠ¸ ê¸¸ì´
                                }
                                logger.info(f"  âœ… A2T validation: '{generated_text[:30]}...'")  # ì„±ê³µ ë¡œê·¸
                        except Exception as e:  # ì˜ˆì™¸ ë°œìƒ ì‹œ
                            logger.warning(f"  âš ï¸ A2T validation failed: {e}")  # ê²½ê³  ë¡œê·¸
                    
                    # ============================================================================
                    # ëª¨ë“  ê²€ì¦ ê²°ê³¼ ë¡œê¹…
                    # ============================================================================
                    for tracker in accelerator.trackers:  # ëª¨ë“  íŠ¸ë˜ì»¤ì— ëŒ€í•´
                        if tracker.name == "wandb":  # wandb íŠ¸ë˜ì»¤ì¸ ê²½ìš°
                            # ============================================================================
                            # ê²€ì¦ ìš”ì•½ í…Œì´ë¸” ìƒì„±
                            # ============================================================================
                            val_data = []  # ê²€ì¦ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                            for task, result in validation_results.items():  # ê° íƒœìŠ¤í¬ì™€ ê²°ê³¼ì— ëŒ€í•´
                                val_data.append({  # ê²€ì¦ ë°ì´í„° ì¶”ê°€
                                    'task': task.upper(),  # íƒœìŠ¤í¬ ì´ë¦„ (ëŒ€ë¬¸ì)
                                    'status': 'âœ… Success',  # ìƒíƒœ (ì„±ê³µ)
                                    'details': str(result)  # ê²°ê³¼ ì„¸ë¶€ì‚¬í•­
                                })
                            
                            if val_data:  # ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                                df = pd.DataFrame(val_data)  # pandas DataFrame ìƒì„±
                                html = wandb.Html(df.to_html(), inject=True)  # HTML í…Œì´ë¸” ìƒì„±
                                tracker.log({f"validation_all_tasks_step_{global_step}": html})  # wandbì— ë¡œê¹…
                            
                            # ============================================================================
                            # ê°œë³„ íƒœìŠ¤í¬ ê²°ê³¼ ë¡œê¹…
                            # ============================================================================
                            for task, result in validation_results.items():  # ê° íƒœìŠ¤í¬ì™€ ê²°ê³¼ì— ëŒ€í•´
                                tracker.log({f"val_{task}": result}, step=global_step)  # wandbì— ê°œë³„ íƒœìŠ¤í¬ ê²°ê³¼ ë¡œê¹…
                    
                    transformer.train()  # íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í›ˆë ¨ ëª¨ë“œë¡œ ë‹¤ì‹œ ì„¤ì •
                    del pipeline  # íŒŒì´í”„ë¼ì¸ ì‚­ì œ
                    torch.cuda.empty_cache()  # CUDA ìºì‹œ ë¹„ìš°ê¸°
                
                if global_step >= args.max_train_steps:  # ìµœëŒ€ í›ˆë ¨ ìŠ¤í…ì— ë„ë‹¬í•œ ê²½ìš°
                    break  # í›ˆë ¨ ë£¨í”„ ì¢…ë£Œ

    # ============================================================================
    # ìµœì¢… ì €ì¥
    # ============================================================================
    accelerator.wait_for_everyone()  # ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
    if accelerator.is_main_process:  # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
        save_path = os.path.join(args.output_dir, f"checkpoint-final")  # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ
        accelerator.save_state(save_path)  # ê°€ì†ê¸° ìƒíƒœ ì €ì¥
        
        if args.use_ema:  # EMA ì‚¬ìš©ì´ í™œì„±í™”ëœ ê²½ìš°
            ema_path = os.path.join(save_path, "ema_transformer.pt")  # EMA ëª¨ë¸ ì €ì¥ ê²½ë¡œ
            torch.save(ema_transformer.state_dict(), ema_path)  # EMA ëª¨ë¸ ìƒíƒœ ì €ì¥
            
        logger.info(f"Training complete! Final checkpoint saved to {save_path}")  # í›ˆë ¨ ì™„ë£Œ ë¡œê·¸
        
    accelerator.end_training()  # í›ˆë ¨ ì¢…ë£Œ


if __name__ == "__main__":
    args = parse_omniges_args()
    main(args)
